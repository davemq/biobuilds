--- configure.ac
+++ configure.ac
@@ -401,9 +401,17 @@
 # If we're using SSE, figure out our @SIMD_CFLAGS@
 if test "$impl_choice" = "sse" && test "x$SIMD_CFLAGS" = x; then
    case $ax_cv_c_compiler_vendor in
-   gnu)   AX_CHECK_COMPILER_FLAGS([-msse2],        [ SIMD_CFLAGS="-msse2" ])
+   gnu)
+    case $host in
+      powerpc*-*-*)
+      SIMD_CFLAGS="-maltivec -mvsx"
+      ;;
+      *)
+          AX_CHECK_COMPILER_FLAGS([-msse2],        [ SIMD_CFLAGS="-msse2" ])
           AX_CHECK_COMPILER_FLAGS([-msse2 -msse3], [ SIMD_CFLAGS="-msse2 -msse3" ])
 	  ;;
+    esac
+    ;;
    intel) ;;
    *)     ;;
    esac
@@ -445,6 +453,12 @@
 # check if the SSE2 implementation support cast functions
 if test "$impl_choice" = "sse"; then
   AC_MSG_CHECKING([compiler support for sse2 cast functions])
+  case $host in
+    powerpc*-*-*)
+    AC_DEFINE([HAVE_SSE2_CAST])
+    ssecast=yes
+    ;;
+    *)
   ssecast=no
   sre_save_cflags="$CFLAGS"
   CFLAGS="$CFLAGS $SIMD_CFLAGS"
@@ -458,6 +472,8 @@
                      ssecast=yes])
   AC_MSG_RESULT([$ssecast])
   CFLAGS="$sre_save_cflags"
+    ;;
+  esac
 fi
 
 # Verify that we can actually build the chosen implementation; else
@@ -468,6 +484,11 @@
 #
 if test "$impl_choice" = "sse"; then
   AC_MSG_CHECKING([whether SSE2 is supported])
+  case $host in
+    powerpc*-*-*)
+    AC_MSG_RESULT([yes])
+    ;;
+    *)
   sre_save_cflags="$CFLAGS"
   CFLAGS="$CFLAGS $SIMD_CFLAGS"
   AC_COMPILE_IFELSE(  [AC_LANG_PROGRAM([[#include <emmintrin.h>]],
@@ -486,6 +507,8 @@
 	]
   )
   CFLAGS="$sre_save_cflags"
+    ;;
+  esac
 fi
 
 if test "$impl_choice" = "vmx"; then
@@ -525,6 +548,11 @@
 # values in the floating point calculations.
 if test "$impl_choice" = "sse"; then
   AC_MSG_CHECKING([whether _MM_SET_FLUSH_ZERO_MODE is supported])
+  case $host in
+    powerpc*-*-*)
+      AC_MSG_RESULT([no])
+    ;;
+    *)
   sre_save_cflags="$CFLAGS"
   CFLAGS="$CFLAGS $SIMD_CFLAGS"
   AC_COMPILE_IFELSE(  [AC_LANG_PROGRAM([[#include <xmmintrin.h>]],
@@ -535,6 +563,8 @@
 	[ AC_MSG_RESULT([no])]
   )
   CFLAGS="$sre_save_cflags"
+    ;;
+  esac
 fi
 
 # Now, we can enable the appropriate optimized implementation.
--- easel/esl_avx.h
+++ easel/esl_avx.h
@@ -17,8 +17,8 @@
 #include "easel.h"
 
 #include <stdio.h>
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 
 #ifdef HAVE_AVX2 // don't include on architectures that can't compile avx2
--- easel/esl_sse.c
+++ easel/esl_sse.c
@@ -29,8 +29,8 @@
 #include <math.h>
 #include <float.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sse.h"
@@ -74,10 +74,10 @@
   static float cephes_p[9] = {  7.0376836292E-2f, -1.1514610310E-1f,  1.1676998740E-1f,
 				-1.2420140846E-1f, 1.4249322787E-1f, -1.6668057665E-1f,
 				2.0000714765E-1f, -2.4999993993E-1f,  3.3333331174E-1f };
-  __m128  onev = _mm_set1_ps(1.0f);          /* all elem = 1.0 */
-  __m128  v0p5 = _mm_set1_ps(0.5f);          /* all elem = 0.5 */
-  __m128i vneg = _mm_set1_epi32(0x80000000); /* all elem have IEEE sign bit up */
-  __m128i vexp = _mm_set1_epi32(0x7f800000); /* all elem have IEEE exponent bits up */
+  __m128  onev = vec_splat4sp(1.0f);          /* all elem = 1.0 */
+  __m128  v0p5 = vec_splat4sp(0.5f);          /* all elem = 0.5 */
+  __m128i vneg = vec_splat4sw(0x80000000); /* all elem have IEEE sign bit up */
+  __m128i vexp = vec_splat4sw(0x7f800000); /* all elem have IEEE exponent bits up */
   __m128i ei;
   __m128  e;
   __m128  invalid_mask, zero_mask, inf_mask;            /* masks used to handle special IEEE754 inputs */
@@ -88,51 +88,51 @@
   __m128  z;
 
   /* first, split x apart: x = frexpf(x, &e); */
-  ei           = _mm_srli_epi32( _mm_castps_si128(x), 23);	                                        /* shift right 23: IEEE754 floats: ei = biased exponents     */
-  invalid_mask = _mm_castsi128_ps ( _mm_cmpeq_epi32( _mm_and_si128(_mm_castps_si128(x), vneg), vneg));  /* mask any elem that's negative; these become NaN           */
-  zero_mask    = _mm_castsi128_ps ( _mm_cmpeq_epi32(ei, _mm_setzero_si128()));                          /* mask any elem zero or subnormal; these become -inf        */
-  inf_mask     = _mm_castsi128_ps ( _mm_cmpeq_epi32( _mm_and_si128(_mm_castps_si128(x), vexp), vexp));  /* mask any elem inf or NaN; log(inf)=inf, log(NaN)=NaN      */
+  ei           = vec_shiftrightimmediate4sw( vec_cast4spto1q(x), 23);	                                        /* shift right 23: IEEE754 floats: ei = biased exponents     */
+  invalid_mask = vec_cast1qto4sp ( vec_compare4sw( vec_bitand1q(vec_cast4spto1q(x), vneg), vneg));  /* mask any elem that's negative; these become NaN           */
+  zero_mask    = vec_cast1qto4sp ( vec_compare4sw(ei, vec_zero1q()));                          /* mask any elem zero or subnormal; these become -inf        */
+  inf_mask     = vec_cast1qto4sp ( vec_compare4sw( vec_bitand1q(vec_cast4spto1q(x), vexp), vexp));  /* mask any elem inf or NaN; log(inf)=inf, log(NaN)=NaN      */
   origx        = x;			                                                                /* store original x, used for log(inf) = inf, log(NaN) = NaN */
 
-  x  = _mm_and_ps(x, _mm_castsi128_ps(_mm_set1_epi32(~0x7f800000))); /* x now the stored 23 bits of the 24-bit significand        */
-  x  = _mm_or_ps (x, v0p5);                                          /* sets hidden bit b[0]                                      */
+  x  = vec_bitwiseand4sp(x, vec_cast1qto4sp(vec_splat4sw(~0x7f800000))); /* x now the stored 23 bits of the 24-bit significand        */
+  x  = vec_bitwiseor4sp (x, v0p5);                                          /* sets hidden bit b[0]                                      */
 
-  ei = _mm_sub_epi32(ei, _mm_set1_epi32(126));                       /* -127 (ei now signed base-2 exponent); then +1             */
-  e  = _mm_cvtepi32_ps(ei);
+  ei = vec_subtract4sw(ei, vec_splat4sw(126));                       /* -127 (ei now signed base-2 exponent); then +1             */
+  e  = vec_convert4swto4sp(ei);
 
   /* now, calculate the log */
-  mask = _mm_cmplt_ps(x, _mm_set1_ps(0.707106781186547524f)); /* avoid conditional branches.           */
-  tmp  = _mm_and_ps(x, mask);	                              /* tmp contains x values < 0.707, else 0 */
-  x    = _mm_sub_ps(x, onev);
-  e    = _mm_sub_ps(e, _mm_and_ps(onev, mask));
-  x    = _mm_add_ps(x, tmp);
-  z    = _mm_mul_ps(x,x);
-
-  y =               _mm_set1_ps(cephes_p[0]);    y = _mm_mul_ps(y, x); 
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[1]));   y = _mm_mul_ps(y, x);    
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[2]));   y = _mm_mul_ps(y, x);   
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[3]));   y = _mm_mul_ps(y, x);   
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[4]));   y = _mm_mul_ps(y, x);    
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[5]));   y = _mm_mul_ps(y, x);   
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[6]));   y = _mm_mul_ps(y, x); 
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[7]));   y = _mm_mul_ps(y, x);  
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[8]));   y = _mm_mul_ps(y, x);
-  y = _mm_mul_ps(y, z);
-
-  tmp = _mm_mul_ps(e, _mm_set1_ps(-2.12194440e-4f));
-  y   = _mm_add_ps(y, tmp);
-
-  tmp = _mm_mul_ps(z, v0p5);
-  y   = _mm_sub_ps(y, tmp);
-
-  tmp = _mm_mul_ps(e, _mm_set1_ps(0.693359375f));
-  x = _mm_add_ps(x, y);
-  x = _mm_add_ps(x, tmp);
+  mask = vec_comparelt4sp(x, vec_splat4sp(0.707106781186547524f)); /* avoid conditional branches.           */
+  tmp  = vec_bitwiseand4sp(x, mask);	                              /* tmp contains x values < 0.707, else 0 */
+  x    = vec_subtract4sp(x, onev);
+  e    = vec_subtract4sp(e, vec_bitwiseand4sp(onev, mask));
+  x    = vec_add4sp(x, tmp);
+  z    = vec_multiply4sp(x,x);
+
+  y =               vec_splat4sp(cephes_p[0]);    y = vec_multiply4sp(y, x); 
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[1]));   y = vec_multiply4sp(y, x);    
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[2]));   y = vec_multiply4sp(y, x);   
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[3]));   y = vec_multiply4sp(y, x);   
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[4]));   y = vec_multiply4sp(y, x);    
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[5]));   y = vec_multiply4sp(y, x);   
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[6]));   y = vec_multiply4sp(y, x); 
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[7]));   y = vec_multiply4sp(y, x);  
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[8]));   y = vec_multiply4sp(y, x);
+  y = vec_multiply4sp(y, z);
+
+  tmp = vec_multiply4sp(e, vec_splat4sp(-2.12194440e-4f));
+  y   = vec_add4sp(y, tmp);
+
+  tmp = vec_multiply4sp(z, v0p5);
+  y   = vec_subtract4sp(y, tmp);
+
+  tmp = vec_multiply4sp(e, vec_splat4sp(0.693359375f));
+  x = vec_add4sp(x, y);
+  x = vec_add4sp(x, tmp);
 
   /* IEEE754 cleanup: */
   x = esl_sse_select_ps(x, origx,                     inf_mask);  /* log(inf)=inf; log(NaN)      = NaN  */
-  x = _mm_or_ps(x, invalid_mask);                                 /* log(x<0, including -0,-inf) = NaN  */
-  x = esl_sse_select_ps(x, _mm_set1_ps(-eslINFINITY), zero_mask); /* x zero or subnormal         = -inf */
+  x = vec_bitwiseor4sp(x, invalid_mask);                                 /* log(x<0, including -0,-inf) = NaN  */
+  x = esl_sse_select_ps(x, vec_splat4sp(-eslINFINITY), zero_mask); /* x zero or subnormal         = -inf */
   return x;
 }
 
@@ -190,48 +190,48 @@
   __m128  mask, tmp, fx, z, y, minmask, maxmask;
   
   /* handle out-of-range and special conditions */
-  maxmask = _mm_cmpgt_ps(x, _mm_set1_ps(maxlogf));
-  minmask = _mm_cmple_ps(x, _mm_set1_ps(minlogf));
+  maxmask = vec_comparegt4sp(x, vec_splat4sp(maxlogf));
+  minmask = vec_comparele4sp(x, vec_splat4sp(minlogf));
 
   /* range reduction: exp(x) = 2^k e^f = exp(f + k log 2); k = floorf(0.5 + x / log2): */
-  fx = _mm_mul_ps(x,  _mm_set1_ps(eslCONST_LOG2R));
-  fx = _mm_add_ps(fx, _mm_set1_ps(0.5f));
+  fx = vec_multiply4sp(x,  vec_splat4sp(eslCONST_LOG2R));
+  fx = vec_add4sp(fx, vec_splat4sp(0.5f));
 
   /* floorf() with SSE:  */
-  k    = _mm_cvttps_epi32(fx);	              /* cast to int with truncation                  */
-  tmp  = _mm_cvtepi32_ps(k);	              /* cast back to float                           */
-  mask = _mm_cmpgt_ps(tmp, fx);               /* if it increased (i.e. if it was negative...) */
-  mask = _mm_and_ps(mask, _mm_set1_ps(1.0f)); /* ...without a conditional branch...           */
-  fx   = _mm_sub_ps(tmp, mask);	              /* then subtract one.                           */
-  k    = _mm_cvttps_epi32(fx);	              /* k is now ready for the 2^k part.             */
+  k    = vec_converttruncating4spto4sw(fx);	              /* cast to int with truncation                  */
+  tmp  = vec_convert4swto4sp(k);	              /* cast back to float                           */
+  mask = vec_comparegt4sp(tmp, fx);               /* if it increased (i.e. if it was negative...) */
+  mask = vec_bitwiseand4sp(mask, vec_splat4sp(1.0f)); /* ...without a conditional branch...           */
+  fx   = vec_subtract4sp(tmp, mask);	              /* then subtract one.                           */
+  k    = vec_converttruncating4spto4sw(fx);	              /* k is now ready for the 2^k part.             */
   
   /* polynomial approx for e^f for f in range [-0.5, 0.5] */
-  tmp = _mm_mul_ps(fx, _mm_set1_ps(cephes_c[0]));
-  z   = _mm_mul_ps(fx, _mm_set1_ps(cephes_c[1]));
-  x   = _mm_sub_ps(x, tmp);
-  x   = _mm_sub_ps(x, z);
-  z   = _mm_mul_ps(x, x);
+  tmp = vec_multiply4sp(fx, vec_splat4sp(cephes_c[0]));
+  z   = vec_multiply4sp(fx, vec_splat4sp(cephes_c[1]));
+  x   = vec_subtract4sp(x, tmp);
+  x   = vec_subtract4sp(x, z);
+  z   = vec_multiply4sp(x, x);
   
-  y =               _mm_set1_ps(cephes_p[0]);    y = _mm_mul_ps(y, x);
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[1]));   y = _mm_mul_ps(y, x);
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[2]));   y = _mm_mul_ps(y, x);
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[3]));   y = _mm_mul_ps(y, x);
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[4]));   y = _mm_mul_ps(y, x);
-  y = _mm_add_ps(y, _mm_set1_ps(cephes_p[5]));   y = _mm_mul_ps(y, z);
-  y = _mm_add_ps(y, x);
-  y = _mm_add_ps(y, _mm_set1_ps(1.0f));
+  y =               vec_splat4sp(cephes_p[0]);    y = vec_multiply4sp(y, x);
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[1]));   y = vec_multiply4sp(y, x);
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[2]));   y = vec_multiply4sp(y, x);
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[3]));   y = vec_multiply4sp(y, x);
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[4]));   y = vec_multiply4sp(y, x);
+  y = vec_add4sp(y, vec_splat4sp(cephes_p[5]));   y = vec_multiply4sp(y, z);
+  y = vec_add4sp(y, x);
+  y = vec_add4sp(y, vec_splat4sp(1.0f));
 
   /* build 2^k by hand, by creating a IEEE754 float */
-  k  = _mm_add_epi32(k, _mm_set1_epi32(127));
-  k  = _mm_slli_epi32(k, 23);
-  fx = _mm_castsi128_ps(k);
+  k  = vec_add4sw(k, vec_splat4sw(127));
+  k  = vec_shiftleftimmediate4sw(k, 23);
+  fx = vec_cast1qto4sp(k);
   
   /* put 2^k e^f together (fx = 2^k,  y = e^f) and we're done */
-  y = _mm_mul_ps(y, fx);	
+  y = vec_multiply4sp(y, fx);	
 
   /* special/range cleanup */
-  y = esl_sse_select_ps(y, _mm_set1_ps(eslINFINITY), maxmask); /* exp(x) = inf for x > log(2^128)  */
-  y = esl_sse_select_ps(y, _mm_set1_ps(0.0f),        minmask); /* exp(x) = 0   for x < log(2^-149) */
+  y = esl_sse_select_ps(y, vec_splat4sp(eslINFINITY), maxmask); /* exp(x) = inf for x > log(2^128)  */
+  y = esl_sse_select_ps(y, vec_splat4sp(0.0f),        minmask); /* exp(x) = 0   for x < log(2^-149) */
   return y;
 }
 
@@ -283,7 +283,7 @@
   int             N       = esl_opt_GetInteger(go, "-N");
   float           origx   = 2.0;
   float           x       = origx;
-  __m128          xv      = _mm_set1_ps(x);
+  __m128          xv      = vec_splat4sp(x);
   int             i;
 
   /* First, serial time. */
@@ -330,7 +330,7 @@
    *    log(-inf) = NaN     log(x<0)  = NaN  log(-0)   = NaN
    *    log(0)    = -inf    log(inf)  = inf  log(NaN)  = NaN
    */
-  x   = _mm_set_ps(0.0, -0.0, -1.0, -eslINFINITY); /* set_ps() is in order 3 2 1 0 */
+  x   = vec_set4sp(0.0, -0.0, -1.0, -eslINFINITY); /* set_ps() is in order 3 2 1 0 */
   r.v =  esl_sse_logf(x); 
   if (esl_opt_GetBoolean(go, "-v")) {
     printf("logf");
@@ -342,7 +342,7 @@
   if (! isnan(r.x[2]))                 esl_fatal("logf(-0)   should be NaN");
   if (! (r.x[3] < 0 && isinf(r.x[3]))) esl_fatal("logf(0)    should be -inf");
 
-  x   = _mm_set_ps(FLT_MAX, FLT_MIN, eslNaN, eslINFINITY);
+  x   = vec_set4sp(FLT_MAX, FLT_MIN, eslNaN, eslINFINITY);
   r.v = esl_sse_logf(x);
   if (esl_opt_GetBoolean(go, "-v")) {
     printf("logf");
@@ -362,7 +362,7 @@
   union { __m128 v; float x[4]; } r;   /* test output */
   
   /* exp(-inf) = 0    exp(-0)  = 1   exp(0) = 1  exp(inf) = inf   exp(NaN)  = NaN */
-  x = _mm_set_ps(eslINFINITY, 0.0, -0.0, -eslINFINITY); /* set_ps() is in order 3 2 1 0 */
+  x = vec_set4sp(eslINFINITY, 0.0, -0.0, -eslINFINITY); /* set_ps() is in order 3 2 1 0 */
   r.v =  esl_sse_expf(x); 
   if (esl_opt_GetBoolean(go, "-v")) {
     printf("expf");
@@ -375,7 +375,7 @@
   if (! isinf(r.x[3]))  esl_fatal("expf(inf)  should be inf");
 
   /* exp(NaN) = NaN    exp(large)  = inf   exp(-large) = 0  exp(1) = exp(1) */
-  x = _mm_set_ps(1.0f, -666.0f, 666.0f, eslNaN); /* set_ps() is in order 3 2 1 0 */
+  x = vec_set4sp(1.0f, -666.0f, 666.0f, eslNaN); /* set_ps() is in order 3 2 1 0 */
   r.v =  esl_sse_expf(x); 
   if (esl_opt_GetBoolean(go, "-v")) {
     printf("expf");
@@ -399,7 +399,7 @@
    *     (3): expf(-87.6832)   => 0
    *     (4): expf(-87.6831)   => <FLT_MIN (subnormal) : ~8.31e-39 (may become 0 in flush-to-zero mode for subnormals)
    */
-  x   = _mm_set_ps(-88.3763, -88.3762, -87.6832, -87.6831);
+  x   = vec_set4sp(-88.3763, -88.3762, -87.6832, -87.6831);
   r.v = esl_sse_expf(x); 
   if (esl_opt_GetBoolean(go, "-v")) {
     printf("expf");
@@ -437,7 +437,7 @@
 
       if (odds == 0.0) esl_fatal("whoa, odds ratio can't be 0!\n");
 
-      r1.v      = esl_sse_logf(_mm_set1_ps(odds));  /* r1.x[z] = log(p1/p2) */
+      r1.v      = esl_sse_logf(vec_splat4sp(odds));  /* r1.x[z] = log(p1/p2) */
       scalar_r1 = log(odds);
 
       err1       = (r1.x[0] == 0. && scalar_r1 == 0.) ? 0.0 : 2 * fabs(r1.x[0] - scalar_r1) / fabs(r1.x[0] + scalar_r1);
@@ -544,7 +544,7 @@
   union { __m128 v; float x[4]; } rv;   /* result vector*/
 
   x    = 2.0;
-  xv   = _mm_set1_ps(x);
+  xv   = vec_splat4sp(x);
   rv.v = esl_sse_logf(xv);
   printf("logf(%f) = %f\n", x, rv.x[0]);
   
--- easel/esl_sse.h
+++ easel/esl_sse.h
@@ -17,8 +17,8 @@
 #include "easel.h"
 
 #include <stdio.h>
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 /* Some compilers (gcc 3.4) did not implement SSE2 cast functions 
  * on the theory that they're unnecessary no-ops -- but then
@@ -26,8 +26,8 @@
  * the no-ops.
  */
 #ifndef HAVE_SSE2_CAST
-#define _mm_castps_si128(x) (__m128i)(x)
-#define _mm_castsi128_ps(x) (__m128)(x)
+#define vec_cast4spto1q(x) (__m128i)(x)
+#define vec_cast1qto4sp(x) (__m128)(x)
 #endif
 
 
@@ -54,8 +54,8 @@
  *            to implement \ccode{if (a > 0) a += a;}:
  *            
  *            \begin{cchunk}
- *              mask = _mm_cmpgt_ps(a, _mm_setzero_ps());
- *              twoa = _mm_add_ps(a, a);
+ *              mask = vec_comparegt4sp(a, vec_zero4sp());
+ *              twoa = vec_add4sp(a, a);
  *              a    = esl_sse_select_ps(a, twoa, mask);
  *            \end{cchunk}
  *
@@ -65,9 +65,9 @@
 static inline __m128
 esl_sse_select_ps(__m128 a, __m128 b, __m128 mask)
 {
-  b = _mm_and_ps(b, mask);
-  a = _mm_andnot_ps(mask, a);
-  return _mm_or_ps(a,b);
+  b = vec_bitwiseand4sp(b, mask);
+  a = vec_bitwiseandnotleft4sp(mask, a);
+  return vec_bitwiseor4sp(a,b);
 }
 
 /* Function:  esl_sse_any_gt_ps()
@@ -81,8 +81,8 @@
 static inline int 
 esl_sse_any_gt_ps(__m128 a, __m128 b)
 {
-  __m128 mask    = _mm_cmpgt_ps(a,b);
-  int   maskbits = _mm_movemask_ps( mask );
+  __m128 mask    = vec_comparegt4sp(a,b);
+  int   maskbits = vec_extractupperbit4sp( mask );
   return maskbits != 0;
 }
 
@@ -98,9 +98,9 @@
 static inline void
 esl_sse_hmax_ps(__m128 a, float *ret_max)
 {
-  a = _mm_max_ps(a, _mm_shuffle_ps(a, a, _MM_SHUFFLE(0, 3, 2, 1)));
-  a = _mm_max_ps(a, _mm_shuffle_ps(a, a, _MM_SHUFFLE(1, 0, 3, 2)));
-  _mm_store_ss(ret_max, a);
+  a = vec_max4sp(a, vec_shufflepermute4sp(a, a, _MM_SHUFFLE(0, 3, 2, 1)));
+  a = vec_max4sp(a, vec_shufflepermute4sp(a, a, _MM_SHUFFLE(1, 0, 3, 2)));
+  vec_storeu4spto1sp(ret_max, a);
 }
 
 
@@ -113,9 +113,9 @@
 static inline void
 esl_sse_hmin_ps(__m128 a, float *ret_min)
 {
-  a = _mm_min_ps(a, _mm_shuffle_ps(a, a, _MM_SHUFFLE(0, 3, 2, 1)));
-  a = _mm_min_ps(a, _mm_shuffle_ps(a, a, _MM_SHUFFLE(1, 0, 3, 2)));
-  _mm_store_ss(ret_min, a);
+  a = vec_min4sp(a, vec_shufflepermute4sp(a, a, _MM_SHUFFLE(0, 3, 2, 1)));
+  a = vec_min4sp(a, vec_shufflepermute4sp(a, a, _MM_SHUFFLE(1, 0, 3, 2)));
+  vec_storeu4spto1sp(ret_min, a);
 }
 
 /* Function:  esl_sse_hsum_ps()
@@ -127,9 +127,9 @@
 static inline void
 esl_sse_hsum_ps(__m128 a, float *ret_sum)
 {
-  a = _mm_add_ps(a, _mm_shuffle_ps(a, a, _MM_SHUFFLE(0, 3, 2, 1)));
-  a = _mm_add_ps(a, _mm_shuffle_ps(a, a, _MM_SHUFFLE(1, 0, 3, 2)));
-  _mm_store_ss(ret_sum, a);
+  a = vec_add4sp(a, vec_shufflepermute4sp(a, a, _MM_SHUFFLE(0, 3, 2, 1)));
+  a = vec_add4sp(a, vec_shufflepermute4sp(a, a, _MM_SHUFFLE(1, 0, 3, 2)));
+  vec_storeu4spto1sp(ret_sum, a);
 }
 
 
@@ -145,7 +145,7 @@
 static inline __m128 
 esl_sse_rightshift_ps(__m128 a, __m128 b)
 {
-  return _mm_move_ss(_mm_shuffle_ps(a, a, _MM_SHUFFLE(2, 1, 0, 0)), b);
+  return vec_insert1spintolower4sp(vec_shufflepermute4sp(a, a, _MM_SHUFFLE(2, 1, 0, 0)), b);
 }
 
 /* Function:  esl_sse_leftshift_ps()
@@ -160,8 +160,8 @@
 static inline __m128
 esl_sse_leftshift_ps(__m128 a, __m128 b)
 {
-  register __m128 v = _mm_move_ss(a, b);                 /* now b[0] a[1] a[2] a[3] */
-  return _mm_shuffle_ps(v, v, _MM_SHUFFLE(0, 3, 2, 1));  /* now a[1] a[2] a[3] b[0] */
+  register __m128 v = vec_insert1spintolower4sp(a, b);                 /* now b[0] a[1] a[2] a[3] */
+  return vec_shufflepermute4sp(v, v, _MM_SHUFFLE(0, 3, 2, 1));  /* now a[1] a[2] a[3] b[0] */
 }
 
 
@@ -188,14 +188,14 @@
 static inline int 
 esl_sse_any_gt_epu8(__m128i a, __m128i b)
 {
-  __m128i mask    = _mm_cmpeq_epi8(_mm_max_epu8(a,b), b); /* anywhere a>b, mask[z] = 0x0; elsewhere 0xff */
-  int   maskbits  = _mm_movemask_epi8(_mm_xor_si128(mask,  _mm_cmpeq_epi8(mask, mask))); /* the xor incantation is a bitwise inversion */
+  __m128i mask    = vec_compareeq16sb(vec_max16ub(a,b), b); /* anywhere a>b, mask[z] = 0x0; elsewhere 0xff */
+  int   maskbits  = vec_extractupperbit16sb(vec_bitxor1q(mask,  vec_compareeq16sb(mask, mask))); /* the xor incantation is a bitwise inversion */
   return maskbits != 0;
 }
 static inline int 
 esl_sse_any_gt_epi16(__m128i a, __m128i b)
 {
-  return (_mm_movemask_epi8(_mm_cmpgt_epi16(a,b)) != 0); 
+  return (vec_extractupperbit16sb(vec_comparegt8sh(a,b)) != 0); 
 }
 
 
@@ -208,11 +208,11 @@
 static inline uint8_t
 esl_sse_hmax_epu8(__m128i a)
 {
-  a = _mm_max_epu8(a, _mm_srli_si128(a, 8));
-  a = _mm_max_epu8(a, _mm_srli_si128(a, 4));
-  a = _mm_max_epu8(a, _mm_srli_si128(a, 2));
-  a = _mm_max_epu8(a, _mm_srli_si128(a, 1));
-  return (uint8_t) _mm_extract_epi16(a, 0);   /* only low-order 8 bits set; so _epi16 or _epi8 equiv; _epi8 is SSE4.1 */
+  a = vec_max16ub(a, vec_shiftrightbytes1q(a, 8));
+  a = vec_max16ub(a, vec_shiftrightbytes1q(a, 4));
+  a = vec_max16ub(a, vec_shiftrightbytes1q(a, 2));
+  a = vec_max16ub(a, vec_shiftrightbytes1q(a, 1));
+  return (uint8_t) vec_extract8sh(a, 0);   /* only low-order 8 bits set; so _epi16 or _epi8 equiv; _epi8 is SSE4.1 */
 }
 
 /* Function:  esl_sse_hmax_epi16()
@@ -224,10 +224,10 @@
 static inline int16_t
 esl_sse_hmax_epi16(__m128i a)
 {
-  a = _mm_max_epi16(a, _mm_srli_si128(a, 8));
-  a = _mm_max_epi16(a, _mm_srli_si128(a, 4));
-  a = _mm_max_epi16(a, _mm_srli_si128(a, 2));
-  return (int16_t) _mm_extract_epi16(a, 0);   /* only low-order 8 bits set; so _epi16 or _epi8 equiv; _epi8 is SSE4.1 */
+  a = vec_max8sh(a, vec_shiftrightbytes1q(a, 8));
+  a = vec_max8sh(a, vec_shiftrightbytes1q(a, 4));
+  a = vec_max8sh(a, vec_shiftrightbytes1q(a, 2));
+  return (int16_t) vec_extract8sh(a, 0);   /* only low-order 8 bits set; so _epi16 or _epi8 equiv; _epi8 is SSE4.1 */
 }
 
 
--- hmmer/src/fm_sse.c
+++ hmmer/src/fm_sse.c
@@ -3,8 +3,7 @@
 #include <stdio.h>
 
 #if   defined (p7_IMPL_SSE)
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
 #endif
 
 #include "easel.h"
@@ -69,14 +68,14 @@
 
 #if   defined (p7_IMPL_SSE)
 
-  cfg->fm_allones_v = _mm_set1_epi8(0xff);
-  cfg->fm_neg128_v  = _mm_set1_epi8((int8_t) -128);
-  cfg->fm_zeros_v = _mm_set1_epi8((int8_t) 0x00);      //00 00 00 00
-  cfg->fm_m0f     = _mm_set1_epi8((int8_t) 0x0f);      //00 00 11 11
+  cfg->fm_allones_v = vec_splat16sb(0xff);
+  cfg->fm_neg128_v  = vec_splat16sb((int8_t) -128);
+  cfg->fm_zeros_v = vec_splat16sb((int8_t) 0x00);      //00 00 00 00
+  cfg->fm_m0f     = vec_splat16sb((int8_t) 0x0f);      //00 00 11 11
 
   if (cfg->meta->alph_type == fm_DNA) {
-    cfg->fm_m01 = _mm_set1_epi8((int8_t) 0x55);   //01 01 01 01
-    cfg->fm_m11 = _mm_set1_epi8((int8_t) 0x03);  //00 00 00 11
+    cfg->fm_m01 = vec_splat16sb((int8_t) 0x55);   //01 01 01 01
+    cfg->fm_m11 = vec_splat16sb((int8_t) 0x03);  //00 00 00 11
   }
     //set up an array of vectors, one for each character in the alphabet
   cfg->fm_chars_v         = NULL;
@@ -93,7 +92,7 @@
       c |= i<<6;
     } //else, just leave it on the right-most bits
 
-    cfg->fm_chars_v[i] = _mm_set1_epi8(c);
+    cfg->fm_chars_v[i] = vec_splat16sb(c);
   }
 
   /* this is a collection of masks used to clear off the left- or right- part
@@ -143,7 +142,7 @@
         arr.bytes[j] = 0x0;
       }
       cfg->fm_masks_v[i]                           = *(__m128i*)(&(arr.m128));
-      cfg->fm_reverse_masks_v[trim_chunk_count-i]  = _mm_andnot_si128(cfg->fm_masks_v[i], cfg->fm_allones_v );
+      cfg->fm_reverse_masks_v[trim_chunk_count-i]  = vec_bitandnotleft1q(cfg->fm_masks_v[i], cfg->fm_allones_v );
 
     }
   }
@@ -184,7 +183,7 @@
  *            alphabet) at a time into the vector co-processors, then counting occurrences. One
  *            constraint of this approach is that occCnts_b checkpoints must be spaced at least
  *            every 32 or 64 chars (16 bytes, in pressed format), and in multiples of 64/32, so
- *            that _mm_load_si128 calls appropriately meet 16-byte-alignment requirements. That's
+ *            that vec_load1q calls appropriately meet 16-byte-alignment requirements. That's
  *            a reasonable expectation, as spacings of 256 or more seem to give the best speed,
  *            and certainly better space-utilization.
  */
@@ -249,7 +248,7 @@
         if (remaining_cnt > 0) {
           BWT_v    = *(__m128i*)(BWT+i);
           FM_MATCH_2BIT(BWT_v, c_v, tmp_v, tmp2_v, tmp_v);
-          tmp_v    = _mm_and_si128(tmp_v, *(cfg->fm_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
+          tmp_v    = vec_bitand1q(tmp_v, *(cfg->fm_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
           FM_COUNT_2BIT(tmp_v, tmp2_v, counts_v);
         }
 
@@ -264,7 +263,7 @@
         if (remaining_cnt > 0) {
           BWT_v = *(__m128i*)(BWT+i);
           FM_MATCH_2BIT(BWT_v, c_v, tmp_v, tmp2_v, tmp_v);
-          tmp_v    = _mm_and_si128(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
+          tmp_v    = vec_bitand1q(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
           FM_COUNT_2BIT(tmp_v, tmp2_v, counts_v);
         }
       }
@@ -281,8 +280,8 @@
         if (remaining_cnt > 0) {
           BWT_v    = *(__m128i*)(BWT+i);
           FM_MATCH_4BIT(BWT_v, c_v, tmp_v, tmp2_v);
-          tmp_v     = _mm_and_si128(tmp_v, *(cfg->fm_masks_v + (remaining_cnt+1)/2)); // mask characters we don't want to count
-          tmp2_v    = _mm_and_si128(tmp2_v, *(cfg->fm_masks_v + remaining_cnt/2));
+          tmp_v     = vec_bitand1q(tmp_v, *(cfg->fm_masks_v + (remaining_cnt+1)/2)); // mask characters we don't want to count
+          tmp2_v    = vec_bitand1q(tmp2_v, *(cfg->fm_masks_v + remaining_cnt/2));
           FM_COUNT_4BIT(tmp_v, tmp2_v, counts_v);
         }
 
@@ -297,8 +296,8 @@
         if (remaining_cnt > 0) {
           BWT_v = *(__m128i*)(BWT+i);
           FM_MATCH_4BIT(BWT_v, c_v, tmp_v, tmp2_v);
-          tmp_v     = _mm_and_si128(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/2)); // mask characters we don't want to count
-          tmp2_v    = _mm_and_si128(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
+          tmp_v     = vec_bitand1q(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/2)); // mask characters we don't want to count
+          tmp2_v    = vec_bitand1q(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
           FM_COUNT_4BIT(tmp_v, tmp2_v, counts_v);
         }
       }
@@ -308,38 +307,38 @@
       if (!up_b) { // count forward, adding
         for (i=1+landmark ; i+15<(pos+1);  i+=16) { // keep running until i begins a run that shouldn't all be counted
           BWT_v    = *(__m128i*)(BWT+i);
-          BWT_v    = _mm_cmpeq_epi8(BWT_v, c_v);  // each byte is all 1s if matching, all zeros otherwise
-          counts_v = _mm_subs_epi8(counts_v, BWT_v); // adds 1 for each matching byte  (subtracting negative 1)
+          BWT_v    = vec_compareeq16sb(BWT_v, c_v);  // each byte is all 1s if matching, all zeros otherwise
+          counts_v = vec_subtractsaturating16sb(counts_v, BWT_v); // adds 1 for each matching byte  (subtracting negative 1)
         }
         int remaining_cnt = pos + 1 -  i ;
         if (remaining_cnt > 0) {
           BWT_v    = *(__m128i*)(BWT+i);
-          BWT_v    = _mm_cmpeq_epi8(BWT_v, c_v);
-          BWT_v    = _mm_and_si128(BWT_v, *(cfg->fm_masks_v + remaining_cnt));// mask characters we don't want to count
-          counts_v = _mm_subs_epi8(counts_v, BWT_v);
+          BWT_v    = vec_compareeq16sb(BWT_v, c_v);
+          BWT_v    = vec_bitand1q(BWT_v, *(cfg->fm_masks_v + remaining_cnt));// mask characters we don't want to count
+          counts_v = vec_subtractsaturating16sb(counts_v, BWT_v);
         }
       } else { // count backwards, subtracting
 
         for (i=landmark-15 ; i>pos;  i-=16) {
           BWT_v = *(__m128i*)(BWT+i);
-          BWT_v    = _mm_cmpeq_epi8(BWT_v, c_v);  // each byte is all 1s if matching, all zeros otherwise
-          counts_v = _mm_subs_epi8(counts_v, BWT_v); // adds 1 for each matching byte  (subtracting negative 1)
+          BWT_v    = vec_compareeq16sb(BWT_v, c_v);  // each byte is all 1s if matching, all zeros otherwise
+          counts_v = vec_subtractsaturating16sb(counts_v, BWT_v); // adds 1 for each matching byte  (subtracting negative 1)
         }
         int remaining_cnt = 16 - (pos + 1 - i);
         if (remaining_cnt > 0) {
           BWT_v = *(__m128i*)(BWT+i);
-          BWT_v    = _mm_cmpeq_epi8(BWT_v, c_v);
-          BWT_v     = _mm_and_si128(BWT_v, *(cfg->fm_reverse_masks_v + remaining_cnt));// mask characters we don't want to count
-          //tmp2_v    = _mm_and_si128(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
-          counts_v = _mm_subs_epi8(counts_v, BWT_v);
+          BWT_v    = vec_compareeq16sb(BWT_v, c_v);
+          BWT_v     = vec_bitand1q(BWT_v, *(cfg->fm_reverse_masks_v + remaining_cnt));// mask characters we don't want to count
+          //tmp2_v    = vec_bitand1q(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
+          counts_v = vec_subtractsaturating16sb(counts_v, BWT_v);
         }
       }
     }
 
-    counts_v = _mm_xor_si128(counts_v, cfg->fm_neg128_v); //counts are stored in signed bytes, base -128. Move them to unsigned bytes
+    counts_v = vec_bitxor1q(counts_v, cfg->fm_neg128_v); //counts are stored in signed bytes, base -128. Move them to unsigned bytes
     FM_GATHER_8BIT_COUNTS(counts_v,counts_v,counts_v);
 
-    cnt  +=   ( up_b == 1 ?  -1 : 1) * ( _mm_extract_epi16(counts_v, 0) );
+    cnt  +=   ( up_b == 1 ?  -1 : 1) * ( vec_extract8sh(counts_v, 0) );
   }
 
   if (c==0 && pos >= fm->term_loc) { // I overcounted 'A' by one, because '$' was replaced with an 'A'
@@ -369,7 +368,7 @@
  *            alphabet) at a time into the vector co-processors, then counting occurrences. One
  *            constraint of this approach is that occCnts_b checkpoints must be spaced at least
  *            every 32 or 64 chars (16 bytes, in pressed format), and in multiples of 64/32, so
- *            that _mm_load_si128 calls appropriately meet 16-byte-alignment requirements. That's
+ *            that vec_load1q calls appropriately meet 16-byte-alignment requirements. That's
  *            a reasonable expectation, as spacings of 256 or more seem to give the best speed,
  *            and certainly better space-utilization.
  *
@@ -460,12 +459,12 @@
           for (j=0; j<c; j++) {
             c_v = *(cfg->fm_chars_v + j);
             FM_MATCH_2BIT(BWT_v, c_v, tmp_v, tmp2_v, tmp_v);
-            tmp_v    = _mm_and_si128(tmp_v, *(cfg->fm_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
+            tmp_v    = vec_bitand1q(tmp_v, *(cfg->fm_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
             FM_COUNT_2BIT(tmp_v, tmp2_v, counts_v_lt);
           }
           c_v = *(cfg->fm_chars_v + c);
           FM_MATCH_2BIT(BWT_v, c_v, tmp_v, tmp2_v, tmp_v);
-          tmp_v    = _mm_and_si128(tmp_v, *(cfg->fm_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
+          tmp_v    = vec_bitand1q(tmp_v, *(cfg->fm_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
           FM_COUNT_2BIT(tmp_v, tmp2_v, counts_v_eq);
 
         }
@@ -490,12 +489,12 @@
           for (j=0; j<c; j++) {
             c_v = *(cfg->fm_chars_v + j);
             FM_MATCH_2BIT(BWT_v, c_v, tmp_v, tmp2_v, tmp_v);
-            tmp_v    = _mm_and_si128(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
+            tmp_v    = vec_bitand1q(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
             FM_COUNT_2BIT(tmp_v, tmp2_v, counts_v_lt);
           }
           c_v = *(cfg->fm_chars_v + c);
           FM_MATCH_2BIT(BWT_v, c_v, tmp_v, tmp2_v, tmp_v);
-          tmp_v    = _mm_and_si128(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
+          tmp_v    = vec_bitand1q(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt)); // leaves only the remaining_cnt chars in the array
           FM_COUNT_2BIT(tmp_v, tmp2_v, counts_v_eq);
         }
       }
@@ -516,13 +515,13 @@
         if (remaining_cnt > 0) {
           BWT_v    = *(__m128i*)(BWT+i);
           FM_LT_4BIT(BWT_v, c_v, tmp_v, tmp2_v);
-          tmp_v     = _mm_and_si128(tmp_v, *(cfg->fm_masks_v + (remaining_cnt+1)/2)); // mask characters we don't want to count
-          tmp2_v    = _mm_and_si128(tmp2_v, *(cfg->fm_masks_v + remaining_cnt/2));
+          tmp_v     = vec_bitand1q(tmp_v, *(cfg->fm_masks_v + (remaining_cnt+1)/2)); // mask characters we don't want to count
+          tmp2_v    = vec_bitand1q(tmp2_v, *(cfg->fm_masks_v + remaining_cnt/2));
           FM_COUNT_4BIT(tmp_v, tmp2_v, counts_v_lt);
 
           FM_MATCH_4BIT(BWT_v, c_v, tmp_v, tmp2_v);
-          tmp_v     = _mm_and_si128(tmp_v, *(cfg->fm_masks_v + (remaining_cnt+1)/2)); // mask characters we don't want to count
-          tmp2_v    = _mm_and_si128(tmp2_v, *(cfg->fm_masks_v + remaining_cnt/2));
+          tmp_v     = vec_bitand1q(tmp_v, *(cfg->fm_masks_v + (remaining_cnt+1)/2)); // mask characters we don't want to count
+          tmp2_v    = vec_bitand1q(tmp2_v, *(cfg->fm_masks_v + remaining_cnt/2));
           FM_COUNT_4BIT(tmp_v, tmp2_v, counts_v_eq);
 
         }
@@ -540,13 +539,13 @@
         if (remaining_cnt > 0) {
           BWT_v = *(__m128i*)(BWT+i);
           FM_LT_4BIT(BWT_v, c_v, tmp_v, tmp2_v);
-          tmp_v     = _mm_and_si128(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/2)); // mask characters we don't want to count
-          tmp2_v    = _mm_and_si128(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
+          tmp_v     = vec_bitand1q(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/2)); // mask characters we don't want to count
+          tmp2_v    = vec_bitand1q(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
           FM_COUNT_4BIT(tmp_v, tmp2_v, counts_v_lt);
 
           FM_MATCH_4BIT(BWT_v, c_v, tmp_v, tmp2_v);
-          tmp_v     = _mm_and_si128(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/2)); // mask characters we don't want to count
-          tmp2_v    = _mm_and_si128(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
+          tmp_v     = vec_bitand1q(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/2)); // mask characters we don't want to count
+          tmp2_v    = vec_bitand1q(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
           FM_COUNT_4BIT(tmp_v, tmp2_v, counts_v_eq);
 
         }
@@ -556,57 +555,57 @@
       if (!up_b) { // count forward, adding
         for (i=1+landmark ; i+15<(pos+1);  i+=16) { // keep running until i begins a run that shouldn't all be counted
           BWT_v       = *(__m128i*)(BWT+i);
-          tmp_v       = _mm_cmplt_epi8(BWT_v, c_v);  // each byte is all 1s if leq, all zeros otherwise
-          counts_v_lt = _mm_subs_epi8(counts_v_lt, tmp_v); // adds 1 for each matching byte  (subtracting negative 1)
-          BWT_v       = _mm_cmpeq_epi8(BWT_v, c_v);  // each byte is all 1s if eq, all zeros otherwise
-          counts_v_eq = _mm_subs_epi8(counts_v_eq, BWT_v);
+          tmp_v       = vec_comparelt16sb(BWT_v, c_v);  // each byte is all 1s if leq, all zeros otherwise
+          counts_v_lt = vec_subtractsaturating16sb(counts_v_lt, tmp_v); // adds 1 for each matching byte  (subtracting negative 1)
+          BWT_v       = vec_compareeq16sb(BWT_v, c_v);  // each byte is all 1s if eq, all zeros otherwise
+          counts_v_eq = vec_subtractsaturating16sb(counts_v_eq, BWT_v);
         }
         int remaining_cnt = pos + 1 -  i ;
         if (remaining_cnt > 0) {
           BWT_v       = *(__m128i*)(BWT+i);
-          tmp_v       = _mm_cmplt_epi8(BWT_v, c_v);  // each byte is all 1s if leq, all zeros otherwise
-          tmp_v       = _mm_and_si128(tmp_v, *(cfg->fm_masks_v + remaining_cnt/4));
-          counts_v_lt = _mm_subs_epi8(counts_v_lt, tmp_v); // adds 1 for each matching byte  (subtracting negative 1)
+          tmp_v       = vec_comparelt16sb(BWT_v, c_v);  // each byte is all 1s if leq, all zeros otherwise
+          tmp_v       = vec_bitand1q(tmp_v, *(cfg->fm_masks_v + remaining_cnt/4));
+          counts_v_lt = vec_subtractsaturating16sb(counts_v_lt, tmp_v); // adds 1 for each matching byte  (subtracting negative 1)
 
-          BWT_v       = _mm_cmpeq_epi8(BWT_v, c_v);
-          BWT_v       = _mm_and_si128(BWT_v, *(cfg->fm_masks_v + remaining_cnt/4));// mask characters we don't want to count
-          counts_v_eq = _mm_subs_epi8(counts_v_eq, BWT_v);
+          BWT_v       = vec_compareeq16sb(BWT_v, c_v);
+          BWT_v       = vec_bitand1q(BWT_v, *(cfg->fm_masks_v + remaining_cnt/4));// mask characters we don't want to count
+          counts_v_eq = vec_subtractsaturating16sb(counts_v_eq, BWT_v);
         }
 
       } else { // count backwards, subtracting
         for (i=landmark-15 ; i>pos;  i-=16) {
           BWT_v = *(__m128i*)(BWT+i);
-          tmp_v       = _mm_cmplt_epi8(BWT_v, c_v);  // each byte is all 1s if leq, all zeros otherwise
-          counts_v_lt = _mm_subs_epi8(counts_v_lt, tmp_v); // adds 1 for each matching byte  (subtracting negative 1)
-          BWT_v       = _mm_cmpeq_epi8(BWT_v, c_v);  // each byte is all 1s if eq, all zeros otherwise
-          counts_v_eq = _mm_subs_epi8(counts_v_eq, BWT_v);
+          tmp_v       = vec_comparelt16sb(BWT_v, c_v);  // each byte is all 1s if leq, all zeros otherwise
+          counts_v_lt = vec_subtractsaturating16sb(counts_v_lt, tmp_v); // adds 1 for each matching byte  (subtracting negative 1)
+          BWT_v       = vec_compareeq16sb(BWT_v, c_v);  // each byte is all 1s if eq, all zeros otherwise
+          counts_v_eq = vec_subtractsaturating16sb(counts_v_eq, BWT_v);
         }
 
         int remaining_cnt = 16 - (pos + 1 - i);
         if (remaining_cnt > 0) {
           BWT_v = *(__m128i*)(BWT+i);
-          tmp_v       = _mm_cmplt_epi8(BWT_v, c_v);  // each byte is all 1s if leq, all zeros otherwise
-          tmp_v       = _mm_and_si128(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/4));
-          counts_v_lt = _mm_subs_epi8(counts_v_lt, tmp_v); // adds 1 for each matching byte  (subtracting negative 1)
-
-          BWT_v       = _mm_cmpeq_epi8(BWT_v, c_v);
-          BWT_v       = _mm_and_si128(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/4));// mask characters we don't want to count
-          //tmp2_v    = _mm_and_si128(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
-          counts_v_eq = _mm_subs_epi8(counts_v_eq, BWT_v);
+          tmp_v       = vec_comparelt16sb(BWT_v, c_v);  // each byte is all 1s if leq, all zeros otherwise
+          tmp_v       = vec_bitand1q(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/4));
+          counts_v_lt = vec_subtractsaturating16sb(counts_v_lt, tmp_v); // adds 1 for each matching byte  (subtracting negative 1)
+
+          BWT_v       = vec_compareeq16sb(BWT_v, c_v);
+          BWT_v       = vec_bitand1q(tmp_v, *(cfg->fm_reverse_masks_v + remaining_cnt/4));// mask characters we don't want to count
+          //tmp2_v    = vec_bitand1q(tmp2_v, *(cfg->fm_reverse_masks_v + (remaining_cnt+1)/2));
+          counts_v_eq = vec_subtractsaturating16sb(counts_v_eq, BWT_v);
         }
       }
 
     }
 
     if (c>0) {
-      counts_v_lt = _mm_xor_si128(counts_v_lt, cfg->fm_neg128_v); //counts are stored in signed bytes, base -128. Move them to unsigned bytes
+      counts_v_lt = vec_bitxor1q(counts_v_lt, cfg->fm_neg128_v); //counts are stored in signed bytes, base -128. Move them to unsigned bytes
       FM_GATHER_8BIT_COUNTS(counts_v_lt,counts_v_lt,counts_v_lt);
-      (*cntlt)  +=   ( up_b == 1 ?  -1 : 1) * ( _mm_extract_epi16(counts_v_lt, 0) );
+      (*cntlt)  +=   ( up_b == 1 ?  -1 : 1) * ( vec_extract8sh(counts_v_lt, 0) );
     }
 
-    counts_v_eq = _mm_xor_si128(counts_v_eq, cfg->fm_neg128_v);
+    counts_v_eq = vec_bitxor1q(counts_v_eq, cfg->fm_neg128_v);
     FM_GATHER_8BIT_COUNTS(counts_v_eq,counts_v_eq,counts_v_eq);
-    (*cnteq)  +=   ( up_b == 1 ?  -1 : 1) * ( _mm_extract_epi16(counts_v_eq, 0) );
+    (*cnteq)  +=   ( up_b == 1 ?  -1 : 1) * ( vec_extract8sh(counts_v_eq, 0) );
   }
 
 
--- hmmer/src/hmmer.h
+++ hmmer/src/hmmer.h
@@ -1025,15 +1025,15 @@
 /* Gather the sum of all counts in a 16x8-bit element into a single 16-bit
  *  element of the register (the 0th element)
  *
- *  the _mm_sad_epu8  accumulates 8-bit counts into 16-bit counts:
+ *  the vec_sumabsdiffs16ub  accumulates 8-bit counts into 16-bit counts:
  *      left 8 counts (64-bits) accumulate in counts_v[0],
  *      right 8 counts in counts_v[4]  (the other 6 16-bit ints are 0)
- *  the _mm_shuffle_epi32  flips the 4th int into the 0th slot
+ *  the vec_permute4sw  flips the 4th int into the 0th slot
  */
 #define FM_GATHER_8BIT_COUNTS( in_v, mid_v, out_v  ) do {\
-    mid_v = _mm_sad_epu8 (in_v, cfg->fm_zeros_v);\
-    tmp_v = _mm_shuffle_epi32(mid_v, _MM_SHUFFLE(1, 1, 1, 2));\
-    out_v = _mm_add_epi16(mid_v, tmp_v);\
+    mid_v = vec_sumabsdiffs16ub (in_v, cfg->fm_zeros_v);\
+    tmp_v = vec_permute4sw(mid_v, _MM_SHUFFLE(1, 1, 1, 2));\
+    out_v = vec_add8sh(mid_v, tmp_v);\
   } while (0)
 
 
@@ -1057,13 +1057,13 @@
  *
  */
 #define FM_MATCH_2BIT(in_v, c_v, a_v, b_v, out_v) do {\
-    a_v = _mm_xor_si128(in_v, c_v);\
+    a_v = vec_bitxor1q(in_v, c_v);\
     \
-    b_v  = _mm_srli_epi16(a_v, 1);\
-    a_v  = _mm_or_si128(a_v, b_v);\
-    b_v  = _mm_and_si128(a_v, cfg->fm_m01);\
+    b_v  = vec_shiftrightimmediate8sh(a_v, 1);\
+    a_v  = vec_bitor1q(a_v, b_v);\
+    b_v  = vec_bitand1q(a_v, cfg->fm_m01);\
     \
-    out_v  = _mm_subs_epi8(cfg->fm_m01,b_v);\
+    out_v  = vec_subtractsaturating16sb(cfg->fm_m01,b_v);\
   } while (0)
 
 
@@ -1081,15 +1081,15 @@
  * final 2 add()s        : tack current counts on to already-tabulated counts.
  */
 #define FM_COUNT_2BIT(a_v, b_v, cnts_v) do {\
-        b_v = _mm_srli_epi16(a_v, 4);\
-        a_v  = _mm_add_epi16(a_v, b_v);\
+        b_v = vec_shiftrightimmediate8sh(a_v, 4);\
+        a_v  = vec_add8sh(a_v, b_v);\
         \
-        b_v = _mm_srli_epi16(a_v, 2);\
-        a_v  = _mm_and_si128(a_v,cfg->fm_m11);\
-        b_v = _mm_and_si128(b_v,cfg->fm_m11);\
+        b_v = vec_shiftrightimmediate8sh(a_v, 2);\
+        a_v  = vec_bitand1q(a_v,cfg->fm_m11);\
+        b_v = vec_bitand1q(b_v,cfg->fm_m11);\
         \
-        cnts_v = _mm_add_epi16(cnts_v, a_v);\
-        cnts_v = _mm_add_epi16(cnts_v, b_v);\
+        cnts_v = vec_add8sh(cnts_v, a_v);\
+        cnts_v = vec_add8sh(cnts_v, b_v);\
   } while (0)
 
 
@@ -1112,12 +1112,12 @@
  * cmpeq()x2     : test if both left and right == c.  For each, if ==c , value = 11111111 (-1)
  */
 #define FM_MATCH_4BIT(in_v, c_v, out1_v, out2_v) do {\
-    out1_v    = _mm_srli_epi16(in_v, 4);\
-    out2_v    = _mm_and_si128(in_v, cfg->fm_m0f);\
-    out1_v    = _mm_and_si128(out1_v, cfg->fm_m0f);\
+    out1_v    = vec_shiftrightimmediate8sh(in_v, 4);\
+    out2_v    = vec_bitand1q(in_v, cfg->fm_m0f);\
+    out1_v    = vec_bitand1q(out1_v, cfg->fm_m0f);\
     \
-    out1_v    = _mm_cmpeq_epi8(out1_v, c_v);\
-    out2_v    = _mm_cmpeq_epi8(out2_v, c_v);\
+    out1_v    = vec_compareeq16sb(out1_v, c_v);\
+    out2_v    = vec_compareeq16sb(out2_v, c_v);\
   } while (0)
 
 
@@ -1139,12 +1139,12 @@
  * cmplt()x2     : test if both left and right < c.  For each, if <c , value = 11111111 (-1)
  */
 #define FM_LT_4BIT(in_v, c_v, out1_v, out2_v) do {\
-    out1_v    = _mm_srli_epi16(in_v, 4);\
-    out2_v    = _mm_and_si128(in_v, cfg->fm_m0f);\
-    out1_v    = _mm_and_si128(out1_v, cfg->fm_m0f);\
+    out1_v    = vec_shiftrightimmediate8sh(in_v, 4);\
+    out2_v    = vec_bitand1q(in_v, cfg->fm_m0f);\
+    out1_v    = vec_bitand1q(out1_v, cfg->fm_m0f);\
     \
-    out1_v    = _mm_cmplt_epi8(out1_v, c_v);\
-    out2_v    = _mm_cmplt_epi8(out2_v, c_v);\
+    out1_v    = vec_comparelt16sb(out1_v, c_v);\
+    out2_v    = vec_comparelt16sb(out2_v, c_v);\
   } while (0)
 
 
@@ -1159,8 +1159,8 @@
  * so subtracting the value of the byte is the same as adding 0 or 1.
  */
 #define FM_COUNT_4BIT(in1_v, in2_v, cnts_v) do {\
-    cnts_v = _mm_subs_epi8(cnts_v, in1_v);\
-    cnts_v = _mm_subs_epi8(cnts_v, in2_v);\
+    cnts_v = vec_subtractsaturating16sb(cnts_v, in1_v);\
+    cnts_v = vec_subtractsaturating16sb(cnts_v, in2_v);\
   } while (0)
 
 
--- hmmer/src/impl_sse/decoding.c
+++ hmmer/src/impl_sse/decoding.c
@@ -17,8 +17,8 @@
 #include <stdio.h>
 #include <math.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sse.h"
@@ -96,9 +96,9 @@
 
   ppv = pp->dpf[0];
   for (q = 0; q < Q; q++) {
-    *ppv = _mm_setzero_ps(); ppv++;
-    *ppv = _mm_setzero_ps(); ppv++;
-    *ppv = _mm_setzero_ps(); ppv++;
+    *ppv = vec_zero4sp(); ppv++;
+    *ppv = vec_zero4sp(); ppv++;
+    *ppv = vec_zero4sp(); ppv++;
   }
   pp->xmx[p7X_E] = 0.0;
   pp->xmx[p7X_N] = 0.0;
@@ -111,22 +111,22 @@
       ppv   =  pp->dpf[i];
       fv    = oxf->dpf[i];
       bv    = oxb->dpf[i];
-      totrv = _mm_set1_ps(scaleproduct * oxf->xmx[i*p7X_NXCELLS+p7X_SCALE]);
+      totrv = vec_splat4sp(scaleproduct * oxf->xmx[i*p7X_NXCELLS+p7X_SCALE]);
 
       for (q = 0; q < Q; q++)
 	{
 	  /* M */
-	  *ppv = _mm_mul_ps(*fv,  *bv);
-	  *ppv = _mm_mul_ps(*ppv,  totrv);
+	  *ppv = vec_multiply4sp(*fv,  *bv);
+	  *ppv = vec_multiply4sp(*ppv,  totrv);
 	  ppv++;  fv++;  bv++;
 
 	  /* D */
-	  *ppv = _mm_setzero_ps();
+	  *ppv = vec_zero4sp();
 	  ppv++;  fv++;  bv++;
 
 	  /* I */
-	  *ppv = _mm_mul_ps(*fv,  *bv);
-	  *ppv = _mm_mul_ps(*ppv,  totrv);
+	  *ppv = vec_multiply4sp(*fv,  *bv);
+	  *ppv = vec_multiply4sp(*ppv,  totrv);
 	  ppv++;  fv++;  bv++;
 	}
       pp->xmx[i*p7X_NXCELLS+p7X_E] = 0.0;
--- hmmer/src/impl_sse/fwdback.c
+++ hmmer/src/impl_sse/fwdback.c
@@ -36,8 +36,8 @@
 #include <stdio.h>
 #include <math.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sse.h"
@@ -277,7 +277,7 @@
   ox->M  = om->M;
   ox->L  = L;
   ox->has_own_scales = TRUE; 	/* all forward matrices control their own scalefactors */
-  zerov  = _mm_setzero_ps();
+  zerov  = vec_zero4sp();
   for (q = 0; q < Q; q++)
     MMO(dpc,q) = IMO(dpc,q) = DMO(dpc,q) = zerov;
   xE    = ox->xmx[p7X_E] = 0.;
@@ -299,9 +299,9 @@
       dpc   = ox->dpf[do_full * i];     /* avoid conditional, use do_full as kronecker delta */
       rp    = om->rfv[dsq[i]];
       tp    = om->tfv;
-      dcv   = _mm_setzero_ps();
-      xEv   = _mm_setzero_ps();
-      xBv   = _mm_set1_ps(xB);
+      dcv   = vec_zero4sp();
+      xEv   = vec_zero4sp();
+      xBv   = vec_splat4sp(xB);
 
       /* Right shifts by 4 bytes. 4,8,12,x becomes x,4,8,12.  Shift zeros on. */
       mpv   = esl_sse_rightshift_ps(MMO(dpp,Q-1), zerov);
@@ -311,12 +311,12 @@
       for (q = 0; q < Q; q++)
 	{
 	  /* Calculate new MMO(i,q); don't store it yet, hold it in sv. */
-	  sv   =                _mm_mul_ps(xBv, *tp);  tp++;
-	  sv   = _mm_add_ps(sv, _mm_mul_ps(mpv, *tp)); tp++;
-	  sv   = _mm_add_ps(sv, _mm_mul_ps(ipv, *tp)); tp++;
-	  sv   = _mm_add_ps(sv, _mm_mul_ps(dpv, *tp)); tp++;
-	  sv   = _mm_mul_ps(sv, *rp);                  rp++;
-	  xEv  = _mm_add_ps(xEv, sv);
+	  sv   =                vec_multiply4sp(xBv, *tp);  tp++;
+	  sv   = vec_add4sp(sv, vec_multiply4sp(mpv, *tp)); tp++;
+	  sv   = vec_add4sp(sv, vec_multiply4sp(ipv, *tp)); tp++;
+	  sv   = vec_add4sp(sv, vec_multiply4sp(dpv, *tp)); tp++;
+	  sv   = vec_multiply4sp(sv, *rp);                  rp++;
+	  xEv  = vec_add4sp(xEv, sv);
 	  
 	  /* Load {MDI}(i-1,q) into mpv, dpv, ipv;
 	   * {MDI}MX(q) is then the current, not the prev row
@@ -332,11 +332,11 @@
 	  /* Calculate the next D(i,q+1) partially: M->D only;
            * delay storage, holding it in dcv
 	   */
-	  dcv   = _mm_mul_ps(sv, *tp); tp++;
+	  dcv   = vec_multiply4sp(sv, *tp); tp++;
 
 	  /* Calculate and store I(i,q); assumes odds ratio for emission is 1.0 */
-	  sv         =                _mm_mul_ps(mpv, *tp);  tp++;
-	  IMO(dpc,q) = _mm_add_ps(sv, _mm_mul_ps(ipv, *tp)); tp++;
+	  sv         =                vec_multiply4sp(mpv, *tp);  tp++;
+	  IMO(dpc,q) = vec_add4sp(sv, vec_multiply4sp(ipv, *tp)); tp++;
 	}	  
 
       /* Now the DD paths. We would rather not serialize them but 
@@ -353,8 +353,8 @@
       tp         = om->tfv + 7*Q;	/* set tp to start of the DD's */
       for (q = 0; q < Q; q++) 
 	{
-	  DMO(dpc,q) = _mm_add_ps(dcv, DMO(dpc,q));	
-	  dcv        = _mm_mul_ps(DMO(dpc,q), *tp); tp++; /* extend DMO(q), so we include M->D and D->D paths */
+	  DMO(dpc,q) = vec_add4sp(dcv, DMO(dpc,q));	
+	  dcv        = vec_multiply4sp(DMO(dpc,q), *tp); tp++; /* extend DMO(q), so we include M->D and D->D paths */
 	}
 
       /* now. on small models, it seems best (empirically) to just go
@@ -373,8 +373,8 @@
 	      tp  = om->tfv + 7*Q;	/* set tp to start of the DD's */
 	      for (q = 0; q < Q; q++) 
 		{ /* note, extend dcv, not DMO(q); only adding DD paths now */
-		  DMO(dpc,q) = _mm_add_ps(dcv, DMO(dpc,q));	
-		  dcv        = _mm_mul_ps(dcv, *tp);   tp++; 
+		  DMO(dpc,q) = vec_add4sp(dcv, DMO(dpc,q));	
+		  dcv        = vec_multiply4sp(dcv, *tp);   tp++; 
 		}	    
 	    }
 	} 
@@ -389,26 +389,26 @@
 	      cv  = zerov;
 	      for (q = 0; q < Q; q++) 
 		{ /* using cmpgt below tests if DD changed any DMO(q) *without* conditional branch */
-		  sv         = _mm_add_ps(dcv, DMO(dpc,q));	
-		  cv         = _mm_or_ps(cv, _mm_cmpgt_ps(sv, DMO(dpc,q))); 
+		  sv         = vec_add4sp(dcv, DMO(dpc,q));	
+		  cv         = vec_bitwiseor4sp(cv, vec_comparegt4sp(sv, DMO(dpc,q))); 
 		  DMO(dpc,q) = sv;	                                    /* store new DMO(q) */
-		  dcv        = _mm_mul_ps(dcv, *tp);   tp++;            /* note, extend dcv, not DMO(q) */
+		  dcv        = vec_multiply4sp(dcv, *tp);   tp++;            /* note, extend dcv, not DMO(q) */
 		}	    
-	      if (! _mm_movemask_ps(cv)) break; /* DD's didn't change any DMO(q)? Then done, break out. */
+	      if (! vec_extractupperbit4sp(cv)) break; /* DD's didn't change any DMO(q)? Then done, break out. */
 	    }
 	}
 
       /* Add D's to xEv */
-      for (q = 0; q < Q; q++) xEv = _mm_add_ps(DMO(dpc,q), xEv);
+      for (q = 0; q < Q; q++) xEv = vec_add4sp(DMO(dpc,q), xEv);
 
       /* Finally the "special" states, which start from Mk->E (->C, ->J->B) */
       /* The following incantation is a horizontal sum of xEv's elements  */
       /* These must follow DD calculations, because D's contribute to E in Forward
        * (as opposed to Viterbi)
        */
-      xEv = _mm_add_ps(xEv, _mm_shuffle_ps(xEv, xEv, _MM_SHUFFLE(0, 3, 2, 1)));
-      xEv = _mm_add_ps(xEv, _mm_shuffle_ps(xEv, xEv, _MM_SHUFFLE(1, 0, 3, 2)));
-      _mm_store_ss(&xE, xEv);
+      xEv = vec_add4sp(xEv, vec_shufflepermute4sp(xEv, xEv, _MM_SHUFFLE(0, 3, 2, 1)));
+      xEv = vec_add4sp(xEv, vec_shufflepermute4sp(xEv, xEv, _MM_SHUFFLE(1, 0, 3, 2)));
+      vec_storeu4spto1sp(&xE, xEv);
 
       xN =  xN * om->xf[p7O_N][p7O_LOOP];
       xC = (xC * om->xf[p7O_C][p7O_LOOP]) +  (xE * om->xf[p7O_E][p7O_MOVE]);
@@ -423,12 +423,12 @@
 	  xC  = xC / xE;
 	  xJ  = xJ / xE;
 	  xB  = xB / xE;
-	  xEv = _mm_set1_ps(1.0 / xE);
+	  xEv = vec_splat4sp(1.0 / xE);
 	  for (q = 0; q < Q; q++)
 	    {
-	      MMO(dpc,q) = _mm_mul_ps(MMO(dpc,q), xEv);
-	      DMO(dpc,q) = _mm_mul_ps(DMO(dpc,q), xEv);
-	      IMO(dpc,q) = _mm_mul_ps(IMO(dpc,q), xEv);
+	      MMO(dpc,q) = vec_multiply4sp(MMO(dpc,q), xEv);
+	      DMO(dpc,q) = vec_multiply4sp(DMO(dpc,q), xEv);
+	      IMO(dpc,q) = vec_multiply4sp(IMO(dpc,q), xEv);
 	    }
 	  ox->xmx[i*p7X_NXCELLS+p7X_SCALE] = xE;
 	  ox->totscale += log(xE);
@@ -495,41 +495,41 @@
   xN     = 0.0;
   xC     = om->xf[p7O_C][p7O_MOVE];      /* C<-T */
   xE     = xC * om->xf[p7O_E][p7O_MOVE]; /* E<-C, no tail */
-  xEv    = _mm_set1_ps(xE); 
-  zerov  = _mm_setzero_ps();  
+  xEv    = vec_splat4sp(xE); 
+  zerov  = vec_zero4sp();  
   dcv    = zerov;		/* solely to silence a compiler warning */
   for (q = 0; q < Q; q++) MMO(dpc,q) = DMO(dpc,q) = xEv;
   for (q = 0; q < Q; q++) IMO(dpc,q) = zerov;
 
   /* init row L's DD paths, 1) first segment includes xE, from DMO(q) */
   tp  = om->tfv + 8*Q - 1;	                        /* <*tp> now the [4 8 12 x] TDD quad         */
-  dpv = _mm_move_ss(DMO(dpc,Q-1), zerov);               /* start leftshift: [1 5 9 13] -> [x 5 9 13] */
-  dpv = _mm_shuffle_ps(dpv, dpv, _MM_SHUFFLE(0,3,2,1)); /* finish leftshift:[x 5 9 13] -> [5 9 13 x] */
+  dpv = vec_insert1spintolower4sp(DMO(dpc,Q-1), zerov);               /* start leftshift: [1 5 9 13] -> [x 5 9 13] */
+  dpv = vec_shufflepermute4sp(dpv, dpv, _MM_SHUFFLE(0,3,2,1)); /* finish leftshift:[x 5 9 13] -> [5 9 13 x] */
   for (q = Q-1; q >= 0; q--)
     {
-      dcv        = _mm_mul_ps(dpv, *tp);      tp--;
-      DMO(dpc,q) = _mm_add_ps(DMO(dpc,q), dcv);
+      dcv        = vec_multiply4sp(dpv, *tp);      tp--;
+      DMO(dpc,q) = vec_add4sp(DMO(dpc,q), dcv);
       dpv        = DMO(dpc,q);
     }
   /* 2) three more passes, only extending DD component (dcv only; no xE contrib from DMO(q)) */
   for (j = 1; j < 4; j++)
     {
       tp  = om->tfv + 8*Q - 1;	                            /* <*tp> now the [4 8 12 x] TDD quad         */
-      dcv = _mm_move_ss(dcv, zerov);                        /* start leftshift: [1 5 9 13] -> [x 5 9 13] */
-      dcv = _mm_shuffle_ps(dcv, dcv, _MM_SHUFFLE(0,3,2,1)); /* finish leftshift:[x 5 9 13] -> [5 9 13 x] */
+      dcv = vec_insert1spintolower4sp(dcv, zerov);                        /* start leftshift: [1 5 9 13] -> [x 5 9 13] */
+      dcv = vec_shufflepermute4sp(dcv, dcv, _MM_SHUFFLE(0,3,2,1)); /* finish leftshift:[x 5 9 13] -> [5 9 13 x] */
       for (q = Q-1; q >= 0; q--)
 	{
-	  dcv        = _mm_mul_ps(dcv, *tp); tp--;
-	  DMO(dpc,q) = _mm_add_ps(DMO(dpc,q), dcv);
+	  dcv        = vec_multiply4sp(dcv, *tp); tp--;
+	  DMO(dpc,q) = vec_add4sp(DMO(dpc,q), dcv);
 	}
     }
   /* now MD init */
   tp  = om->tfv + 7*Q - 3;	                        /* <*tp> now the [4 8 12 x] Mk->Dk+1 quad    */
-  dcv = _mm_move_ss(DMO(dpc,0), zerov);                 /* start leftshift: [1 5 9 13] -> [x 5 9 13] */
-  dcv = _mm_shuffle_ps(dcv, dcv, _MM_SHUFFLE(0,3,2,1)); /* finish leftshift:[x 5 9 13] -> [5 9 13 x] */
+  dcv = vec_insert1spintolower4sp(DMO(dpc,0), zerov);                 /* start leftshift: [1 5 9 13] -> [x 5 9 13] */
+  dcv = vec_shufflepermute4sp(dcv, dcv, _MM_SHUFFLE(0,3,2,1)); /* finish leftshift:[x 5 9 13] -> [5 9 13 x] */
   for (q = Q-1; q >= 0; q--)
     {
-      MMO(dpc,q) = _mm_add_ps(MMO(dpc,q), _mm_mul_ps(dcv, *tp)); tp -= 7;
+      MMO(dpc,q) = vec_add4sp(MMO(dpc,q), vec_multiply4sp(dcv, *tp)); tp -= 7;
       dcv        = DMO(dpc,q);
     }
 
@@ -541,11 +541,11 @@
       xC  = xC / fwd->xmx[L*p7X_NXCELLS+p7X_SCALE];
       xJ  = xJ / fwd->xmx[L*p7X_NXCELLS+p7X_SCALE];
       xB  = xB / fwd->xmx[L*p7X_NXCELLS+p7X_SCALE];
-      xEv = _mm_set1_ps(1.0 / fwd->xmx[L*p7X_NXCELLS+p7X_SCALE]);
+      xEv = vec_splat4sp(1.0 / fwd->xmx[L*p7X_NXCELLS+p7X_SCALE]);
       for (q = 0; q < Q; q++) {
-	MMO(dpc,q) = _mm_mul_ps(MMO(dpc,q), xEv);
-	DMO(dpc,q) = _mm_mul_ps(DMO(dpc,q), xEv);
-	IMO(dpc,q) = _mm_mul_ps(IMO(dpc,q), xEv);
+	MMO(dpc,q) = vec_multiply4sp(MMO(dpc,q), xEv);
+	DMO(dpc,q) = vec_multiply4sp(DMO(dpc,q), xEv);
+	IMO(dpc,q) = vec_multiply4sp(IMO(dpc,q), xEv);
       }
     }
   bck->xmx[L*p7X_NXCELLS+p7X_SCALE] = fwd->xmx[L*p7X_NXCELLS+p7X_SCALE];
@@ -574,79 +574,79 @@
       tp  = om->tfv + 7*Q - 1;	     /* <*tp> is now the [4 8 12 x] TII transition quad  */
 
       /* leftshift the first transition quads */
-      tmmv = _mm_move_ss(om->tfv[1], zerov); tmmv = _mm_shuffle_ps(tmmv, tmmv, _MM_SHUFFLE(0,3,2,1));
-      timv = _mm_move_ss(om->tfv[2], zerov); timv = _mm_shuffle_ps(timv, timv, _MM_SHUFFLE(0,3,2,1));
-      tdmv = _mm_move_ss(om->tfv[3], zerov); tdmv = _mm_shuffle_ps(tdmv, tdmv, _MM_SHUFFLE(0,3,2,1));
+      tmmv = vec_insert1spintolower4sp(om->tfv[1], zerov); tmmv = vec_shufflepermute4sp(tmmv, tmmv, _MM_SHUFFLE(0,3,2,1));
+      timv = vec_insert1spintolower4sp(om->tfv[2], zerov); timv = vec_shufflepermute4sp(timv, timv, _MM_SHUFFLE(0,3,2,1));
+      tdmv = vec_insert1spintolower4sp(om->tfv[3], zerov); tdmv = vec_shufflepermute4sp(tdmv, tdmv, _MM_SHUFFLE(0,3,2,1));
 
-      mpv = _mm_mul_ps(MMO(dpp,0), om->rfv[dsq[i+1]][0]); /* precalc M(i+1,k+1) * e(M_k+1, x_{i+1}) */
-      mpv = _mm_move_ss(mpv, zerov);
-      mpv = _mm_shuffle_ps(mpv, mpv, _MM_SHUFFLE(0,3,2,1));
+      mpv = vec_multiply4sp(MMO(dpp,0), om->rfv[dsq[i+1]][0]); /* precalc M(i+1,k+1) * e(M_k+1, x_{i+1}) */
+      mpv = vec_insert1spintolower4sp(mpv, zerov);
+      mpv = vec_shufflepermute4sp(mpv, mpv, _MM_SHUFFLE(0,3,2,1));
 
       xBv = zerov;
       for (q = Q-1; q >= 0; q--)     /* backwards stride */
 	{
 	  ipv = IMO(dpp,q); /* assumes emission odds ratio of 1.0; i+1's IMO(q) now free */
-	  IMO(dpc,q) = _mm_add_ps(_mm_mul_ps(ipv, *tp), _mm_mul_ps(mpv, timv));   tp--;
-	  DMO(dpc,q) =                                  _mm_mul_ps(mpv, tdmv); 
-	  mcv        = _mm_add_ps(_mm_mul_ps(ipv, *tp), _mm_mul_ps(mpv, tmmv));   tp-= 2;
+	  IMO(dpc,q) = vec_add4sp(vec_multiply4sp(ipv, *tp), vec_multiply4sp(mpv, timv));   tp--;
+	  DMO(dpc,q) =                                  vec_multiply4sp(mpv, tdmv); 
+	  mcv        = vec_add4sp(vec_multiply4sp(ipv, *tp), vec_multiply4sp(mpv, tmmv));   tp-= 2;
 	  
-	  mpv        = _mm_mul_ps(MMO(dpp,q), *rp);  rp--;  /* obtain mpv for next q. i+1's MMO(q) is freed  */
+	  mpv        = vec_multiply4sp(MMO(dpp,q), *rp);  rp--;  /* obtain mpv for next q. i+1's MMO(q) is freed  */
 	  MMO(dpc,q) = mcv;
 
 	  tdmv = *tp;   tp--;
 	  timv = *tp;   tp--;
 	  tmmv = *tp;   tp--;
 
-	  xBv = _mm_add_ps(xBv, _mm_mul_ps(mpv, *tp)); tp--;
+	  xBv = vec_add4sp(xBv, vec_multiply4sp(mpv, *tp)); tp--;
 	}
 
       /* phase 2: now that we have accumulated the B->Mk transitions in xBv, we can do the specials */
-      /* this incantation is a horiz sum of xBv elements: (_mm_hadd_ps() would require SSE3) */
-      xBv = _mm_add_ps(xBv, _mm_shuffle_ps(xBv, xBv, _MM_SHUFFLE(0, 3, 2, 1)));
-      xBv = _mm_add_ps(xBv, _mm_shuffle_ps(xBv, xBv, _MM_SHUFFLE(1, 0, 3, 2)));
-      _mm_store_ss(&xB, xBv);
+      /* this incantation is a horiz sum of xBv elements: (vec_horizontaladd4sp() would require SSE3) */
+      xBv = vec_add4sp(xBv, vec_shufflepermute4sp(xBv, xBv, _MM_SHUFFLE(0, 3, 2, 1)));
+      xBv = vec_add4sp(xBv, vec_shufflepermute4sp(xBv, xBv, _MM_SHUFFLE(1, 0, 3, 2)));
+      vec_storeu4spto1sp(&xB, xBv);
 
       xC =  xC * om->xf[p7O_C][p7O_LOOP];
       xJ = (xB * om->xf[p7O_J][p7O_MOVE]) + (xJ * om->xf[p7O_J][p7O_LOOP]); /* must come after xB */
       xN = (xB * om->xf[p7O_N][p7O_MOVE]) + (xN * om->xf[p7O_N][p7O_LOOP]); /* must come after xB */
       xE = (xC * om->xf[p7O_E][p7O_MOVE]) + (xJ * om->xf[p7O_E][p7O_LOOP]); /* must come after xJ, xC */
-      xEv = _mm_set1_ps(xE);	/* splat */
+      xEv = vec_splat4sp(xE);	/* splat */
 
 
       /* phase 3: {MD}->E paths and one step of the D->D paths */
       tp  = om->tfv + 8*Q - 1;	/* <*tp> now the [4 8 12 x] TDD quad */
-      dpv = _mm_add_ps(DMO(dpc,0), xEv);
-      dpv = _mm_move_ss(dpv, zerov);
-      dpv = _mm_shuffle_ps(dpv, dpv, _MM_SHUFFLE(0,3,2,1));
+      dpv = vec_add4sp(DMO(dpc,0), xEv);
+      dpv = vec_insert1spintolower4sp(dpv, zerov);
+      dpv = vec_shufflepermute4sp(dpv, dpv, _MM_SHUFFLE(0,3,2,1));
       for (q = Q-1; q >= 0; q--)
 	{
-	  dcv        = _mm_mul_ps(dpv, *tp); tp--;
-	  DMO(dpc,q) = _mm_add_ps(DMO(dpc,q), _mm_add_ps(dcv, xEv));
+	  dcv        = vec_multiply4sp(dpv, *tp); tp--;
+	  DMO(dpc,q) = vec_add4sp(DMO(dpc,q), vec_add4sp(dcv, xEv));
 	  dpv        = DMO(dpc,q);
-	  MMO(dpc,q) = _mm_add_ps(MMO(dpc,q), xEv);
+	  MMO(dpc,q) = vec_add4sp(MMO(dpc,q), xEv);
 	}
       
       /* phase 4: finish extending the DD paths */
       /* fully serialized for now */
       for (j = 1; j < 4; j++)	/* three passes: we've already done 1 segment, we need 4 total */
 	{
-	  dcv = _mm_move_ss(dcv, zerov);
-	  dcv = _mm_shuffle_ps(dcv, dcv, _MM_SHUFFLE(0,3,2,1));
+	  dcv = vec_insert1spintolower4sp(dcv, zerov);
+	  dcv = vec_shufflepermute4sp(dcv, dcv, _MM_SHUFFLE(0,3,2,1));
 	  tp  = om->tfv + 8*Q - 1;	/* <*tp> now the [4 8 12 x] TDD quad */
 	  for (q = Q-1; q >= 0; q--)
 	    {
-	      dcv        = _mm_mul_ps(dcv, *tp); tp--;
-	      DMO(dpc,q) = _mm_add_ps(DMO(dpc,q), dcv);
+	      dcv        = vec_multiply4sp(dcv, *tp); tp--;
+	      DMO(dpc,q) = vec_add4sp(DMO(dpc,q), dcv);
 	    }
 	}
 
       /* phase 5: add M->D paths */
-      dcv = _mm_move_ss(DMO(dpc,0), zerov);
-      dcv = _mm_shuffle_ps(dcv, dcv, _MM_SHUFFLE(0,3,2,1));
+      dcv = vec_insert1spintolower4sp(DMO(dpc,0), zerov);
+      dcv = vec_shufflepermute4sp(dcv, dcv, _MM_SHUFFLE(0,3,2,1));
       tp  = om->tfv + 7*Q - 3;	/* <*tp> is now the [4 8 12 x] Mk->Dk+1 quad */
       for (q = Q-1; q >= 0; q--)
 	{
-	  MMO(dpc,q) = _mm_add_ps(MMO(dpc,q), _mm_mul_ps(dcv, *tp)); tp -= 7;
+	  MMO(dpc,q) = vec_add4sp(MMO(dpc,q), vec_multiply4sp(dcv, *tp)); tp -= 7;
 	  dcv        = DMO(dpc,q);
 	}
 
@@ -670,11 +670,11 @@
 	  xJ /= bck->xmx[i*p7X_NXCELLS+p7X_SCALE];
 	  xB /= bck->xmx[i*p7X_NXCELLS+p7X_SCALE];
 	  xC /= bck->xmx[i*p7X_NXCELLS+p7X_SCALE];
-	  xBv = _mm_set1_ps(1.0 / bck->xmx[i*p7X_NXCELLS+p7X_SCALE]);
+	  xBv = vec_splat4sp(1.0 / bck->xmx[i*p7X_NXCELLS+p7X_SCALE]);
 	  for (q = 0; q < Q; q++) {
-	    MMO(dpc,q) = _mm_mul_ps(MMO(dpc,q), xBv);
-	    DMO(dpc,q) = _mm_mul_ps(DMO(dpc,q), xBv);
-	    IMO(dpc,q) = _mm_mul_ps(IMO(dpc,q), xBv);
+	    MMO(dpc,q) = vec_multiply4sp(MMO(dpc,q), xBv);
+	    DMO(dpc,q) = vec_multiply4sp(DMO(dpc,q), xBv);
+	    IMO(dpc,q) = vec_multiply4sp(IMO(dpc,q), xBv);
 	  }
 	  bck->totscale += log(bck->xmx[i*p7X_NXCELLS+p7X_SCALE]);
 	}
@@ -701,14 +701,14 @@
   xBv = zerov;
   for (q = 0; q < Q; q++)
     {
-      mpv = _mm_mul_ps(MMO(dpp,q), *rp);  rp++;
-      mpv = _mm_mul_ps(mpv,        *tp);  tp += 7;
-      xBv = _mm_add_ps(xBv,        mpv);
+      mpv = vec_multiply4sp(MMO(dpp,q), *rp);  rp++;
+      mpv = vec_multiply4sp(mpv,        *tp);  tp += 7;
+      xBv = vec_add4sp(xBv,        mpv);
     }
   /* horizontal sum of xBv */
-  xBv = _mm_add_ps(xBv, _mm_shuffle_ps(xBv, xBv, _MM_SHUFFLE(0, 3, 2, 1)));
-  xBv = _mm_add_ps(xBv, _mm_shuffle_ps(xBv, xBv, _MM_SHUFFLE(1, 0, 3, 2)));
-  _mm_store_ss(&xB, xBv);
+  xBv = vec_add4sp(xBv, vec_shufflepermute4sp(xBv, xBv, _MM_SHUFFLE(0, 3, 2, 1)));
+  xBv = vec_add4sp(xBv, vec_shufflepermute4sp(xBv, xBv, _MM_SHUFFLE(1, 0, 3, 2)));
+  vec_storeu4spto1sp(&xB, xBv);
  
   xN = (xB * om->xf[p7O_N][p7O_MOVE]) + (xN * om->xf[p7O_N][p7O_LOOP]);  
 
--- hmmer/src/impl_sse/impl_sse.h
+++ hmmer/src/impl_sse/impl_sse.h
@@ -12,11 +12,10 @@
 #include "esl_alphabet.h"
 #include "esl_random.h"
 
-#include <xmmintrin.h>    /* SSE  */
-#include <emmintrin.h>    /* SSE2 */
-#ifdef __SSE3__
-#include <pmmintrin.h>   /* DENORMAL_MODE */
-#endif
+#include <vec128int.h>
+#include <vec128sp.h>
+#include <vecmisc.h>
+
 #include "hmmer.h"
 
 /* In calculating Q, the number of vectors we need in a row, we have
@@ -361,7 +360,7 @@
    * values in the floating point calculations, set the processor flag
    * so sub-normals are "flushed" immediately to zero.
    */
-  _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);
+  vec_setfpflushtozeromode(0);
 #endif
 
 #ifdef _PMMINTRIN_H_INCLUDED
--- hmmer/src/impl_sse/io.c
+++ hmmer/src/impl_sse/io.c
@@ -37,8 +37,8 @@
 #include <pthread.h>
 #endif
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 
--- hmmer/src/impl_sse/mpi.c
+++ hmmer/src/impl_sse/mpi.c
@@ -19,8 +19,8 @@
 
 #include "mpi.h"
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_mpi.h"
--- hmmer/src/impl_sse/msvfilter.c
+++ hmmer/src/impl_sse/msvfilter.c
@@ -22,8 +22,8 @@
 #include <stdio.h>
 #include <math.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sse.h"
@@ -107,26 +107,26 @@
 
   /* Initialization. In offset unsigned arithmetic, -infinity is 0, and 0 is om->base.
    */
-  biasv = _mm_set1_epi8((int8_t) om->bias_b); /* yes, you can set1() an unsigned char vector this way */
-  for (q = 0; q < Q; q++) dp[q] = _mm_setzero_si128();
+  biasv = vec_splat16sb((int8_t) om->bias_b); /* yes, you can set1() an unsigned char vector this way */
+  for (q = 0; q < Q; q++) dp[q] = vec_zero1q();
   xJ   = 0;
 
   /* saturate simd register for overflow test */
-  ceilingv = _mm_cmpeq_epi8(biasv, biasv);
-  basev = _mm_set1_epi8((int8_t) om->base_b);
+  ceilingv = vec_compareeq16sb(biasv, biasv);
+  basev = vec_splat16sb((int8_t) om->base_b);
 
-  tjbmv = _mm_set1_epi8((int8_t) om->tjb_b + (int8_t) om->tbm_b);
-  tecv = _mm_set1_epi8((int8_t) om->tec_b);
+  tjbmv = vec_splat16sb((int8_t) om->tjb_b + (int8_t) om->tbm_b);
+  tecv = vec_splat16sb((int8_t) om->tec_b);
 
-  xJv = _mm_subs_epu8(biasv, biasv);
-  xBv = _mm_subs_epu8(basev, tjbmv);
+  xJv = vec_subtractsaturating16ub(biasv, biasv);
+  xBv = vec_subtractsaturating16ub(basev, tjbmv);
 
 #if p7_DEBUGGING
   if (ox->debugging)
   {
       uint8_t xB;
-      xB = _mm_extract_epi16(xBv, 0);
-      xJ = _mm_extract_epi16(xJv, 0);
+      xB = vec_extract8sh(xBv, 0);
+      xJ = vec_extract8sh(xJv, 0);
       p7_omx_DumpMFRow(ox, 0, 0, 0, xJ, xB, xJ);
   }
 #endif
@@ -134,29 +134,29 @@
   for (i = 1; i <= L; i++)
   {
       rsc = om->rbv[dsq[i]];
-      xEv = _mm_setzero_si128();      
+      xEv = vec_zero1q();      
 
       /* Right shifts by 1 byte. 4,8,12,x becomes x,4,8,12. 
        * Because ia32 is littlendian, this means a left bit shift.
        * Zeros shift on automatically, which is our -infinity.
        */
-      mpv = _mm_slli_si128(dp[Q-1], 1);   
+      mpv = vec_shiftleftbytes1q(dp[Q-1], 1);   
       for (q = 0; q < Q; q++)
       {
         /* Calculate new MMXo(i,q); don't store it yet, hold it in sv. */
-        sv   = _mm_max_epu8(mpv, xBv);
-        sv   = _mm_adds_epu8(sv, biasv);
-        sv   = _mm_subs_epu8(sv, *rsc);   rsc++;
-        xEv  = _mm_max_epu8(xEv, sv);
+        sv   = vec_max16ub(mpv, xBv);
+        sv   = vec_addsaturating16ub(sv, biasv);
+        sv   = vec_subtractsaturating16ub(sv, *rsc);   rsc++;
+        xEv  = vec_max16ub(xEv, sv);
 
         mpv   = dp[q];   	  /* Load {MDI}(i-1,q) into mpv */
         dp[q] = sv;       	  /* Do delayed store of M(i,q) now that memory is usable */
       }
 
       /* test for the overflow condition */
-      tempv = _mm_adds_epu8(xEv, biasv);
-      tempv = _mm_cmpeq_epi8(tempv, ceilingv);
-      cmp = _mm_movemask_epi8(tempv);
+      tempv = vec_addsaturating16ub(xEv, biasv);
+      tempv = vec_compareeq16sb(tempv, ceilingv);
+      cmp = vec_extractupperbit16sb(tempv);
 
       /* Now the "special" states, which start from Mk->E (->C, ->J->B)
        * Use shuffles instead of shifts so when the last max has completed,
@@ -164,15 +164,15 @@
        * max value.  Then the last shuffle will broadcast the max value
        * to all simd elements.
        */
-      tempv = _mm_shuffle_epi32(xEv, _MM_SHUFFLE(2, 3, 0, 1));
-      xEv = _mm_max_epu8(xEv, tempv);
-      tempv = _mm_shuffle_epi32(xEv, _MM_SHUFFLE(0, 1, 2, 3));
-      xEv = _mm_max_epu8(xEv, tempv);
-      tempv = _mm_shufflelo_epi16(xEv, _MM_SHUFFLE(2, 3, 0, 1));
-      xEv = _mm_max_epu8(xEv, tempv);
-      tempv = _mm_srli_si128(xEv, 1);
-      xEv = _mm_max_epu8(xEv, tempv);
-      xEv = _mm_shuffle_epi32(xEv, _MM_SHUFFLE(0, 0, 0, 0));
+      tempv = vec_permute4sw(xEv, _MM_SHUFFLE(2, 3, 0, 1));
+      xEv = vec_max16ub(xEv, tempv);
+      tempv = vec_permute4sw(xEv, _MM_SHUFFLE(0, 1, 2, 3));
+      xEv = vec_max16ub(xEv, tempv);
+      tempv = vec_permutelower4sh(xEv, _MM_SHUFFLE(2, 3, 0, 1));
+      xEv = vec_max16ub(xEv, tempv);
+      tempv = vec_shiftrightbytes1q(xEv, 1);
+      xEv = vec_max16ub(xEv, tempv);
+      xEv = vec_permute4sw(xEv, _MM_SHUFFLE(0, 0, 0, 0));
 
       /* immediately detect overflow */
       if (cmp != 0x0000)
@@ -181,25 +181,25 @@
         return eslERANGE;
       }
 
-      xEv = _mm_subs_epu8(xEv, tecv);
-      xJv = _mm_max_epu8(xJv,xEv);
+      xEv = vec_subtractsaturating16ub(xEv, tecv);
+      xJv = vec_max16ub(xJv,xEv);
       
-      xBv = _mm_max_epu8(basev, xJv);
-      xBv = _mm_subs_epu8(xBv, tjbmv);
+      xBv = vec_max16ub(basev, xJv);
+      xBv = vec_subtractsaturating16ub(xBv, tjbmv);
 	  
 #if p7_DEBUGGING
       if (ox->debugging)
       {
         uint8_t xB, xE;
-        xB = _mm_extract_epi16(xBv, 0);
-        xE = _mm_extract_epi16(xEv, 0);
-        xJ = _mm_extract_epi16(xJv, 0);
+        xB = vec_extract8sh(xBv, 0);
+        xE = vec_extract8sh(xEv, 0);
+        xJ = vec_extract8sh(xJv, 0);
         p7_omx_DumpMFRow(ox, i, xE, 0, xJ, xB, xJ);
       }
 #endif
   } /* end loop over sequence residues 1..L */
 
-  xJ = (uint8_t) _mm_extract_epi16(xJv, 0);
+  xJ = (uint8_t) vec_extract8sh(xJv, 0);
 
   /* finally C->T, and add our missing precision on the NN,CC,JJ back */
   *ret_sc = ((float) (xJ - om->tjb_b) - (float) om->base_b);
@@ -324,44 +324,44 @@
   p7_bg_NullOne  (bg, dsq, om->max_length, &nullsc);
 
   sc_thresh = (int) ceil( ( ( nullsc  + (invP * eslCONST_LOG2) + 3.0 )  * om->scale_b ) + om->base_b +  om->tec_b  + om->tjb_b );
-  sc_threshv = _mm_set1_epi8((int8_t) 255 - sc_thresh);
+  sc_threshv = vec_splat16sb((int8_t) 255 - sc_thresh);
 
   /* Initialization. In offset unsigned  arithmetic, -infinity is 0, and 0 is om->base.
    */
-  biasv = _mm_set1_epi8((int8_t) om->bias_b); /* yes, you can set1() an unsigned char vector this way */
-  ceilingv = _mm_cmpeq_epi8(biasv, biasv);
-  for (q = 0; q < Q; q++) dp[q] = _mm_setzero_si128();
+  biasv = vec_splat16sb((int8_t) om->bias_b); /* yes, you can set1() an unsigned char vector this way */
+  ceilingv = vec_compareeq16sb(biasv, biasv);
+  for (q = 0; q < Q; q++) dp[q] = vec_zero1q();
 
-  basev = _mm_set1_epi8((int8_t) om->base_b);
-  tjbmv = _mm_set1_epi8((int8_t) om->tjb_b + (int8_t) om->tbm_b);
+  basev = vec_splat16sb((int8_t) om->base_b);
+  tjbmv = vec_splat16sb((int8_t) om->tjb_b + (int8_t) om->tbm_b);
 
-  xBv = _mm_subs_epu8(basev, tjbmv);
+  xBv = vec_subtractsaturating16ub(basev, tjbmv);
 
   for (i = 1; i <= L; i++) {
     rsc = om->rbv[dsq[i]];
-    xEv = _mm_setzero_si128();
+    xEv = vec_zero1q();
 
 	  /* Right shifts by 1 byte. 4,8,12,x becomes x,4,8,12.
 	   * Because ia32 is littlendian, this means a left bit shift.
 	   * Zeros shift on automatically, which is our -infinity.
 	   */
-	  mpv = _mm_slli_si128(dp[Q-1], 1);
+	  mpv = vec_shiftleftbytes1q(dp[Q-1], 1);
 	  for (q = 0; q < Q; q++) {
 		  /* Calculate new MMXo(i,q); don't store it yet, hold it in sv. */
-		  sv   = _mm_max_epu8(mpv, xBv);
-		  sv   = _mm_adds_epu8(sv, biasv);
-		  sv   = _mm_subs_epu8(sv, *rsc);   rsc++;
-		  xEv  = _mm_max_epu8(xEv, sv);
+		  sv   = vec_max16ub(mpv, xBv);
+		  sv   = vec_addsaturating16ub(sv, biasv);
+		  sv   = vec_subtractsaturating16ub(sv, *rsc);   rsc++;
+		  xEv  = vec_max16ub(xEv, sv);
 
 		  mpv   = dp[q];   	  /* Load {MDI}(i-1,q) into mpv */
 		  dp[q] = sv;       	  /* Do delayed store of M(i,q) now that memory is usable */
 	  }
 
 	  /* test if the pthresh significance threshold has been reached;
-	   * note: don't use _mm_cmpgt_epi8, because it's a signed comparison, which won't work on uint8s */
-	  tempv = _mm_adds_epu8(xEv, sc_threshv);
-	  tempv = _mm_cmpeq_epi8(tempv, ceilingv);
-	  cmp = _mm_movemask_epi8(tempv);
+	   * note: don't use vec_comparegt16sb, because it's a signed comparison, which won't work on uint8s */
+	  tempv = vec_addsaturating16ub(xEv, sc_threshv);
+	  tempv = vec_compareeq16sb(tempv, ceilingv);
+	  cmp = vec_extractupperbit16sb(tempv);
 
 	  if (cmp != 0) {  //hit pthresh, so add position to list and reset values
 	    //figure out which model state hit threshold
@@ -376,7 +376,7 @@
               rem_sc = u.b[k];
             }
           }
-          dp[q] = _mm_set1_epi8(0); // while we're here ... this will cause values to get reset to xB in next dp iteration
+          dp[q] = vec_splat16sb(0); // while we're here ... this will cause values to get reset to xB in next dp iteration
 	    }
 
 	    //recover the diagonal that hit threshold
--- hmmer/src/impl_sse/null2.c
+++ hmmer/src/impl_sse/null2.c
@@ -16,8 +16,8 @@
 #include <stdlib.h>
 #include <string.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sse.h"
@@ -69,8 +69,8 @@
     {
       for (q = 0; q < Q; q++)
 	{
-	  pp->dpf[0][q*3 + p7X_M] = _mm_add_ps(pp->dpf[i][q*3 + p7X_M], pp->dpf[0][q*3 + p7X_M]);
-	  pp->dpf[0][q*3 + p7X_I] = _mm_add_ps(pp->dpf[i][q*3 + p7X_I], pp->dpf[0][q*3 + p7X_I]);
+	  pp->dpf[0][q*3 + p7X_M] = vec_add4sp(pp->dpf[i][q*3 + p7X_M], pp->dpf[0][q*3 + p7X_M]);
+	  pp->dpf[0][q*3 + p7X_I] = vec_add4sp(pp->dpf[i][q*3 + p7X_I], pp->dpf[0][q*3 + p7X_I]);
 	}
       XMXo(0,p7X_N) += XMXo(i,p7X_N);
       XMXo(0,p7X_C) += XMXo(i,p7X_C); 
@@ -79,11 +79,11 @@
 
   /* Convert those expected #'s to frequencies, to use as posterior weights. */
   norm = 1.0 / (float) Ld;
-  sv   = _mm_set1_ps(norm);
+  sv   = vec_splat4sp(norm);
   for (q = 0; q < Q; q++)
     {
-      pp->dpf[0][q*3 + p7X_M] = _mm_mul_ps(pp->dpf[0][q*3 + p7X_M], sv);
-      pp->dpf[0][q*3 + p7X_I] = _mm_mul_ps(pp->dpf[0][q*3 + p7X_I], sv);
+      pp->dpf[0][q*3 + p7X_M] = vec_multiply4sp(pp->dpf[0][q*3 + p7X_M], sv);
+      pp->dpf[0][q*3 + p7X_I] = vec_multiply4sp(pp->dpf[0][q*3 + p7X_I], sv);
     }
   XMXo(0,p7X_N) *= norm;
   XMXo(0,p7X_C) *= norm;
@@ -95,13 +95,13 @@
   xfactor = XMXo(0, p7X_N) + XMXo(0, p7X_C) + XMXo(0, p7X_J); 
   for (x = 0; x < om->abc->K; x++)
     {
-      sv = _mm_setzero_ps();
+      sv = vec_zero4sp();
       rp = om->rfv[x];
       for (q = 0; q < Q; q++)
 	{
-	  sv = _mm_add_ps(sv, _mm_mul_ps(pp->dpf[0][q*3 + p7X_M], *rp)); rp++;
-	  sv = _mm_add_ps(sv,            pp->dpf[0][q*3 + p7X_I]);              /* insert odds implicitly 1.0 */
-	  //	  sv = _mm_add_ps(sv, _mm_mul_ps(pp->dpf[0][q*3 + p7X_I], *rp)); rp++; 
+	  sv = vec_add4sp(sv, vec_multiply4sp(pp->dpf[0][q*3 + p7X_M], *rp)); rp++;
+	  sv = vec_add4sp(sv,            pp->dpf[0][q*3 + p7X_I]);              /* insert odds implicitly 1.0 */
+	  //	  sv = vec_add4sp(sv, vec_multiply4sp(pp->dpf[0][q*3 + p7X_I], *rp)); rp++; 
 	}
       esl_sse_hsum_ps(sv, &(null2[x]));
       null2[x] += xfactor;
@@ -148,8 +148,8 @@
   /* We'll use the i=0 row in wrk for working space: dp[0][] and xmx[][0]. */
   for (q = 0; q < Q; q++)
     {
-      wrk->dpf[0][q*3 + p7X_M] = _mm_setzero_ps();
-      wrk->dpf[0][q*3 + p7X_I] = _mm_setzero_ps();
+      wrk->dpf[0][q*3 + p7X_M] = vec_zero4sp();
+      wrk->dpf[0][q*3 + p7X_I] = vec_zero4sp();
     }
   XMXo(0,p7X_N) =  0.0;
   XMXo(0,p7X_C) =  0.0;
@@ -180,11 +180,11 @@
 	}
     }
   norm = 1.0 / (float) Ld;
-  sv = _mm_set1_ps(norm);
+  sv = vec_splat4sp(norm);
   for (q = 0; q < Q; q++)
     {
-      wrk->dpf[0][q*3 + p7X_M] = _mm_mul_ps(wrk->dpf[0][q*3 + p7X_M], sv);
-      wrk->dpf[0][q*3 + p7X_I] = _mm_mul_ps(wrk->dpf[0][q*3 + p7X_I], sv);
+      wrk->dpf[0][q*3 + p7X_M] = vec_multiply4sp(wrk->dpf[0][q*3 + p7X_M], sv);
+      wrk->dpf[0][q*3 + p7X_I] = vec_multiply4sp(wrk->dpf[0][q*3 + p7X_I], sv);
     }
   XMXo(0,p7X_N) *= norm;
   XMXo(0,p7X_C) *= norm;
@@ -196,13 +196,13 @@
   xfactor =  XMXo(0,p7X_N) + XMXo(0,p7X_C) + XMXo(0,p7X_J);
   for (x = 0; x < om->abc->K; x++)
     {
-      sv = _mm_setzero_ps();
+      sv = vec_zero4sp();
       rp = om->rfv[x];
       for (q = 0; q < Q; q++)
 	{
-	  sv = _mm_add_ps(sv, _mm_mul_ps(wrk->dpf[0][q*3 + p7X_M], *rp)); rp++;
-	  sv = _mm_add_ps(sv,            wrk->dpf[0][q*3 + p7X_I]); /* insert emission odds implicitly 1.0 */
-	  //	  sv = _mm_add_ps(sv, _mm_mul_ps(wrk->dpf[0][q*3 + p7X_I], *rp)); rp++;
+	  sv = vec_add4sp(sv, vec_multiply4sp(wrk->dpf[0][q*3 + p7X_M], *rp)); rp++;
+	  sv = vec_add4sp(sv,            wrk->dpf[0][q*3 + p7X_I]); /* insert emission odds implicitly 1.0 */
+	  //	  sv = vec_add4sp(sv, vec_multiply4sp(wrk->dpf[0][q*3 + p7X_I], *rp)); rp++;
 	}
       esl_sse_hsum_ps(sv, &(null2[x]));
       null2[x] += xfactor;
--- hmmer/src/impl_sse/optacc.c
+++ hmmer/src/impl_sse/optacc.c
@@ -16,8 +16,8 @@
 
 #include <float.h>
 
-#include <xmmintrin.h>
-#include <emmintrin.h>
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sse.h"
@@ -69,8 +69,8 @@
   __m128 *dpp;                     /* previous row, for use in {MDI}MO(dpp,q) access macro      */
   __m128 *ppp;			   /* quads in the <pp> posterior probability matrix            */
   __m128 *tp;			   /* quads in the <om->tfv> transition scores                  */
-  __m128 zerov = _mm_setzero_ps();
-  __m128 infv  = _mm_set1_ps(-eslINFINITY);
+  __m128 zerov = vec_zero4sp();
+  __m128 infv  = vec_splat4sp(-eslINFINITY);
   int M = om->M;
   int Q = p7O_NQF(M);
   int q;
@@ -95,19 +95,19 @@
       tp  = om->tfv;		/* transition probabilities */
       dcv = infv;
       xEv = infv;
-      xBv = _mm_set1_ps(XMXo(i-1, p7X_B));
+      xBv = vec_splat4sp(XMXo(i-1, p7X_B));
 
       mpv = esl_sse_rightshift_ps(MMO(dpp,Q-1), infv);  /* Right shifts by 4 bytes. 4,8,12,x becomes x,4,8,12. */
       dpv = esl_sse_rightshift_ps(DMO(dpp,Q-1), infv);
       ipv = esl_sse_rightshift_ps(IMO(dpp,Q-1), infv);
       for (q = 0; q < Q; q++)
 	{
-	  sv  =                _mm_and_ps(_mm_cmpgt_ps(*tp, zerov), xBv);  tp++;
-	  sv  = _mm_max_ps(sv, _mm_and_ps(_mm_cmpgt_ps(*tp, zerov), mpv)); tp++;
-	  sv  = _mm_max_ps(sv, _mm_and_ps(_mm_cmpgt_ps(*tp, zerov), ipv)); tp++;
-	  sv  = _mm_max_ps(sv, _mm_and_ps(_mm_cmpgt_ps(*tp, zerov), dpv)); tp++;
-	  sv  = _mm_add_ps(sv, *ppp);                                      ppp += 2;
-	  xEv = _mm_max_ps(xEv, sv);
+	  sv  =                vec_bitwiseand4sp(vec_comparegt4sp(*tp, zerov), xBv);  tp++;
+	  sv  = vec_max4sp(sv, vec_bitwiseand4sp(vec_comparegt4sp(*tp, zerov), mpv)); tp++;
+	  sv  = vec_max4sp(sv, vec_bitwiseand4sp(vec_comparegt4sp(*tp, zerov), ipv)); tp++;
+	  sv  = vec_max4sp(sv, vec_bitwiseand4sp(vec_comparegt4sp(*tp, zerov), dpv)); tp++;
+	  sv  = vec_add4sp(sv, *ppp);                                      ppp += 2;
+	  xEv = vec_max4sp(xEv, sv);
 	  
 	  mpv = MMO(dpp,q);
 	  dpv = DMO(dpp,q);
@@ -116,11 +116,11 @@
 	  MMO(dpc,q) = sv;
 	  DMO(dpc,q) = dcv;
 
-	  dcv = _mm_and_ps(_mm_cmpgt_ps(*tp, zerov), sv); tp++;
+	  dcv = vec_bitwiseand4sp(vec_comparegt4sp(*tp, zerov), sv); tp++;
 
-	  sv         =                _mm_and_ps(_mm_cmpgt_ps(*tp, zerov), mpv);   tp++;
-	  sv         = _mm_max_ps(sv, _mm_and_ps(_mm_cmpgt_ps(*tp, zerov), ipv));  tp++;
-	  IMO(dpc,q) = _mm_add_ps(sv, *ppp);                                       ppp++;
+	  sv         =                vec_bitwiseand4sp(vec_comparegt4sp(*tp, zerov), mpv);   tp++;
+	  sv         = vec_max4sp(sv, vec_bitwiseand4sp(vec_comparegt4sp(*tp, zerov), ipv));  tp++;
+	  IMO(dpc,q) = vec_add4sp(sv, *ppp);                                       ppp++;
 	}
       
       /* dcv has carried through from end of q loop above; store it 
@@ -130,8 +130,8 @@
       tp  = om->tfv + 7*Q;	/* set tp to start of the DD's */
       for (q = 0; q < Q; q++)
 	{
-	  DMO(dpc, q) = _mm_max_ps(dcv, DMO(dpc, q));
-	  dcv         = _mm_and_ps(_mm_cmpgt_ps(*tp, zerov), DMO(dpc,q));   tp++;
+	  DMO(dpc, q) = vec_max4sp(dcv, DMO(dpc, q));
+	  dcv         = vec_bitwiseand4sp(vec_comparegt4sp(*tp, zerov), DMO(dpc,q));   tp++;
 	}
 
       /* fully serialized D->D; can optimize later */
@@ -141,13 +141,13 @@
 	  tp  = om->tfv + 7*Q;	
 	  for (q = 0; q < Q; q++)
 	    {
-	      DMO(dpc, q) = _mm_max_ps(dcv, DMO(dpc, q));
-	      dcv         = _mm_and_ps(_mm_cmpgt_ps(*tp, zerov), dcv);   tp++;
+	      DMO(dpc, q) = vec_max4sp(dcv, DMO(dpc, q));
+	      dcv         = vec_bitwiseand4sp(vec_comparegt4sp(*tp, zerov), dcv);   tp++;
 	    }
 	}
 
       /* D->E paths */
-      for (q = 0; q < Q; q++) xEv = _mm_max_ps(xEv, DMO(dpc,q));
+      for (q = 0; q < Q; q++) xEv = vec_max4sp(xEv, DMO(dpc,q));
       
       /* Specials */
       esl_sse_hmax_ps(xEv, &(XMXo(i,p7X_E)));
@@ -290,8 +290,8 @@
   int     q     = (k-1) % Q;		/* (q,r) is position of the current DP cell M(i,k) */
   int     r     = (k-1) / Q;
   __m128 *tp    = om->tfv + 7*q;       	/* *tp now at start of transitions to cur cell M(i,k) */
-  __m128  xBv   = _mm_set1_ps(ox->xmx[(i-1)*p7X_NXCELLS+p7X_B]);
-  __m128  zerov = _mm_setzero_ps();
+  __m128  xBv   = vec_splat4sp(ox->xmx[(i-1)*p7X_NXCELLS+p7X_B]);
+  __m128  zerov = vec_zero4sp();
   __m128  mpv, dpv, ipv;
   union { __m128 v; float p[4]; } u, tv;
   float   path[4];
@@ -323,7 +323,7 @@
   int     Q     = p7O_NQF(ox->M);
   int     q     = (k-1) % Q;		/* (q,r) is position of the current DP cell D(i,k) */
   int     r     = (k-1) / Q;
-  __m128  zerov = _mm_setzero_ps();
+  __m128  zerov = vec_zero4sp();
   union { __m128 v; float p[4]; } mpv, dpv, tmdv, tddv;
   float   path[2];
 
--- hmmer/src/impl_sse/p7_omx.c
+++ hmmer/src/impl_sse/p7_omx.c
@@ -17,8 +17,8 @@
 #include <math.h>
 #include <float.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_alphabet.h"
--- hmmer/src/impl_sse/p7_oprofile.c
+++ hmmer/src/impl_sse/p7_oprofile.c
@@ -18,8 +18,8 @@
 #include <string.h>
 #include <math.h>		/* roundf() */
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_random.h"
@@ -749,12 +749,12 @@
    * hmmscan where many models are loaded.
    */
 
-  tmp = _mm_set1_epi8((int8_t) (om->bias_b + 127));
-  tmp2  = _mm_set1_epi8(127);
+  tmp = vec_splat16sb((int8_t) (om->bias_b + 127));
+  tmp2  = vec_splat16sb(127);
 
   for (x = 0; x < om->abc->Kp; x++)
     {
-      for (q = 0;  q < nq;            q++) om->sbv[x][q] = _mm_xor_si128(_mm_subs_epu8(tmp, om->rbv[x][q]), tmp2);
+      for (q = 0;  q < nq;            q++) om->sbv[x][q] = vec_bitxor1q(vec_subtractsaturating16ub(tmp, om->rbv[x][q]), tmp2);
       for (q = nq; q < nq + p7O_EXTRA_SB; q++) om->sbv[x][q] = om->sbv[x][q % nq];
     }
 
@@ -1422,7 +1422,7 @@
       for (q = 0; q < nq; q++)
 	{
 	  fprintf(fp, "[ ");
-	  _mm_store_si128(&tmp.v, om->rbv[x][q]);
+	  vec_store1q(&tmp.v, om->rbv[x][q]);
 	  for (z = 0; z < 16; z++) fprintf(fp, "%4d ", tmp.i[z]);
 	  fprintf(fp, "]");
 	}
@@ -1482,7 +1482,7 @@
       for (q = 0; q < nq; q++)
 	{
 	  fprintf(fp, "[ ");
-	  _mm_store_si128(&tmp.v, om->rwv[x][q]);
+	  vec_store1q(&tmp.v, om->rwv[x][q]);
 	  for (z = 0; z < 8; z++) fprintf(fp, "%6d ", tmp.i[z]);
 	  fprintf(fp, "]");
 	}
@@ -1524,7 +1524,7 @@
       for (q = 0; q < nq; q++)
 	{
 	  fprintf(fp, "[ ");
-	  _mm_store_si128(&tmp.v, om->twv[q*7 + t]);
+	  vec_store1q(&tmp.v, om->twv[q*7 + t]);
 	  for (z = 0; z < 8; z++) fprintf(fp, "%6d ", tmp.i[z]);
 	  fprintf(fp, "]");
 	}
@@ -1545,7 +1545,7 @@
   for (j = nq*7, q = 0; q < nq; q++, j++)
     {
       fprintf(fp, "[ ");
-      _mm_store_si128(&tmp.v, om->twv[j]);
+      vec_store1q(&tmp.v, om->twv[j]);
       for (z = 0; z < 8; z++) fprintf(fp, "%6d ", tmp.i[z]);
       fprintf(fp, "]");
     }
--- hmmer/src/impl_sse/ssvfilter.c
+++ hmmer/src/impl_sse/ssvfilter.c
@@ -49,9 +49,9 @@
  *
  * The code governing the use of the J state in the original filter is:
  *
- *   xEv = _mm_subs_epu8(xEv, tecv);
- *   xJv = _mm_max_epu8(xJv,xEv);
- *   xBv = _mm_max_epu8(basev, xJv);
+ *   xEv = vec_subtractsaturating16ub(xEv, tecv);
+ *   xJv = vec_max16ub(xJv,xEv);
+ *   xBv = vec_max16ub(basev, xJv);
  *
  * So for an xE value to be high enough to affect xJ, the following
  * inequality must be true:
@@ -79,10 +79,10 @@
  * Here is an analysis of what is going on in the central loop. The
  * original code is:
  *
- *   1: sv  = _mm_max_epu8(sv, xBv);
- *   2: sv  = _mm_adds_epu8(sv, biasv);      
- *   3: sv  = _mm_subs_epu8(sv, *rsc); rsc++;
- *   4: xEv = _mm_max_epu8(xEv, sv);	
+ *   1: sv  = vec_max16ub(sv, xBv);
+ *   2: sv  = vec_addsaturating16ub(sv, biasv);      
+ *   3: sv  = vec_subtractsaturating16ub(sv, *rsc); rsc++;
+ *   4: xEv = vec_max16ub(xEv, sv);	
  *
  * Here is a line by line description:
  *
@@ -129,8 +129,8 @@
  * operation, yet we need to subtract a signed byte in idea B. First
  * the new code, then the explanation:
  *
- *   sv   = _mm_subs_epi8(sv, *rsc); rsc++;
- *   xEv  = _mm_max_epu8(xEv, sv);
+ *   sv   = vec_subtractsaturating16sb(sv, *rsc); rsc++;
+ *   xEv  = vec_max16ub(xEv, sv);
  *
  * The last line is unchanged, i.e. the overall max is still done as
  * an unsigned maximum. The subtraction is saturated to satisfy idea A
@@ -336,8 +336,8 @@
  * central calculation is done as described above:
  *
  *   #define STEP_SINGLE(sv)
- *     sv   = _mm_subs_epi8(sv, *rsc); rsc++;
- *     xEv  = _mm_max_epu8(xEv, sv);
+ *     sv   = vec_subtractsaturating16sb(sv, *rsc); rsc++;
+ *     xEv  = vec_max16ub(xEv, sv);
  *
  * The CONVERT macro handles the second phase mentioned above where
  * the vectors have to be shifted. This is yet another recursive
@@ -361,8 +361,8 @@
  *     length_check(label)
  *     rsc = om->sbv[dsq[i]] + pos;
  *     step()
- *     sv = _mm_slli_si128(sv, 1);
- *     sv = _mm_or_si128(sv, beginv);
+ *     sv = vec_shiftleftbytes1q(sv, 1);
+ *     sv = vec_bitor1q(sv, beginv);
  *     i++;
  *
  * First a check is made. This is sometimes used to check whether the
@@ -393,7 +393,7 @@
  *
  * Even though the code is only around 500 lines, it expands to a
  * fairly large file when the macros are parsed. For example,
- * _mm_subs_epi8() is called 6,840 times even though it is only
+ * vec_subtractsaturating16sb() is called 6,840 times even though it is only
  * present once in this file. The object file is still not
  * ridiculously large.
  *
@@ -409,8 +409,8 @@
 
 #include <math.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sse.h"
@@ -431,8 +431,8 @@
 
 
 #define STEP_SINGLE(sv)                         \
-  sv   = _mm_subs_epi8(sv, *rsc); rsc++;        \
-  xEv  = _mm_max_epu8(xEv, sv);
+  sv   = vec_subtractsaturating16sb(sv, *rsc); rsc++;        \
+  xEv  = vec_max16ub(xEv, sv);
 
 
 #define LENGTH_CHECK(label)                     \
@@ -518,8 +518,8 @@
   length_check(label)                                           \
   rsc = om->sbv[dsq[i]] + pos;                                   \
   step()                                                        \
-  sv = _mm_slli_si128(sv, 1);                                   \
-  sv = _mm_or_si128(sv, beginv);                                \
+  sv = vec_shiftleftbytes1q(sv, 1);                                   \
+  sv = vec_bitor1q(sv, beginv);                                \
   i++;
 
 
@@ -856,7 +856,7 @@
 #endif
   };
 
-  beginv =  _mm_set1_epi8(128);
+  beginv =  vec_splat16sb(128);
   xEv    =  beginv;
 
   /* Use the highest number of bands but no more than MAX_BANDS */
--- hmmer/src/impl_sse/stotrace.c
+++ hmmer/src/impl_sse/stotrace.c
@@ -18,8 +18,8 @@
 #include <stdio.h>
 #include <math.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_random.h"
@@ -130,8 +130,8 @@
   int     q     = (k-1) % Q;		/* (q,r) is position of the current DP cell M(i,k) */
   int     r     = (k-1) / Q;
   __m128 *tp    = om->tfv + 7*q;       	/* *tp now at start of transitions to cur cell M(i,k) */
-  __m128  xBv   = _mm_set1_ps(ox->xmx[(i-1)*p7X_NXCELLS+p7X_B]);
-  __m128  zerov = _mm_setzero_ps();
+  __m128  xBv   = vec_splat4sp(ox->xmx[(i-1)*p7X_NXCELLS+p7X_B]);
+  __m128  zerov = vec_zero4sp();
   __m128  mpv, dpv, ipv;
   union { __m128 v; float p[4]; } u;
   float   path[4];
@@ -147,10 +147,10 @@
     ipv = esl_sse_rightshift_ps(ox->dpf[i-1][(Q-1)*3 + p7X_I], zerov);
   }	  
   
-  u.v = _mm_mul_ps(xBv, *tp); tp++;  path[0] = u.p[r];
-  u.v = _mm_mul_ps(mpv, *tp); tp++;  path[1] = u.p[r];
-  u.v = _mm_mul_ps(ipv, *tp); tp++;  path[2] = u.p[r];
-  u.v = _mm_mul_ps(dpv, *tp);        path[3] = u.p[r];
+  u.v = vec_multiply4sp(xBv, *tp); tp++;  path[0] = u.p[r];
+  u.v = vec_multiply4sp(mpv, *tp); tp++;  path[1] = u.p[r];
+  u.v = vec_multiply4sp(ipv, *tp); tp++;  path[2] = u.p[r];
+  u.v = vec_multiply4sp(dpv, *tp);        path[3] = u.p[r];
   esl_vec_FNorm(path, 4);
   return state[esl_rnd_FChoose(rng, path, 4)];
 }
@@ -162,7 +162,7 @@
   int     Q     = p7O_NQF(ox->M);
   int     q     = (k-1) % Q;		/* (q,r) is position of the current DP cell D(i,k) */
   int     r     = (k-1) / Q;
-  __m128  zerov = _mm_setzero_ps();
+  __m128  zerov = vec_zero4sp();
   __m128  mpv, dpv;
   __m128  tmdv, tddv;
   union { __m128 v; float p[4]; } u;
@@ -181,8 +181,8 @@
     tddv = esl_sse_rightshift_ps(om->tfv[8*Q-1],              zerov);
   }	  
 
-  u.v = _mm_mul_ps(mpv, tmdv); path[0] = u.p[r];
-  u.v = _mm_mul_ps(dpv, tddv); path[1] = u.p[r];
+  u.v = vec_multiply4sp(mpv, tmdv); path[0] = u.p[r];
+  u.v = vec_multiply4sp(dpv, tddv); path[1] = u.p[r];
   esl_vec_FNorm(path, 2);
   return state[esl_rnd_FChoose(rng, path, 2)];
 }
@@ -201,8 +201,8 @@
   float   path[2];
   int     state[2] = { p7T_M, p7T_I };
 
-  u.v = _mm_mul_ps(mpv, *tp); tp++;  path[0] = u.p[r];
-  u.v = _mm_mul_ps(ipv, *tp);        path[1] = u.p[r];
+  u.v = vec_multiply4sp(mpv, *tp); tp++;  path[0] = u.p[r];
+  u.v = vec_multiply4sp(ipv, *tp);        path[1] = u.p[r];
   esl_vec_FNorm(path, 2);
   return state[esl_rnd_FChoose(rng, path, 2)];
 }
@@ -255,20 +255,20 @@
   double sum   = 0.0;
   double roll  = esl_random(rng);
   double norm  = 1.0 / ox->xmx[i*p7X_NXCELLS+p7X_E];
-  __m128 xEv   = _mm_set1_ps(norm); /* all M, D already scaled exactly the same */
+  __m128 xEv   = vec_splat4sp(norm); /* all M, D already scaled exactly the same */
   union { __m128 v; float p[4]; } u;
   int    q,r;
 
   while (1) {
     for (q = 0; q < Q; q++)
       {
-	u.v = _mm_mul_ps(ox->dpf[i][q*3 + p7X_M], xEv);
+	u.v = vec_multiply4sp(ox->dpf[i][q*3 + p7X_M], xEv);
 	for (r = 0; r < 4; r++) {
 	  sum += u.p[r];
 	  if (roll < sum) { *ret_k = r*Q + q + 1; return p7T_M;}
 	}
 
-	u.v = _mm_mul_ps(ox->dpf[i][q*3 + p7X_D], xEv);
+	u.v = vec_multiply4sp(ox->dpf[i][q*3 + p7X_D], xEv);
 	for (r = 0; r < 4; r++) {
 	  sum += u.p[r];
 	  if (roll < sum) { *ret_k = r*Q + q + 1; return p7T_D;}
--- hmmer/src/impl_sse/vitfilter.c
+++ hmmer/src/impl_sse/vitfilter.c
@@ -22,8 +22,8 @@
 #include <stdio.h>
 #include <math.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sse.h"
@@ -106,13 +106,13 @@
   ox->M   = om->M;
 
   /* -infinity is -32768 */
-  negInfv = _mm_set1_epi16(-32768);
-  negInfv = _mm_srli_si128(negInfv, 14);  /* negInfv = 16-byte vector, 14 0 bytes + 2-byte value=-32768, for an OR operation. */
+  negInfv = vec_splat8sh(-32768);
+  negInfv = vec_shiftrightbytes1q(negInfv, 14);  /* negInfv = 16-byte vector, 14 0 bytes + 2-byte value=-32768, for an OR operation. */
 
   /* Initialization. In unsigned arithmetic, -infinity is -32768
    */
   for (q = 0; q < Q; q++)
-    MMXo(q) = IMXo(q) = DMXo(q) = _mm_set1_epi16(-32768);
+    MMXo(q) = IMXo(q) = DMXo(q) = vec_splat8sh(-32768);
   xN   = om->base_w;
   xB   = xN + om->xw[p7O_N][p7O_MOVE];
   xJ   = -32768;
@@ -127,28 +127,28 @@
     {
       rsc   = om->rwv[dsq[i]];
       tsc   = om->twv;
-      dcv   = _mm_set1_epi16(-32768);      /* "-infinity" */
-      xEv   = _mm_set1_epi16(-32768);     
-      Dmaxv = _mm_set1_epi16(-32768);     
-      xBv   = _mm_set1_epi16(xB);
+      dcv   = vec_splat8sh(-32768);      /* "-infinity" */
+      xEv   = vec_splat8sh(-32768);     
+      Dmaxv = vec_splat8sh(-32768);     
+      xBv   = vec_splat8sh(xB);
 
       /* Right shifts by 1 value (2 bytes). 4,8,12,x becomes x,4,8,12. 
        * Because ia32 is littlendian, this means a left bit shift.
        * Zeros shift on automatically; replace it with -32768.
        */
-      mpv = MMXo(Q-1);  mpv = _mm_slli_si128(mpv, 2);  mpv = _mm_or_si128(mpv, negInfv);
-      dpv = DMXo(Q-1);  dpv = _mm_slli_si128(dpv, 2);  dpv = _mm_or_si128(dpv, negInfv);
-      ipv = IMXo(Q-1);  ipv = _mm_slli_si128(ipv, 2);  ipv = _mm_or_si128(ipv, negInfv);
+      mpv = MMXo(Q-1);  mpv = vec_shiftleftbytes1q(mpv, 2);  mpv = vec_bitor1q(mpv, negInfv);
+      dpv = DMXo(Q-1);  dpv = vec_shiftleftbytes1q(dpv, 2);  dpv = vec_bitor1q(dpv, negInfv);
+      ipv = IMXo(Q-1);  ipv = vec_shiftleftbytes1q(ipv, 2);  ipv = vec_bitor1q(ipv, negInfv);
 
       for (q = 0; q < Q; q++)
       {
         /* Calculate new MMXo(i,q); don't store it yet, hold it in sv. */
-        sv   =                    _mm_adds_epi16(xBv, *tsc);  tsc++;
-        sv   = _mm_max_epi16 (sv, _mm_adds_epi16(mpv, *tsc)); tsc++;
-        sv   = _mm_max_epi16 (sv, _mm_adds_epi16(ipv, *tsc)); tsc++;
-        sv   = _mm_max_epi16 (sv, _mm_adds_epi16(dpv, *tsc)); tsc++;
-        sv   = _mm_adds_epi16(sv, *rsc);                      rsc++;
-        xEv  = _mm_max_epi16(xEv, sv);
+        sv   =                    vec_addsaturating8sh(xBv, *tsc);  tsc++;
+        sv   = vec_max8sh (sv, vec_addsaturating8sh(mpv, *tsc)); tsc++;
+        sv   = vec_max8sh (sv, vec_addsaturating8sh(ipv, *tsc)); tsc++;
+        sv   = vec_max8sh (sv, vec_addsaturating8sh(dpv, *tsc)); tsc++;
+        sv   = vec_addsaturating8sh(sv, *rsc);                      rsc++;
+        xEv  = vec_max8sh(xEv, sv);
 
         /* Load {MDI}(i-1,q) into mpv, dpv, ipv;
          * {MDI}MX(q) is then the current, not the prev row
@@ -164,12 +164,12 @@
         /* Calculate the next D(i,q+1) partially: M->D only;
                * delay storage, holding it in dcv
          */
-        dcv   = _mm_adds_epi16(sv, *tsc);  tsc++;
-        Dmaxv = _mm_max_epi16(dcv, Dmaxv);
+        dcv   = vec_addsaturating8sh(sv, *tsc);  tsc++;
+        Dmaxv = vec_max8sh(dcv, Dmaxv);
 
         /* Calculate and store I(i,q) */
-        sv     =                    _mm_adds_epi16(mpv, *tsc);  tsc++;
-        IMXo(q)= _mm_max_epi16 (sv, _mm_adds_epi16(ipv, *tsc)); tsc++;
+        sv     =                    vec_addsaturating8sh(mpv, *tsc);  tsc++;
+        IMXo(q)= vec_max8sh (sv, vec_addsaturating8sh(ipv, *tsc)); tsc++;
       }
 
       /* Now the "special" states, which start from Mk->E (->C, ->J->B) */
@@ -200,13 +200,13 @@
 	{
 	  /* Now we're obligated to do at least one complete DD path to be sure. */
 	  /* dcv has carried through from end of q loop above */
-	  dcv = _mm_slli_si128(dcv, 2); 
-	  dcv = _mm_or_si128(dcv, negInfv);
+	  dcv = vec_shiftleftbytes1q(dcv, 2); 
+	  dcv = vec_bitor1q(dcv, negInfv);
 	  tsc = om->twv + 7*Q;	/* set tsc to start of the DD's */
 	  for (q = 0; q < Q; q++) 
 	    {
-	      DMXo(q) = _mm_max_epi16(dcv, DMXo(q));	
-	      dcv     = _mm_adds_epi16(DMXo(q), *tsc); tsc++;
+	      DMXo(q) = vec_max8sh(dcv, DMXo(q));	
+	      dcv     = vec_addsaturating8sh(DMXo(q), *tsc); tsc++;
 	    }
 
 	  /* We may have to do up to three more passes; the check
@@ -214,21 +214,21 @@
 	   * our score. 
 	   */
 	  do {
-	    dcv = _mm_slli_si128(dcv, 2);
-	    dcv = _mm_or_si128(dcv, negInfv);
+	    dcv = vec_shiftleftbytes1q(dcv, 2);
+	    dcv = vec_bitor1q(dcv, negInfv);
 	    tsc = om->twv + 7*Q;	/* set tsc to start of the DD's */
 	    for (q = 0; q < Q; q++) 
 	      {
 		if (! esl_sse_any_gt_epi16(dcv, DMXo(q))) break;
-		DMXo(q) = _mm_max_epi16(dcv, DMXo(q));	
-		dcv     = _mm_adds_epi16(DMXo(q), *tsc);   tsc++;
+		DMXo(q) = vec_max8sh(dcv, DMXo(q));	
+		dcv     = vec_addsaturating8sh(DMXo(q), *tsc);   tsc++;
 	      }	    
 	  } while (q == Q);
 	}
       else  /* not calculating DD? then just store the last M->D vector calc'ed.*/
 	{
-	  dcv = _mm_slli_si128(dcv, 2);
-	  DMXo(0) = _mm_or_si128(dcv, negInfv);
+	  dcv = vec_shiftleftbytes1q(dcv, 2);
+	  DMXo(0) = vec_bitor1q(dcv, negInfv);
 	}
 	  
 #if p7_DEBUGGING
@@ -342,13 +342,13 @@
   ox->M   = om->M;
 
   /* -infinity is -32768 */
-  negInfv = _mm_set1_epi16(-32768);
-  negInfv = _mm_srli_si128(negInfv, 14);  /* negInfv = 16-byte vector, 14 0 bytes + 2-byte value=-32768, for an OR operation. */
+  negInfv = vec_splat8sh(-32768);
+  negInfv = vec_shiftrightbytes1q(negInfv, 14);  /* negInfv = 16-byte vector, 14 0 bytes + 2-byte value=-32768, for an OR operation. */
 
   /* Initialization. In unsigned arithmetic, -infinity is -32768
    */
   for (q = 0; q < Q; q++)
-    MMXo(q) = IMXo(q) = DMXo(q) = _mm_set1_epi16(-32768);
+    MMXo(q) = IMXo(q) = DMXo(q) = vec_splat8sh(-32768);
   xN   = om->base_w;
   xB   = xN + om->xw[p7O_N][p7O_MOVE];
   xJ   = -32768;
@@ -364,28 +364,28 @@
   {
       rsc   = om->rwv[dsq[i]];
       tsc   = om->twv;
-      dcv   = _mm_set1_epi16(-32768);      /* "-infinity" */
-      xEv   = _mm_set1_epi16(-32768);
-      Dmaxv = _mm_set1_epi16(-32768);
-      xBv   = _mm_set1_epi16(xB);
+      dcv   = vec_splat8sh(-32768);      /* "-infinity" */
+      xEv   = vec_splat8sh(-32768);
+      Dmaxv = vec_splat8sh(-32768);
+      xBv   = vec_splat8sh(xB);
 
       /* Right shifts by 1 value (2 bytes). 4,8,12,x becomes x,4,8,12.
        * Because ia32 is littlendian, this means a left bit shift.
        * Zeros shift on automatically; replace it with -32768.
        */
-      mpv = MMXo(Q-1);  mpv = _mm_slli_si128(mpv, 2);  mpv = _mm_or_si128(mpv, negInfv);
-      dpv = DMXo(Q-1);  dpv = _mm_slli_si128(dpv, 2);  dpv = _mm_or_si128(dpv, negInfv);
-      ipv = IMXo(Q-1);  ipv = _mm_slli_si128(ipv, 2);  ipv = _mm_or_si128(ipv, negInfv);
+      mpv = MMXo(Q-1);  mpv = vec_shiftleftbytes1q(mpv, 2);  mpv = vec_bitor1q(mpv, negInfv);
+      dpv = DMXo(Q-1);  dpv = vec_shiftleftbytes1q(dpv, 2);  dpv = vec_bitor1q(dpv, negInfv);
+      ipv = IMXo(Q-1);  ipv = vec_shiftleftbytes1q(ipv, 2);  ipv = vec_bitor1q(ipv, negInfv);
 
       for (q = 0; q < Q; q++)
       {
         /* Calculate new MMXo(i,q); don't store it yet, hold it in sv. */
-        sv   =                    _mm_adds_epi16(xBv, *tsc);  tsc++;
-        sv   = _mm_max_epi16 (sv, _mm_adds_epi16(mpv, *tsc)); tsc++;
-        sv   = _mm_max_epi16 (sv, _mm_adds_epi16(ipv, *tsc)); tsc++;
-        sv   = _mm_max_epi16 (sv, _mm_adds_epi16(dpv, *tsc)); tsc++;
-        sv   = _mm_adds_epi16(sv, *rsc);                      rsc++;
-        xEv  = _mm_max_epi16(xEv, sv);
+        sv   =                    vec_addsaturating8sh(xBv, *tsc);  tsc++;
+        sv   = vec_max8sh (sv, vec_addsaturating8sh(mpv, *tsc)); tsc++;
+        sv   = vec_max8sh (sv, vec_addsaturating8sh(ipv, *tsc)); tsc++;
+        sv   = vec_max8sh (sv, vec_addsaturating8sh(dpv, *tsc)); tsc++;
+        sv   = vec_addsaturating8sh(sv, *rsc);                      rsc++;
+        xEv  = vec_max8sh(xEv, sv);
 
         /* Load {MDI}(i-1,q) into mpv, dpv, ipv;
          * {MDI}MX(q) is then the current, not the prev row
@@ -401,12 +401,12 @@
         /* Calculate the next D(i,q+1) partially: M->D only;
                * delay storage, holding it in dcv
          */
-        dcv   = _mm_adds_epi16(sv, *tsc);  tsc++;
-        Dmaxv = _mm_max_epi16(dcv, Dmaxv);
+        dcv   = vec_addsaturating8sh(sv, *tsc);  tsc++;
+        Dmaxv = vec_max8sh(dcv, Dmaxv);
 
         /* Calculate and store I(i,q) */
-        sv     =                    _mm_adds_epi16(mpv, *tsc);  tsc++;
-        IMXo(q)= _mm_max_epi16 (sv, _mm_adds_epi16(ipv, *tsc)); tsc++;
+        sv     =                    vec_addsaturating8sh(mpv, *tsc);  tsc++;
+        IMXo(q)= vec_max8sh (sv, vec_addsaturating8sh(ipv, *tsc)); tsc++;
       }
 
       /* Now the "special" states, which start from Mk->E (->C, ->J->B) */
@@ -424,7 +424,7 @@
               p7_hmmwindow_new(windowlist, 0, i, i-1, (q+Q*z+1), 1, 0.0, p7_NOCOMPLEMENT, L );
             }
           }
-          MMXo(q) = IMXo(q) = DMXo(q) = _mm_set1_epi16(-32768); //reset score to start search for next vit window.
+          MMXo(q) = IMXo(q) = DMXo(q) = vec_splat8sh(-32768); //reset score to start search for next vit window.
         }
 
       } else {
@@ -455,13 +455,13 @@
         {
           /* Now we're obligated to do at least one complete DD path to be sure. */
           /* dcv has carried through from end of q loop above */
-          dcv = _mm_slli_si128(dcv, 2);
-          dcv = _mm_or_si128(dcv, negInfv);
+          dcv = vec_shiftleftbytes1q(dcv, 2);
+          dcv = vec_bitor1q(dcv, negInfv);
           tsc = om->twv + 7*Q;  /* set tsc to start of the DD's */
           for (q = 0; q < Q; q++)
           {
-            DMXo(q) = _mm_max_epi16(dcv, DMXo(q));
-            dcv     = _mm_adds_epi16(DMXo(q), *tsc); tsc++;
+            DMXo(q) = vec_max8sh(dcv, DMXo(q));
+            dcv     = vec_addsaturating8sh(DMXo(q), *tsc); tsc++;
           }
 
           /* We may have to do up to three more passes; the check
@@ -469,21 +469,21 @@
            * our score.
            */
           do {
-            dcv = _mm_slli_si128(dcv, 2);
-            dcv = _mm_or_si128(dcv, negInfv);
+            dcv = vec_shiftleftbytes1q(dcv, 2);
+            dcv = vec_bitor1q(dcv, negInfv);
             tsc = om->twv + 7*Q;  /* set tsc to start of the DD's */
             for (q = 0; q < Q; q++)
             {
               if (! esl_sse_any_gt_epi16(dcv, DMXo(q))) break;
-              DMXo(q) = _mm_max_epi16(dcv, DMXo(q));
-              dcv     = _mm_adds_epi16(DMXo(q), *tsc);   tsc++;
+              DMXo(q) = vec_max8sh(dcv, DMXo(q));
+              dcv     = vec_addsaturating8sh(DMXo(q), *tsc);   tsc++;
             }
           } while (q == Q);
         }
         else  /* not calculating DD? then just store the last M->D vector calc'ed.*/
         {
-          dcv = _mm_slli_si128(dcv, 2);
-          DMXo(0) = _mm_or_si128(dcv, negInfv);
+          dcv = vec_shiftleftbytes1q(dcv, 2);
+          DMXo(0) = vec_bitor1q(dcv, negInfv);
         }
       }
 #if p7_DEBUGGING
--- hmmer/src/impl_sse/vitscore.c
+++ hmmer/src/impl_sse/vitscore.c
@@ -27,8 +27,8 @@
 #include <stdio.h>
 #include <math.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sse.h"
@@ -91,7 +91,7 @@
   ox->M  = om->M;
 
   /* Initialization. */
-  infv = _mm_set1_ps(-eslINFINITY);
+  infv = vec_splat4sp(-eslINFINITY);
   for (q = 0; q < Q; q++)
     MMXo(q) = IMXo(q) = DMXo(q) = infv;
   xN   = 0.;
@@ -111,7 +111,7 @@
       dcv   = infv;
       xEv   = infv;
       Dmaxv = infv;
-      xBv   = _mm_set1_ps(xB);
+      xBv   = vec_splat4sp(xB);
 
       mpv = esl_sse_rightshift_ps(MMXo(Q-1), infv);  /* Right shifts by 4 bytes. 4,8,12,x becomes x,4,8,12. */
       dpv = esl_sse_rightshift_ps(DMXo(Q-1), infv);
@@ -119,12 +119,12 @@
       for (q = 0; q < Q; q++)
 	{
 	  /* Calculate new MMXo(i,q); don't store it yet, hold it in sv. */
-	  sv   =                _mm_add_ps(xBv, *tsc);  tsc++;
-	  sv   = _mm_max_ps(sv, _mm_add_ps(mpv, *tsc)); tsc++;
-	  sv   = _mm_max_ps(sv, _mm_add_ps(ipv, *tsc)); tsc++;
-	  sv   = _mm_max_ps(sv, _mm_add_ps(dpv, *tsc)); tsc++;
-	  sv   = _mm_add_ps(sv, *rsc);                  rsc++;
-	  xEv  = _mm_max_ps(xEv, sv);
+	  sv   =                vec_add4sp(xBv, *tsc);  tsc++;
+	  sv   = vec_max4sp(sv, vec_add4sp(mpv, *tsc)); tsc++;
+	  sv   = vec_max4sp(sv, vec_add4sp(ipv, *tsc)); tsc++;
+	  sv   = vec_max4sp(sv, vec_add4sp(dpv, *tsc)); tsc++;
+	  sv   = vec_add4sp(sv, *rsc);                  rsc++;
+	  xEv  = vec_max4sp(xEv, sv);
 	  
 	  /* Load {MDI}(i-1,q) into mpv, dpv, ipv;
 	   * {MDI}MX(q) is then the current, not the prev row
@@ -140,13 +140,13 @@
 	  /* Calculate the next D(i,q+1) partially: M->D only;
            * delay storage, holding it in dcv
 	   */
-	  dcv   = _mm_add_ps(sv, *tsc); tsc++;
-	  Dmaxv = _mm_max_ps(dcv, Dmaxv);
+	  dcv   = vec_add4sp(sv, *tsc); tsc++;
+	  Dmaxv = vec_max4sp(dcv, Dmaxv);
 
 	  /* Calculate and store I(i,q) */
-	  sv     =                _mm_add_ps(mpv, *tsc);  tsc++;
-	  sv     = _mm_max_ps(sv, _mm_add_ps(ipv, *tsc)); tsc++;
-	  IMXo(q) = _mm_add_ps(sv, *rsc);                  rsc++;
+	  sv     =                vec_add4sp(mpv, *tsc);  tsc++;
+	  sv     = vec_max4sp(sv, vec_add4sp(ipv, *tsc)); tsc++;
+	  IMXo(q) = vec_add4sp(sv, *rsc);                  rsc++;
 	}	  
 
       /* Now the "special" states, which start from Mk->E (->C, ->J->B) */
@@ -180,8 +180,8 @@
 	  tsc = om->tf + 7*Q;	/* set tsc to start of the DD's */
 	  for (q = 0; q < Q; q++) 
 	    {
-	      DMXo(q) = _mm_max_ps(dcv, DMXo(q));	
-	      dcv     = _mm_add_ps(DMXo(q), *tsc); tsc++;
+	      DMXo(q) = vec_max4sp(dcv, DMXo(q));	
+	      dcv     = vec_add4sp(DMXo(q), *tsc); tsc++;
 	    }
 
 	  /* We may have to do up to three more passes; the check
@@ -194,8 +194,8 @@
 	    for (q = 0; q < Q; q++) 
 	      {
 		if (! esl_sse_any_gt_ps(dcv, DMXo(q))) break;
-		DMXo(q) = _mm_max_ps(dcv, DMXo(q));	
-		dcv     = _mm_add_ps(DMXo(q), *tsc);   tsc++;
+		DMXo(q) = vec_max4sp(dcv, DMXo(q));	
+		dcv     = vec_add4sp(DMXo(q), *tsc);   tsc++;
 	      }	    
 	  } while (q == Q);
 	}
--- src/impl_sse/impl_sse.h
+++ src/impl_sse/impl_sse.h
@@ -4,8 +4,8 @@
 #ifndef CM_IMPL_SSE_INCLUDED
 #define CM_IMPL_SSE_INCLUDED
 
-#include <xmmintrin.h>
-#include <emmintrin.h>
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "esl_config.h"
 #include "p7_config.h"
@@ -190,16 +190,16 @@
  *
  *            This is designed as a limited-use
  *            replacement for the call:
- *            _mm_insert_epi16(a, -32768, 0)
+ *            vec_insert8sh(a, -32768, 0)
  *            which suffers from a compiler
  *            bug in gcc 3.4.x
  */
 static inline __m128i sse_setlw_neginfv(__m128i a)
 {
-  __m128i mask = _mm_setr_epi16(0x0000,0xffff,0xffff,0xffff,0xffff,0xffff,0xffff,0xffff);
-  __m128i setv = _mm_setr_epi16(-32768,0x0000,0x0000,0x0000,0x0000,0x0000,0x0000,0x0000);
+  __m128i mask = vec_setreverse8sh(0x0000,0xffff,0xffff,0xffff,0xffff,0xffff,0xffff,0xffff);
+  __m128i setv = vec_setreverse8sh(-32768,0x0000,0x0000,0x0000,0x0000,0x0000,0x0000,0x0000);
 
-  return _mm_or_si128(_mm_and_si128(a,mask),setv);
+  return vec_bitor1q(vec_bitand1q(a,mask),setv);
 }
 
 /* Function:  sse_select_si128()
@@ -209,9 +209,9 @@
  */
 static inline __m128i sse_select_si128(__m128i a, __m128i b, __m128i mask)
 {
-  b = _mm_and_si128(b, mask);
-  a = _mm_andnot_si128(mask, a);
-  return _mm_or_si128(a, b);
+  b = vec_bitand1q(b, mask);
+  a = vec_bitandnotleft1q(mask, a);
+  return vec_bitor1q(a, b);
 }
 
 
--- src/impl_sse/sse_cm_dpsearch.c
+++ src/impl_sse/sse_cm_dpsearch.c
@@ -22,8 +22,8 @@
 #include <stdio.h>
 #include <stdlib.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sqio.h"
@@ -186,8 +186,8 @@
    * by 2 to allow clean access for the range -2..sW.  
    */
   sW = W/4 + 1; 
-  zerov = _mm_setzero_ps();
-  neginfv = _mm_set1_ps(-eslINFINITY);
+  zerov = vec_zero4sp();
+  neginfv = vec_splat4sp(-eslINFINITY);
   ESL_ALLOC(vec_init_scAA,      sizeof(__m128 *) * cm->M);
   ESL_ALLOC(mem_init_scAA,      sizeof(__m128  ) * cm->M * (sW+2) + 15);
   ESL_ALLOC(vec_alpha,          sizeof(__m128 **) * 2);
@@ -237,25 +237,25 @@
   for (v = 0; v < cm->M; v++) {
     for (d = 0; d < sW; d++)
       {
-        vec_init_scAA[v][d] = _mm_setr_ps((     d <= W) ? init_scAA[v][     d] : -eslINFINITY,
+        vec_init_scAA[v][d] = vec_setreverse4sp((     d <= W) ? init_scAA[v][     d] : -eslINFINITY,
                                           (  sW+d <= W) ? init_scAA[v][  sW+d] : -eslINFINITY,
                                           (2*sW+d <= W) ? init_scAA[v][2*sW+d] : -eslINFINITY,
                                           (3*sW+d <= W) ? init_scAA[v][3*sW+d] : -eslINFINITY);
         if (cm->stid[v] == BEGL_S) {
           for (j = 0; j <= W; j++)
             {
-              vec_alpha_begl[j][v][d] = _mm_setr_ps((    +d <= W) ? alpha_begl[j][v][    +d] : -eslINFINITY,
+              vec_alpha_begl[j][v][d] = vec_setreverse4sp((    +d <= W) ? alpha_begl[j][v][    +d] : -eslINFINITY,
                                                     (  sW+d <= W) ? alpha_begl[j][v][  sW+d] : -eslINFINITY,
                                                     (2*sW+d <= W) ? alpha_begl[j][v][2*sW+d] : -eslINFINITY,
                                                     (3*sW+d <= W) ? alpha_begl[j][v][3*sW+d] : -eslINFINITY);
             }
           }
         else {
-          vec_alpha[0][v][d] = _mm_setr_ps((     d <= W) ? alpha[0][v][    +d] : -eslINFINITY,
+          vec_alpha[0][v][d] = vec_setreverse4sp((     d <= W) ? alpha[0][v][    +d] : -eslINFINITY,
                                            (  sW+d <= W) ? alpha[0][v][  sW+d] : -eslINFINITY,
                                            (2*sW+d <= W) ? alpha[0][v][2*sW+d] : -eslINFINITY,
                                            (3*sW+d <= W) ? alpha[0][v][3*sW+d] : -eslINFINITY);
-          vec_alpha[1][v][d] = _mm_setr_ps((     d <= W) ? alpha[1][v][    +d] : -eslINFINITY,
+          vec_alpha[1][v][d] = vec_setreverse4sp((     d <= W) ? alpha[1][v][    +d] : -eslINFINITY,
                                            (  sW+d <= W) ? alpha[1][v][  sW+d] : -eslINFINITY,
                                            (2*sW+d <= W) ? alpha[1][v][2*sW+d] : -eslINFINITY,
                                            (3*sW+d <= W) ? alpha[1][v][3*sW+d] : -eslINFINITY);
@@ -314,14 +314,14 @@
             else if (cm->sttype[v] == MR_st || cm->sttype[v] == IR_st) {
               /* esc constant across the row */
                 for (d = 0; d < sW; d++) {
-                  vec_esc[dsq[j]][v][d] = _mm_set1_ps(cm->oesc[v][dsq[j]]);
+                  vec_esc[dsq[j]][v][d] = vec_splat4sp(cm->oesc[v][dsq[j]]);
                 }
             }
             else if (cm->sttype[v] == ML_st || cm->sttype[v] == IL_st) {
               for (d = 0; d < sW; d++) {
                 //for (z = 0; z < 4; z++) tmp.x[z] = (z*sW+d <= j && ((j^W)|d|z) ) ? cm->oesc[v][dsq[j-(z*sW+d)+1]] : -eslINFINITY;
                 //for (z = 0; z < 4; z++) tmp.x[z] = (z*sW+d <= j && ((j^j0)|d|z) ) ? cm->oesc[v][dsq[j-(z*sW+d)+1]] : -eslINFINITY;
-                vec_esc[dsq[j]][v][d] = _mm_setr_ps((     d <= j && ((j^j0)|d|0) ) ? cm->oesc[v][dsq[j-(     d)+1]] : -eslINFINITY,
+                vec_esc[dsq[j]][v][d] = vec_setreverse4sp((     d <= j && ((j^j0)|d|0) ) ? cm->oesc[v][dsq[j-(     d)+1]] : -eslINFINITY,
                                                     (  sW+d <= j && ((j^j0)|d|1) ) ? cm->oesc[v][dsq[j-(  sW+d)+1]] : -eslINFINITY,
                                                     (2*sW+d <= j && ((j^j0)|d|2) ) ? cm->oesc[v][dsq[j-(2*sW+d)+1]] : -eslINFINITY,
                                                     (3*sW+d <= j && ((j^j0)|d|3) ) ? cm->oesc[v][dsq[j-(3*sW+d)+1]] : -eslINFINITY);
@@ -331,7 +331,7 @@
               for (d = 0; d < sW; d++) {
                 //for (z = 0; z < 4; z++) tmp.x[z] = (z*sW+d <= j && ((j^W)|d|z) ) ? cm->oesc[v][dsq[j-(z*sW+d)+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY;
                 //for (z = 0; z < 4; z++) tmp.x[z] = (z*sW+d <= j && ((j^j0)|d|z) ) ? cm->oesc[v][dsq[j-(z*sW+d)+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY;
-                vec_esc[dsq[j]][v][d] = _mm_setr_ps((     d <= j && ((j^j0)|d|0) ) ? cm->oesc[v][dsq[j-(     d)+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY,
+                vec_esc[dsq[j]][v][d] = vec_setreverse4sp((     d <= j && ((j^j0)|d|0) ) ? cm->oesc[v][dsq[j-(     d)+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY,
                                                     (  sW+d <= j && ((j^j0)|d|1) ) ? cm->oesc[v][dsq[j-(  sW+d)+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY,
                                                     (2*sW+d <= j && ((j^j0)|d|2) ) ? cm->oesc[v][dsq[j-(2*sW+d)+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY,
                                                     (3*sW+d <= j && ((j^j0)|d|3) ) ? cm->oesc[v][dsq[j-(3*sW+d)+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY);
@@ -347,11 +347,11 @@
           if (cm->sttype[v] == ML_st || cm->sttype[v] == IL_st) {
             for (d = 0; d < delta; d++) {
               tmpary[d] = vec_esc[dsq[j]][v][sW-delta+d];
-              tmpary[d] = _mm_shuffle_ps(tmpary[d], tmpary[d], _MM_SHUFFLE(2,1,0,0));
+              tmpary[d] = vec_shufflepermute4sp(tmpary[d], tmpary[d], _MM_SHUFFLE(2,1,0,0));
               //tmp_esc     = (j^W)|d ? cm->oesc[v][dsq[j-d+1]] : -eslINFINITY;
               tmp_esc     = (j^j0)|d ? cm->oesc[v][dsq[j-d+1]] : -eslINFINITY;
               //tmpary[d].x[0] = tmp_esc;
-              tmpary[d] = _mm_move_ss(tmpary[d],_mm_set1_ps(tmp_esc));
+              tmpary[d] = vec_insert1spintolower4sp(tmpary[d],vec_splat4sp(tmp_esc));
             }
             for (d = sW-1; d >= delta; d--) {
               vec_esc[dsq[j]][v][d] = vec_esc[dsq[j]][v][d-delta];
@@ -363,11 +363,11 @@
           else if (cm->sttype[v] == MP_st) {
             for (d = 0; d < delta; d++) {
               tmpary[d] = vec_esc[dsq[j]][v][sW-delta+d];
-              tmpary[d] = _mm_shuffle_ps(tmpary[d], tmpary[d], _MM_SHUFFLE(2,1,0,0));
+              tmpary[d] = vec_shufflepermute4sp(tmpary[d], tmpary[d], _MM_SHUFFLE(2,1,0,0));
               //tmp_esc     = (j^W)|d ? cm->oesc[v][dsq[j-d+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY;
               tmp_esc     = (j^j0)|d ? cm->oesc[v][dsq[j-d+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY;
               //tmpary[d].x[0] = tmp_esc;
-              tmpary[d] = _mm_move_ss(tmpary[d],_mm_set1_ps(tmp_esc));
+              tmpary[d] = vec_insert1spintolower4sp(tmpary[d],vec_splat4sp(tmp_esc));
             }
             for (d = sW-1; d >= delta; d--) {
               vec_esc[dsq[j]][v][d] = vec_esc[dsq[j]][v][d-delta];
@@ -407,7 +407,7 @@
 /*            int dkindex = 0;
             for (k = 0; k < W && k <=j; k++) {
               vec_access = (float *) (&vec_alpha[jp_y][y][k%sW])+k/sW;
-              vec_tmp_begr = _mm_set1_ps(*vec_access);
+              vec_tmp_begr = vec_splat4sp(*vec_access);
 
               for (d = 0; d < sW; d++) {
                 if (dkindex >= sW) dkindex -= sW;
@@ -418,7 +418,7 @@
                   vec_tmp_begl = esl_sse_rightshift_ps(vec_alpha_begl[jp_wA[k]][w][dkindex],neginfv);
                 }
                 else if (k <= 2*sW+d) {
-                  vec_tmp_begl = _mm_movelh_ps(neginfv, vec_alpha_begl[jp_wA[k]][w][dkindex]);
+                  vec_tmp_begl = vec_extractlower2spinsertupper2spof4sp(neginfv, vec_alpha_begl[jp_wA[k]][w][dkindex]);
                 }
                 else {
                   vec_tmp_begl = esl_sse_leftshift_ps(neginfv, vec_alpha_begl[jp_wA[k]][w][dkindex]);
@@ -426,7 +426,7 @@
 
                 dkindex++;
 
-                vec_alpha[jp_v][v][d] = _mm_max_ps(vec_alpha[jp_v][v][d], _mm_add_ps(vec_tmp_begl,vec_tmp_begr));
+                vec_alpha[jp_v][v][d] = vec_max4sp(vec_alpha[jp_v][v][d], vec_add4sp(vec_tmp_begl,vec_tmp_begr));
               }
               dkindex--;
             }
@@ -436,19 +436,19 @@
             //for (k = 0; k < sW && k <= j; k++) {
             for (k = 0; k <= kmax; k++) {
               vec_access = (float *) (&vec_alpha[jp_y][y][k%sW])+k/sW;
-              vec_tmp_begr = _mm_set1_ps(*vec_access);
+              vec_tmp_begr = vec_splat4sp(*vec_access);
 
               dkindex = sW - k;
               for (d = 0; d < k; d++) {
                 vec_tmp_begl = esl_sse_rightshift_ps(vec_alpha_begl[jp_wA[k]][w][dkindex],neginfv);
-                vec_alpha[jp_v][v][d] = _mm_max_ps(vec_alpha[jp_v][v][d], _mm_add_ps(vec_tmp_begl,vec_tmp_begr));
+                vec_alpha[jp_v][v][d] = vec_max4sp(vec_alpha[jp_v][v][d], vec_add4sp(vec_tmp_begl,vec_tmp_begr));
                 dkindex++;
               }
 
               dkindex = 0;
               for (     ; d < sW; d++) {
                 vec_tmp_begl = vec_alpha_begl[jp_wA[k]][w][dkindex];
-                vec_alpha[jp_v][v][d] = _mm_max_ps(vec_alpha[jp_v][v][d], _mm_add_ps(vec_tmp_begl,vec_tmp_begr));
+                vec_alpha[jp_v][v][d] = vec_max4sp(vec_alpha[jp_v][v][d], vec_add4sp(vec_tmp_begl,vec_tmp_begr));
                 dkindex++;
               }
             }
@@ -456,19 +456,19 @@
             kmax = j < 2*sW - 1 ? j : 2*sW - 1;
             for ( ; k <= kmax; k++) {
               vec_access = (float *) (&vec_alpha[jp_y][y][k%sW])+k/sW;
-              vec_tmp_begr = _mm_set1_ps(*vec_access);
+              vec_tmp_begr = vec_splat4sp(*vec_access);
 
               dkindex = 2*sW - k;
               for (d = 0; d < k - sW; d++) {
-                vec_tmp_begl = _mm_movelh_ps(neginfv, vec_alpha_begl[jp_wA[k]][w][dkindex]);
-                vec_alpha[jp_v][v][d] = _mm_max_ps(vec_alpha[jp_v][v][d], _mm_add_ps(vec_tmp_begl,vec_tmp_begr));
+                vec_tmp_begl = vec_extractlower2spinsertupper2spof4sp(neginfv, vec_alpha_begl[jp_wA[k]][w][dkindex]);
+                vec_alpha[jp_v][v][d] = vec_max4sp(vec_alpha[jp_v][v][d], vec_add4sp(vec_tmp_begl,vec_tmp_begr));
                 dkindex++;
               }
 
               dkindex = 0;
               for ( ; d < sW; d++) {
                 vec_tmp_begl = esl_sse_rightshift_ps(vec_alpha_begl[jp_wA[k]][w][dkindex],neginfv);
-                vec_alpha[jp_v][v][d] = _mm_max_ps(vec_alpha[jp_v][v][d], _mm_add_ps(vec_tmp_begl,vec_tmp_begr));
+                vec_alpha[jp_v][v][d] = vec_max4sp(vec_alpha[jp_v][v][d], vec_add4sp(vec_tmp_begl,vec_tmp_begr));
                 dkindex++;
               }
             }
@@ -476,19 +476,19 @@
             kmax = j < 3*sW - 1 ? j : 3*sW - 1;
             for ( ; k <= kmax; k++) {
               vec_access = (float *) (&vec_alpha[jp_y][y][k%sW])+k/sW;
-              vec_tmp_begr = _mm_set1_ps(*vec_access);
+              vec_tmp_begr = vec_splat4sp(*vec_access);
 
               dkindex = 3*sW - k;
               for (d = 0; d < k - 2*sW; d++) {
                 vec_tmp_begl = esl_sse_leftshift_ps(neginfv, vec_alpha_begl[jp_wA[k]][w][dkindex]);
-                vec_alpha[jp_v][v][d] = _mm_max_ps(vec_alpha[jp_v][v][d], _mm_add_ps(vec_tmp_begl,vec_tmp_begr));
+                vec_alpha[jp_v][v][d] = vec_max4sp(vec_alpha[jp_v][v][d], vec_add4sp(vec_tmp_begl,vec_tmp_begr));
                 dkindex++;
               }
 
               dkindex = 0;
               for ( ; d < sW; d++) {
-                vec_tmp_begl = _mm_movelh_ps(neginfv, vec_alpha_begl[jp_wA[k]][w][dkindex]);
-                vec_alpha[jp_v][v][d] = _mm_max_ps(vec_alpha[jp_v][v][d], _mm_add_ps(vec_tmp_begl,vec_tmp_begr));
+                vec_tmp_begl = vec_extractlower2spinsertupper2spof4sp(neginfv, vec_alpha_begl[jp_wA[k]][w][dkindex]);
+                vec_alpha[jp_v][v][d] = vec_max4sp(vec_alpha[jp_v][v][d], vec_add4sp(vec_tmp_begl,vec_tmp_begr));
                 dkindex++;
               }
             }
@@ -497,12 +497,12 @@
             kmax = j < W ? j : W;
             for ( ; k <= kmax; k++) {
               vec_access = (float *) (&vec_alpha[jp_y][y][k%sW])+k/sW;
-              vec_tmp_begr = _mm_set1_ps(*vec_access);
+              vec_tmp_begr = vec_splat4sp(*vec_access);
 
               dkindex = 0;
               for (d = k - 3*sW; d < sW; d++) {
                 vec_tmp_begl = esl_sse_leftshift_ps(neginfv, vec_alpha_begl[jp_wA[k]][w][dkindex]);
-                vec_alpha[jp_v][v][d] = _mm_max_ps(vec_alpha[jp_v][v][d], _mm_add_ps(vec_tmp_begl,vec_tmp_begr));
+                vec_alpha[jp_v][v][d] = vec_max4sp(vec_alpha[jp_v][v][d], vec_add4sp(vec_tmp_begl,vec_tmp_begr));
                 dkindex++;
               }
             }
@@ -521,14 +521,14 @@
 	  else if (cm->stid[v] == BEGL_S) {
 	    y = cm->cfirst[v]; 
 
-            vec_tsc = _mm_set1_ps(tsc_v[0]);
+            vec_tsc = vec_splat4sp(tsc_v[0]);
             for (d = 0; d < sW; d++) {
-              vec_alpha_begl[jp_v][v][d] = _mm_max_ps(vec_init_scAA[v][d], _mm_add_ps(vec_alpha[jp_y][y][d], vec_tsc));
+              vec_alpha_begl[jp_v][v][d] = vec_max4sp(vec_init_scAA[v][d], vec_add4sp(vec_alpha[jp_y][y][d], vec_tsc));
             }
 	    for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
-              vec_tsc = _mm_set1_ps(tsc_v[yoffset]);
+              vec_tsc = vec_splat4sp(tsc_v[yoffset]);
               for (d = 0; d < sW; d++) {
-	        vec_alpha_begl[jp_v][v][d] = _mm_max_ps(vec_alpha_begl[jp_v][v][d], _mm_add_ps(vec_alpha[jp_y][y+yoffset][d], vec_tsc));
+	        vec_alpha_begl[jp_v][v][d] = vec_max4sp(vec_alpha_begl[jp_v][v][d], vec_add4sp(vec_alpha[jp_y][y+yoffset][d], vec_tsc));
               }
             }
 	    /* careful: y is in alpha (all children of a BEGL_S must be non BEGL_S) */
@@ -549,23 +549,23 @@
 // sequential operations
             for (d = 0; d < sW; d++) tmpary[d] = vec_init_scAA[v][d-1];
             for (yoffset = cm->cnum[v]-1; yoffset > 0; yoffset--) {
-              vec_tsc = _mm_set1_ps(tsc_v[yoffset]);
+              vec_tsc = vec_splat4sp(tsc_v[yoffset]);
               for (d = 0; d < sW; d++) {
-                tmpary[d] = _mm_max_ps(tmpary[d], _mm_add_ps(vec_alpha[jp_y][y+yoffset][d - 1], vec_tsc));
+                tmpary[d] = vec_max4sp(tmpary[d], vec_add4sp(vec_alpha[jp_y][y+yoffset][d - 1], vec_tsc));
               }
             }
             for (d = 0; d < sW; d++) {
-              vec_alpha[jp_v][v][d] = _mm_add_ps(tmpary[d],vec_esc[dsq[j]][v][d]);
+              vec_alpha[jp_v][v][d] = vec_add4sp(tmpary[d],vec_esc[dsq[j]][v][d]);
             }
             vec_alpha[jp_v][v][-1] = esl_sse_rightshift_ps(vec_alpha[jp_v][v][sW-1],neginfv);
 
             /* yoffset = 0  - self-transition */
-            vec_tsc = _mm_set1_ps(tsc_v[0]);
+            vec_tsc = vec_splat4sp(tsc_v[0]);
             do
               {
                 for (d = 0; d < sW; d++) {
-                  tmpary[d] = _mm_max_ps(tmpary[d], _mm_add_ps(vec_alpha[jp_y][y][d - 1], vec_tsc));
-                  vec_alpha[jp_v][v][d] = _mm_add_ps(tmpary[d],vec_esc[dsq[j]][v][d]);
+                  tmpary[d] = vec_max4sp(tmpary[d], vec_add4sp(vec_alpha[jp_y][y][d - 1], vec_tsc));
+                  vec_alpha[jp_v][v][d] = vec_add4sp(tmpary[d],vec_esc[dsq[j]][v][d]);
                 }
               tmpv = vec_alpha[jp_v][v][-1];
               vec_alpha[jp_v][v][-1] = esl_sse_rightshift_ps(vec_alpha[jp_v][v][sW-1],neginfv);
@@ -583,18 +583,18 @@
 	  else { /* ! B_st, ! BEGL_S st , ! IL_st; */
 	    y = cm->cfirst[v]; 
 
-            vec_tsc = _mm_set1_ps(tsc_v[0]);
+            vec_tsc = vec_splat4sp(tsc_v[0]);
             for (d = 0; d < sW; d++) {
-              vec_alpha[jp_v][v][d] = _mm_max_ps(vec_init_scAA[v][d-sd], _mm_add_ps(vec_alpha[jp_y][y][d - sd], vec_tsc));
+              vec_alpha[jp_v][v][d] = vec_max4sp(vec_init_scAA[v][d-sd], vec_add4sp(vec_alpha[jp_y][y][d - sd], vec_tsc));
             }
             for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
-              vec_tsc = _mm_set1_ps(tsc_v[yoffset]);
+              vec_tsc = vec_splat4sp(tsc_v[yoffset]);
               for (d = 0; d < sW; d++) {
-                vec_alpha[jp_v][v][d] = _mm_max_ps(vec_alpha[jp_v][v][d], _mm_add_ps(vec_alpha[jp_y][y+yoffset][d - sd], vec_tsc));
+                vec_alpha[jp_v][v][d] = vec_max4sp(vec_alpha[jp_v][v][d], vec_add4sp(vec_alpha[jp_y][y+yoffset][d - sd], vec_tsc));
               }
             }
             for (d = 0; d < sW; d++) {
-              vec_alpha[jp_v][v][d] = _mm_add_ps(vec_alpha[jp_v][v][d],vec_esc[dsq[j]][v][d]);
+              vec_alpha[jp_v][v][d] = vec_add4sp(vec_alpha[jp_v][v][d],vec_esc[dsq[j]][v][d]);
             }
 
             vec_alpha[jp_v][v][-1] = esl_sse_rightshift_ps(vec_alpha[jp_v][v][sW-1],neginfv);
@@ -609,8 +609,8 @@
           } /* end of else (which was entered if ! B_st && ! BEGL_S st) */
 	  if(vsc != NULL) {
             tmpv = neginfv;
-	    if(cm->stid[v] != BEGL_S) for (d = 0; d <= sW; d++) tmpv = _mm_max_ps(tmpv, vec_alpha[jp_v][v][d]);
-	    else                      for (d = 0; d <= sW; d++) tmpv = _mm_max_ps(tmpv, vec_alpha_begl[jp_v][v][d]);
+	    if(cm->stid[v] != BEGL_S) for (d = 0; d <= sW; d++) tmpv = vec_max4sp(tmpv, vec_alpha[jp_v][v][d]);
+	    else                      for (d = 0; d <= sW; d++) tmpv = vec_max4sp(tmpv, vec_alpha_begl[jp_v][v][d]);
             esl_sse_hmax_ps(tmpv, &vsc[v]);
 	  }
 	} /*loop over decks v>0 */
@@ -628,37 +628,37 @@
       /* determine min/max d we're allowing for the root state and this position j */
       jp_v = cur;
       for (d = 0; d < sW; d++) {
-	vec_bestr[d] = _mm_setzero_ps();	/* root of the traceback = root state 0 */
+	vec_bestr[d] = vec_zero4sp();	/* root of the traceback = root state 0 */
 	y = cm->cfirst[0];
-        vec_tsc = _mm_set1_ps(tsc_v[0]);
-	vec_alpha[jp_v][0][d] = _mm_max_ps(neginfv, _mm_add_ps(vec_alpha[cur][y][d],vec_tsc));
+        vec_tsc = vec_splat4sp(tsc_v[0]);
+	vec_alpha[jp_v][0][d] = vec_max4sp(neginfv, vec_add4sp(vec_alpha[cur][y][d],vec_tsc));
 	for (yoffset = 1; yoffset < cm->cnum[0]; yoffset++) {
-          vec_tsc = _mm_set1_ps(tsc_v[yoffset]);
-	  vec_alpha[jp_v][0][d] = _mm_max_ps(vec_alpha[jp_v][0][d], _mm_add_ps(vec_alpha[cur][y+yoffset][d],vec_tsc));
+          vec_tsc = vec_splat4sp(tsc_v[yoffset]);
+	  vec_alpha[jp_v][0][d] = vec_max4sp(vec_alpha[jp_v][0][d], vec_add4sp(vec_alpha[cur][y+yoffset][d],vec_tsc));
         }
       }
 	
       if (cm->flags & CMH_LOCAL_BEGIN) {
 	for (y = 1; y < cm->M; y++) {
 	  if(NOT_IMPOSSIBLE(cm->beginsc[y])) {
-            vec_beginsc = _mm_set1_ps(cm->beginsc[y]);
+            vec_beginsc = vec_splat4sp(cm->beginsc[y]);
 	    if(cm->stid[y] == BEGL_S)
 	      {
 		jp_y = j % (W+1);
 		for (d = 0; d < sW; d++) {
-                  tmpv = _mm_add_ps(vec_alpha_begl[jp_y][y][d], vec_beginsc);
-                  mask  = _mm_cmpgt_ps(tmpv, vec_alpha[jp_v][0][d]);
-                  vec_alpha[jp_v][0][d] = _mm_max_ps(tmpv, vec_alpha[jp_v][0][d]);
-                  vec_bestr[d] = esl_sse_select_ps(_mm_set1_ps((float) y), vec_bestr[d], mask);
+                  tmpv = vec_add4sp(vec_alpha_begl[jp_y][y][d], vec_beginsc);
+                  mask  = vec_comparegt4sp(tmpv, vec_alpha[jp_v][0][d]);
+                  vec_alpha[jp_v][0][d] = vec_max4sp(tmpv, vec_alpha[jp_v][0][d]);
+                  vec_bestr[d] = esl_sse_select_ps(vec_splat4sp((float) y), vec_bestr[d], mask);
 		}
 	      }
 	    else { /* y != BEGL_S */
 	      jp_y = cur;
 	      for (d = 0; d < sW; d++) {
-                  tmpv = _mm_add_ps(vec_alpha[jp_y][y][d], vec_beginsc);
-                  mask  = _mm_cmpgt_ps(tmpv, vec_alpha[jp_v][0][d]);
-                  vec_alpha[jp_v][0][d] = _mm_max_ps(tmpv, vec_alpha[jp_v][0][d]);
-                  vec_bestr[d] = esl_sse_select_ps(_mm_set1_ps((float) y), vec_bestr[d], mask);
+                  tmpv = vec_add4sp(vec_alpha[jp_y][y][d], vec_beginsc);
+                  mask  = vec_comparegt4sp(tmpv, vec_alpha[jp_v][0][d]);
+                  vec_alpha[jp_v][0][d] = vec_max4sp(tmpv, vec_alpha[jp_v][0][d]);
+                  vec_bestr[d] = esl_sse_select_ps(vec_splat4sp((float) y), vec_bestr[d], mask);
 	        }
 	      }
 	  }
@@ -674,7 +674,7 @@
       /* find the best score */
       tmpv = neginfv;
       for (d = 0; d < sW; d++) 
-	tmpv = _mm_max_ps(tmpv, vec_alpha[jp_v][0][d]);
+	tmpv = vec_max4sp(tmpv, vec_alpha[jp_v][0][d]);
       esl_sse_hmax_ps(tmpv, &tmp_esc);		/* overloaded tmp_esc, just need a temporary float */
       vsc_root = ESL_MAX(vsc_root, tmp_esc);
       /* for UpdateGammaHitMx to work, these data need to be un-vectorized: alpha[jp_v][0], bestr */
--- src/impl_sse/sse_cm_dpsmall.c
+++ src/impl_sse/sse_cm_dpsmall.c
@@ -53,8 +53,8 @@
 #include <stdio.h>
 #include <stdlib.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_stack.h"
@@ -67,7 +67,7 @@
 
 #include "impl_sse.h"
 
-#define WORDRSHIFTX(a,b,x) (_mm_or_si128(_mm_slli_si128(a,2*x),_mm_srli_si128(b,(8-x)*2)))
+#define WORDRSHIFTX(a,b,x) (vec_bitor1q(vec_shiftleftbytes1q(a,2*x),vec_shiftrightbytes1q(b,(8-x)*2)))
 
 typedef struct sse_deck_s {
    __m128   *mem;
@@ -493,7 +493,7 @@
   const int vecwidth = 4;
   __m128   neginfv;
 
-  neginfv = _mm_set1_ps(-eslINFINITY);
+  neginfv = vec_splat4sp(-eslINFINITY);
 
   /* 1. If the generic problem is small enough, solve it with inside^T,
    *    and append the trace to tr.
@@ -565,24 +565,24 @@
    */
 
   W = j0-i0+1;
-  doffset = _mm_setr_epi32(0, 1, 2, 3);
-  vb_sc = _mm_set1_ps(-eslINFINITY);
+  doffset = vec_setreverse4sw(0, 1, 2, 3);
+  vb_sc = vec_splat4sp(-eslINFINITY);
   for (jp = 0; jp <= W; jp++) {
     j = i0-1+jp;
-    vec_j = (__m128) _mm_set1_epi32(j);
+    vec_j = (__m128) vec_splat4sw(j);
     sW = jp/vecwidth;
 
     /* case: k = 0 */
     vec_access = (float *) (&alpha[y]->vec[j][0]);
-    begr_v = _mm_set1_ps(*vec_access);
-    vec_k = (__m128) _mm_set1_epi32(0);
+    begr_v = vec_splat4sp(*vec_access);
+    vec_k = (__m128) vec_splat4sw(0);
     for (dp = 0; dp <= sW; dp++)
       {
-        vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
-        tmpv  = _mm_add_ps(alpha[w]->vec[j][dp], begr_v);
-        tmpv  = _mm_add_ps(tmpv, beta[v]->vec[j][dp]);
-        mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-        vb_sc = _mm_max_ps(tmpv, vb_sc);
+        vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
+        tmpv  = vec_add4sp(alpha[w]->vec[j][dp], begr_v);
+        tmpv  = vec_add4sp(tmpv, beta[v]->vec[j][dp]);
+        mask  = vec_comparegt4sp(tmpv, vb_sc);
+        vb_sc = vec_max4sp(tmpv, vb_sc);
         vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
         vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
         vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -590,18 +590,18 @@
     /* case: k = 4x */
     for (k = 4; k <= jp; k+=4)
       {
-        vec_k = (__m128) _mm_set1_epi32(k);
+        vec_k = (__m128) vec_splat4sw(k);
         kp = k/vecwidth;
         vec_access = (float *) (&alpha[y]->vec[j][kp]);
-        begr_v = _mm_set1_ps(*vec_access);
+        begr_v = vec_splat4sp(*vec_access);
 
         for (dp = kp; dp <= sW; dp++)
           {
-            vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
-            tmpv  = _mm_add_ps(alpha[w]->vec[j-k][dp-kp], begr_v);
-            tmpv  = _mm_add_ps(tmpv, beta[v]->vec[j][dp]);
-            mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-            vb_sc = _mm_max_ps(tmpv, vb_sc);
+            vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
+            tmpv  = vec_add4sp(alpha[w]->vec[j-k][dp-kp], begr_v);
+            tmpv  = vec_add4sp(tmpv, beta[v]->vec[j][dp]);
+            mask  = vec_comparegt4sp(tmpv, vb_sc);
+            vb_sc = vec_max4sp(tmpv, vb_sc);
             vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
             vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
             vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -610,29 +610,29 @@
     /* case k = 4x+1 */
     for (k = 1; k <= jp; k+=4)
       {
-        vec_k = (__m128) _mm_set1_epi32(k);
+        vec_k = (__m128) vec_splat4sw(k);
         dp = kp = k/vecwidth;
         vec_access = (float *) (&alpha[y]->vec[j][kp]) + 1;
-        begr_v = _mm_set1_ps(*vec_access);
+        begr_v = vec_splat4sp(*vec_access);
 
-        vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
+        vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
         tmpv  = esl_sse_rightshift_ps(alpha[w]->vec[j-k][0], neginfv);
-        tmpv  = _mm_add_ps(tmpv, begr_v);
-        tmpv  = _mm_add_ps(tmpv, beta[v]->vec[j][dp]);
-        mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-        vb_sc = _mm_max_ps(tmpv, vb_sc);
+        tmpv  = vec_add4sp(tmpv, begr_v);
+        tmpv  = vec_add4sp(tmpv, beta[v]->vec[j][dp]);
+        mask  = vec_comparegt4sp(tmpv, vb_sc);
+        vb_sc = vec_max4sp(tmpv, vb_sc);
         vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
         vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
         vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
 
         for (dp = kp+1; dp <= sW; dp++)
           {
-            vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
+            vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
             tmpv  = alt_rightshift_ps(alpha[w]->vec[j-k][dp-kp], alpha[w]->vec[j-k][dp-kp-1]);
-            tmpv  = _mm_add_ps(tmpv, begr_v);
-            tmpv  = _mm_add_ps(tmpv, beta[v]->vec[j][dp]);
-            mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-            vb_sc = _mm_max_ps(tmpv, vb_sc);
+            tmpv  = vec_add4sp(tmpv, begr_v);
+            tmpv  = vec_add4sp(tmpv, beta[v]->vec[j][dp]);
+            mask  = vec_comparegt4sp(tmpv, vb_sc);
+            vb_sc = vec_max4sp(tmpv, vb_sc);
             vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
             vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
             vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -641,30 +641,30 @@
     /* case k = 4x+2 */
     for (k = 2; k <= jp; k+=4)
       {
-        vec_k = (__m128) _mm_set1_epi32(k);
+        vec_k = (__m128) vec_splat4sw(k);
         dp = kp = k/vecwidth;
         vec_access = (float *) (&alpha[y]->vec[j][kp]) + 2;
-        begr_v = _mm_set1_ps(*vec_access);
-
-        vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
-        tmpv  = _mm_movelh_ps(neginfv, alpha[w]->vec[j-k][0]);
-        tmpv  = _mm_add_ps(tmpv, begr_v);
-        tmpv  = _mm_add_ps(tmpv, beta[v]->vec[j][dp]);
-        mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-        vb_sc = _mm_max_ps(tmpv, vb_sc);
+        begr_v = vec_splat4sp(*vec_access);
+
+        vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
+        tmpv  = vec_extractlower2spinsertupper2spof4sp(neginfv, alpha[w]->vec[j-k][0]);
+        tmpv  = vec_add4sp(tmpv, begr_v);
+        tmpv  = vec_add4sp(tmpv, beta[v]->vec[j][dp]);
+        mask  = vec_comparegt4sp(tmpv, vb_sc);
+        vb_sc = vec_max4sp(tmpv, vb_sc);
         vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
         vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
         vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
 
         for (dp = kp+1; dp <= sW; dp++)
           {
-            vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
-            tmpv  = _mm_movelh_ps(neginfv, alpha[w]->vec[j-k][dp-kp]);
-            tmpv  = _mm_movehl_ps(tmpv, alpha[w]->vec[j-k][dp-kp-1]);
-            tmpv  = _mm_add_ps(tmpv, begr_v);
-            tmpv  = _mm_add_ps(tmpv, beta[v]->vec[j][dp]);
-            mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-            vb_sc = _mm_max_ps(tmpv, vb_sc);
+            vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
+            tmpv  = vec_extractlower2spinsertupper2spof4sp(neginfv, alpha[w]->vec[j-k][dp-kp]);
+            tmpv  = vec_extractupper2spinsertlower2spof4sp(tmpv, alpha[w]->vec[j-k][dp-kp-1]);
+            tmpv  = vec_add4sp(tmpv, begr_v);
+            tmpv  = vec_add4sp(tmpv, beta[v]->vec[j][dp]);
+            mask  = vec_comparegt4sp(tmpv, vb_sc);
+            vb_sc = vec_max4sp(tmpv, vb_sc);
             vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
             vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
             vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -673,29 +673,29 @@
     /* case k = 4x+3 */
     for (k = 3; k <= jp; k+= 4)
       {
-        vec_k = (__m128) _mm_set1_epi32(k);
+        vec_k = (__m128) vec_splat4sw(k);
         dp = kp = k/vecwidth;
         vec_access = (float *) (&alpha[y]->vec[j][kp]) + 3;
-        begr_v = _mm_set1_ps(*vec_access);
+        begr_v = vec_splat4sp(*vec_access);
 
-        vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
+        vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
         tmpv  = esl_sse_leftshift_ps(neginfv, alpha[w]->vec[j-k][0]);
-        tmpv  = _mm_add_ps(tmpv, begr_v);
-        tmpv  = _mm_add_ps(tmpv, beta[v]->vec[j][dp]);
-        mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-        vb_sc = _mm_max_ps(tmpv, vb_sc);
+        tmpv  = vec_add4sp(tmpv, begr_v);
+        tmpv  = vec_add4sp(tmpv, beta[v]->vec[j][dp]);
+        mask  = vec_comparegt4sp(tmpv, vb_sc);
+        vb_sc = vec_max4sp(tmpv, vb_sc);
         vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
         vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
         vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
 
         for (dp = kp+1; dp <= sW; dp++)
           {
-            vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
+            vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
             tmpv  = esl_sse_leftshift_ps(alpha[w]->vec[j-k][dp-kp-1], alpha[w]->vec[j-k][dp-kp]);
-            tmpv  = _mm_add_ps(tmpv, begr_v);
-            tmpv  = _mm_add_ps(tmpv, beta[v]->vec[j][dp]);
-            mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-            vb_sc = _mm_max_ps(tmpv, vb_sc);
+            tmpv  = vec_add4sp(tmpv, begr_v);
+            tmpv  = vec_add4sp(tmpv, beta[v]->vec[j][dp]);
+            mask  = vec_comparegt4sp(tmpv, vb_sc);
+            vb_sc = vec_max4sp(tmpv, vb_sc);
             vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
             vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
             vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -706,11 +706,11 @@
   /* Local alignment only: maybe we're better off in EL?
    */
   if (cm->flags & CMH_LOCAL_END) {
-    vec_k = (__m128) _mm_set1_epi32(-1); /* flag for using EL above v */
+    vec_k = (__m128) vec_splat4sw(-1); /* flag for using EL above v */
     for (jp = 0; jp <= W; jp++) 
       {
 	j = i0-1+jp;
-        vec_j = (__m128) _mm_set1_epi32(j);
+        vec_j = (__m128) vec_splat4sw(j);
         sW = jp/vecwidth;
 	/*for (d = jp; d >= 0; d--)
 	  if ((sc = beta[cm->M][j][d]) > best_sc) {
@@ -720,9 +720,9 @@
 	    best_d  = d;
 	  } */
         for (dp = 0; dp <= sW; dp++) {
-          vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
-          mask  = _mm_cmpgt_ps(beta[cm->M]->vec[j][dp], vb_sc);
-          vb_sc = _mm_max_ps(beta[cm->M]->vec[j][dp], vb_sc);
+          vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
+          mask  = vec_comparegt4sp(beta[cm->M]->vec[j][dp], vb_sc);
+          vb_sc = vec_max4sp(beta[cm->M]->vec[j][dp], vb_sc);
           vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
           vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
           vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -733,21 +733,21 @@
   /* Local alignment only: maybe we're better off in ROOT?
    */
   if (r == 0 && cm->flags & CMH_LOCAL_BEGIN) {
-    vec_j = (__m128) _mm_set1_epi32(j0);
-    vec_d = (__m128) _mm_set1_epi32(W);
+    vec_j = (__m128) vec_splat4sw(j0);
+    vec_d = (__m128) vec_splat4sw(W);
 
-    tmpv  = _mm_set1_ps(b1_sc);
-    vec_k = (__m128) _mm_set1_epi32(-2); /* flag for using local begin into left wedge w..wend */
-    mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-    vb_sc = _mm_max_ps(tmpv, vb_sc);
+    tmpv  = vec_splat4sp(b1_sc);
+    vec_k = (__m128) vec_splat4sw(-2); /* flag for using local begin into left wedge w..wend */
+    mask  = vec_comparegt4sp(tmpv, vb_sc);
+    vb_sc = vec_max4sp(tmpv, vb_sc);
     vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
     vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
     vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
 
-    tmpv  = _mm_set1_ps(b2_sc);
-    vec_k = (__m128) _mm_set1_epi32(-3); /* flag for using local begin into right wedge y..yend */
-    mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-    vb_sc = _mm_max_ps(tmpv, vb_sc);
+    tmpv  = vec_splat4sp(b2_sc);
+    vec_k = (__m128) vec_splat4sw(-3); /* flag for using local begin into right wedge y..yend */
+    mask  = vec_comparegt4sp(tmpv, vb_sc);
+    vb_sc = vec_max4sp(tmpv, vb_sc);
     vb_k  = esl_sse_select_ps(vb_k, vec_k, mask);
     vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
     vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -763,24 +763,24 @@
 
   /* determine values corresponding to best score out of our 4x vector */
   /* like esl_sse_hmax(), but re-using the mask from the scores */
-  tmpv  = _mm_shuffle_ps(vb_sc, vb_sc, _MM_SHUFFLE(0, 3, 2, 1));
-  mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-  vb_sc = _mm_max_ps(tmpv, vb_sc);
-  tmpv  = _mm_shuffle_ps(vb_k, vb_k, _MM_SHUFFLE(0, 3, 2, 1));
+  tmpv  = vec_shufflepermute4sp(vb_sc, vb_sc, _MM_SHUFFLE(0, 3, 2, 1));
+  mask  = vec_comparegt4sp(tmpv, vb_sc);
+  vb_sc = vec_max4sp(tmpv, vb_sc);
+  tmpv  = vec_shufflepermute4sp(vb_k, vb_k, _MM_SHUFFLE(0, 3, 2, 1));
   vb_k  = esl_sse_select_ps(vb_k, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_j, vb_j, _MM_SHUFFLE(0, 3, 2, 1));
+  tmpv  = vec_shufflepermute4sp(vb_j, vb_j, _MM_SHUFFLE(0, 3, 2, 1));
   vb_j  = esl_sse_select_ps(vb_j, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_d, vb_d, _MM_SHUFFLE(0, 3, 2, 1));
+  tmpv  = vec_shufflepermute4sp(vb_d, vb_d, _MM_SHUFFLE(0, 3, 2, 1));
   vb_d  = esl_sse_select_ps(vb_d, tmpv, mask);
 
-  tmpv  = _mm_shuffle_ps(vb_sc, vb_sc, _MM_SHUFFLE(1, 0, 3, 2));
-  mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-  vb_sc = _mm_max_ps(tmpv, vb_sc);
-  tmpv  = _mm_shuffle_ps(vb_k, vb_k, _MM_SHUFFLE(1, 0, 3, 2));
+  tmpv  = vec_shufflepermute4sp(vb_sc, vb_sc, _MM_SHUFFLE(1, 0, 3, 2));
+  mask  = vec_comparegt4sp(tmpv, vb_sc);
+  vb_sc = vec_max4sp(tmpv, vb_sc);
+  tmpv  = vec_shufflepermute4sp(vb_k, vb_k, _MM_SHUFFLE(1, 0, 3, 2));
   vb_k  = esl_sse_select_ps(vb_k, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_j, vb_j, _MM_SHUFFLE(1, 0, 3, 2));
+  tmpv  = vec_shufflepermute4sp(vb_j, vb_j, _MM_SHUFFLE(1, 0, 3, 2));
   vb_j  = esl_sse_select_ps(vb_j, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_d, vb_d, _MM_SHUFFLE(1, 0, 3, 2));
+  tmpv  = vec_shufflepermute4sp(vb_d, vb_d, _MM_SHUFFLE(1, 0, 3, 2));
   vb_d  = esl_sse_select_ps(vb_d, tmpv, mask);
 
 union {
@@ -913,7 +913,7 @@
   __m128i doffset;
   const int vecwidth = 4;
 
-  doffset = _mm_setr_epi32(0, 1, 2, 3);
+  doffset = vec_setreverse4sw(0, 1, 2, 3);
   
   /* 1. If the wedge problem is either a boundary condition,
    *    or small enough, solve it with inside^T and append
@@ -963,15 +963,15 @@
    */
   W = j0-i0+1;
   best_sc = IMPOSSIBLE;
-  vb_sc = _mm_set1_ps(-eslINFINITY);
-  vb_v = vb_j = vb_d = (__m128) _mm_set1_epi32(-3);
+  vb_sc = vec_splat4sp(-eslINFINITY);
+  vb_v = vb_j = vb_d = (__m128) vec_splat4sw(-3);
 
   for (v = w; v <= y; v++) {
-    vec_v = (__m128) _mm_set1_epi32(v);
+    vec_v = (__m128) vec_splat4sw(v);
     for (jp = 0; jp <= W; jp++) 
       {
 	j = i0-1+jp;
-        vec_j = (__m128) _mm_set1_epi32(j);
+        vec_j = (__m128) vec_splat4sw(j);
         sW = jp/vecwidth;
 	/*for (d = 0; d <= jp; d++) 
 	  if ((sc = alpha[v][j][d] + beta[v][j][d]) > best_sc)
@@ -983,10 +983,10 @@
 	    } */
         for (dp = 0; dp <= sW; dp++)
           {
-            vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
-            tmpv = _mm_add_ps(alpha[v]->vec[j][dp], beta[v]->vec[j][dp]);
-            mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-            vb_sc = _mm_max_ps(tmpv, vb_sc);
+            vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
+            tmpv = vec_add4sp(alpha[v]->vec[j][dp], beta[v]->vec[j][dp]);
+            mask  = vec_comparegt4sp(tmpv, vb_sc);
+            vb_sc = vec_max4sp(tmpv, vb_sc);
             vb_v  = esl_sse_select_ps(vb_v, vec_v, mask);
             vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
             vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -998,11 +998,11 @@
    * not in the split set?
    */
   if (cm->flags & CMH_LOCAL_END) {
-    vec_v = (__m128) _mm_set1_epi32(-1);	/* flag for local alignment */
+    vec_v = (__m128) vec_splat4sw(-1);	/* flag for local alignment */
     for (jp = 0; jp <= W; jp++) 
       {
 	j = i0-1+jp;
-        vec_j = (__m128) _mm_set1_epi32(j);
+        vec_j = (__m128) vec_splat4sw(j);
         sW = jp/vecwidth;
 	/*for (d = 0; d <= jp; d++)
 	  if ((sc = beta[cm->M][j][d]) > best_sc) {
@@ -1013,10 +1013,10 @@
 	  } */
         for (dp = 0; dp <= sW; dp++)
           {
-            vec_d = (__m128) _mm_add_epi32(doffset, _mm_set1_epi32(dp*vecwidth));
+            vec_d = (__m128) vec_add4sw(doffset, vec_splat4sw(dp*vecwidth));
             tmpv = beta[cm->M]->vec[j][dp];
-            mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-            vb_sc = _mm_max_ps(tmpv, vb_sc);
+            mask  = vec_comparegt4sp(tmpv, vb_sc);
+            vb_sc = vec_max4sp(tmpv, vb_sc);
             vb_v  = esl_sse_select_ps(vb_v, vec_v, mask);
             vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
             vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -1033,12 +1033,12 @@
       best_j  = j0;
       best_d  = W;
     } */
-    vec_v = (__m128) _mm_set1_epi32(-2);	/* flag for local alignment */
-    vec_j = (__m128) _mm_set1_epi32(j0);
-    vec_d = (__m128) _mm_set1_epi32(W);
-    tmpv  = _mm_set1_ps(bsc);
-    mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-    vb_sc = _mm_max_ps(tmpv, vb_sc);
+    vec_v = (__m128) vec_splat4sw(-2);	/* flag for local alignment */
+    vec_j = (__m128) vec_splat4sw(j0);
+    vec_d = (__m128) vec_splat4sw(W);
+    tmpv  = vec_splat4sp(bsc);
+    mask  = vec_comparegt4sp(tmpv, vb_sc);
+    vb_sc = vec_max4sp(tmpv, vb_sc);
     vb_v  = esl_sse_select_ps(vb_v, vec_v, mask);
     vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
     vb_d  = esl_sse_select_ps(vb_d, vec_d, mask);
@@ -1058,24 +1058,24 @@
 
   /* determine values corresponding to best score out of our 4x vector */
   /* like esl_sse_hmax(), but re-using the mask from the scores */
-  tmpv  = _mm_shuffle_ps(vb_sc, vb_sc, _MM_SHUFFLE(0, 3, 2, 1));
-  mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-  vb_sc = _mm_max_ps(tmpv, vb_sc);
-  tmpv  = _mm_shuffle_ps(vb_v, vb_v, _MM_SHUFFLE(0, 3, 2, 1));
+  tmpv  = vec_shufflepermute4sp(vb_sc, vb_sc, _MM_SHUFFLE(0, 3, 2, 1));
+  mask  = vec_comparegt4sp(tmpv, vb_sc);
+  vb_sc = vec_max4sp(tmpv, vb_sc);
+  tmpv  = vec_shufflepermute4sp(vb_v, vb_v, _MM_SHUFFLE(0, 3, 2, 1));
   vb_v  = esl_sse_select_ps(vb_v, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_j, vb_j, _MM_SHUFFLE(0, 3, 2, 1));
+  tmpv  = vec_shufflepermute4sp(vb_j, vb_j, _MM_SHUFFLE(0, 3, 2, 1));
   vb_j  = esl_sse_select_ps(vb_j, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_d, vb_d, _MM_SHUFFLE(0, 3, 2, 1));
+  tmpv  = vec_shufflepermute4sp(vb_d, vb_d, _MM_SHUFFLE(0, 3, 2, 1));
   vb_d  = esl_sse_select_ps(vb_d, tmpv, mask);
 
-  tmpv  = _mm_shuffle_ps(vb_sc, vb_sc, _MM_SHUFFLE(1, 0, 3, 2));
-  mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-  vb_sc = _mm_max_ps(tmpv, vb_sc);
-  tmpv  = _mm_shuffle_ps(vb_v, vb_v, _MM_SHUFFLE(1, 0, 3, 2));
+  tmpv  = vec_shufflepermute4sp(vb_sc, vb_sc, _MM_SHUFFLE(1, 0, 3, 2));
+  mask  = vec_comparegt4sp(tmpv, vb_sc);
+  vb_sc = vec_max4sp(tmpv, vb_sc);
+  tmpv  = vec_shufflepermute4sp(vb_v, vb_v, _MM_SHUFFLE(1, 0, 3, 2));
   vb_v  = esl_sse_select_ps(vb_v, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_j, vb_j, _MM_SHUFFLE(1, 0, 3, 2));
+  tmpv  = vec_shufflepermute4sp(vb_j, vb_j, _MM_SHUFFLE(1, 0, 3, 2));
   vb_j  = esl_sse_select_ps(vb_j, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_d, vb_d, _MM_SHUFFLE(1, 0, 3, 2));
+  tmpv  = vec_shufflepermute4sp(vb_d, vb_d, _MM_SHUFFLE(1, 0, 3, 2));
   vb_d  = esl_sse_select_ps(vb_d, tmpv, mask);
 
 union {
@@ -1202,7 +1202,7 @@
   const int vecwidth = 4;
 
   alpha = NULL; beta = NULL;
-  ioffset = _mm_setr_epi32(0, 1, 2, 3);
+  ioffset = vec_setreverse4sw(0, 1, 2, 3);
 
   /* 1. If the V problem is either a boundary condition, or small
    *    enough, solve it with v_inside^T and append the trace to tr.
@@ -1241,12 +1241,12 @@
   /* 4. Find the optimal split: v, ip, jp. 
    */
   best_sc = IMPOSSIBLE;
-  vb_sc = _mm_set1_ps(-eslINFINITY);
-  vb_v = vb_j = vb_i = (__m128) _mm_set1_epi32(-3);
+  vb_sc = vec_splat4sp(-eslINFINITY);
+  vb_v = vb_j = vb_i = (__m128) vec_splat4sw(-3);
   for (v = w; v <= y; v++) {
-    vec_v = (__m128) _mm_set1_epi32(v);
+    vec_v = (__m128) vec_splat4sw(v);
     for (jp = 0; jp <= j0-j1; jp++) {
-      vec_j = (__m128) _mm_set1_epi32(jp+j1);
+      vec_j = (__m128) vec_splat4sw(jp+j1);
       sW = (i1-i0)/vecwidth;
       //for (ip = 0; ip <= i1-i0; ip++) {
       for (ip = 0; ip <= sW; ip++) {
@@ -1257,11 +1257,11 @@
 	    best_i  = ip + i0;
 	    best_j  = jp + j1;
 	  } */
-        vec_i = (__m128) _mm_add_epi32(_mm_set1_epi32(ip*vecwidth), _mm_set1_epi32(i0)); 
-        vec_i = (__m128) _mm_add_epi32((__m128i) vec_i, ioffset);
-        tmpv  = _mm_add_ps(alpha[v]->vec[jp][ip], beta[v]->vec[jp][ip]);
-        mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-        vb_sc = _mm_max_ps(tmpv, vb_sc);
+        vec_i = (__m128) vec_add4sw(vec_splat4sw(ip*vecwidth), vec_splat4sw(i0)); 
+        vec_i = (__m128) vec_add4sw((__m128i) vec_i, ioffset);
+        tmpv  = vec_add4sp(alpha[v]->vec[jp][ip], beta[v]->vec[jp][ip]);
+        mask  = vec_comparegt4sp(tmpv, vb_sc);
+        vb_sc = vec_max4sp(tmpv, vb_sc);
         vb_v  = esl_sse_select_ps(vb_v, vec_v, mask);
         vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
         vb_i  = esl_sse_select_ps(vb_i, vec_i, mask);
@@ -1273,9 +1273,9 @@
    * the split set?
    */
   if (useEL && (cm->flags & CMH_LOCAL_END)) {
-    vec_v = (__m128) _mm_set1_epi32(-1);
+    vec_v = (__m128) vec_splat4sw(-1);
     for (jp = 0; jp <= j0-j1; jp++) {
-      vec_j = (__m128) _mm_set1_epi32(jp+j1);
+      vec_j = (__m128) vec_splat4sw(jp+j1);
       sW = (i1-i0)/vecwidth;
       //for (ip = 0; ip <= i1-i0; ip++) {
       for (ip = 0; ip <= sW; ip++) {
@@ -1285,11 +1285,11 @@
 	  best_i  = ip + i0;
 	  best_j  = jp + j1;
 	} */
-        vec_i = (__m128) _mm_add_epi32(_mm_set1_epi32(ip*vecwidth), _mm_set1_epi32(i0)); 
-        vec_i = (__m128) _mm_add_epi32((__m128i) vec_i, ioffset);
+        vec_i = (__m128) vec_add4sw(vec_splat4sw(ip*vecwidth), vec_splat4sw(i0)); 
+        vec_i = (__m128) vec_add4sw((__m128i) vec_i, ioffset);
         tmpv  = beta[cm->M]->vec[jp][ip];
-        mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-        vb_sc = _mm_max_ps(tmpv, vb_sc);
+        mask  = vec_comparegt4sp(tmpv, vb_sc);
+        vb_sc = vec_max4sp(tmpv, vb_sc);
         vb_v  = esl_sse_select_ps(vb_v, vec_v, mask);
         vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
         vb_i  = esl_sse_select_ps(vb_i, vec_i, mask);
@@ -1306,12 +1306,12 @@
       best_i  = i0;
       best_j  = j0;
     } */
-    vec_v = (__m128) _mm_set1_epi32(-2);
-    vec_j = (__m128) _mm_set1_epi32(j0);
-    vec_i = (__m128) _mm_set1_epi32(i0);
-    tmpv  = _mm_set1_ps(bsc);
-    mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-    vb_sc = _mm_max_ps(tmpv, vb_sc);
+    vec_v = (__m128) vec_splat4sw(-2);
+    vec_j = (__m128) vec_splat4sw(j0);
+    vec_i = (__m128) vec_splat4sw(i0);
+    tmpv  = vec_splat4sp(bsc);
+    mask  = vec_comparegt4sp(tmpv, vb_sc);
+    vb_sc = vec_max4sp(tmpv, vb_sc);
     vb_v  = esl_sse_select_ps(vb_v, vec_v, mask);
     vb_j  = esl_sse_select_ps(vb_j, vec_j, mask);
     vb_i  = esl_sse_select_ps(vb_i, vec_i, mask);
@@ -1324,24 +1324,24 @@
 
   /* determine values corresponding to best score out of our 4x vector */
   /* like esl_sse_hmax(), but re-using the mask from the scores */
-  tmpv  = _mm_shuffle_ps(vb_sc, vb_sc, _MM_SHUFFLE(0, 3, 2, 1));
-  mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-  vb_sc = _mm_max_ps(tmpv, vb_sc);
-  tmpv  = _mm_shuffle_ps(vb_v, vb_v, _MM_SHUFFLE(0, 3, 2, 1));
+  tmpv  = vec_shufflepermute4sp(vb_sc, vb_sc, _MM_SHUFFLE(0, 3, 2, 1));
+  mask  = vec_comparegt4sp(tmpv, vb_sc);
+  vb_sc = vec_max4sp(tmpv, vb_sc);
+  tmpv  = vec_shufflepermute4sp(vb_v, vb_v, _MM_SHUFFLE(0, 3, 2, 1));
   vb_v  = esl_sse_select_ps(vb_v, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_j, vb_j, _MM_SHUFFLE(0, 3, 2, 1));
+  tmpv  = vec_shufflepermute4sp(vb_j, vb_j, _MM_SHUFFLE(0, 3, 2, 1));
   vb_j  = esl_sse_select_ps(vb_j, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_i, vb_i, _MM_SHUFFLE(0, 3, 2, 1));
+  tmpv  = vec_shufflepermute4sp(vb_i, vb_i, _MM_SHUFFLE(0, 3, 2, 1));
   vb_i  = esl_sse_select_ps(vb_i, tmpv, mask);
 
-  tmpv  = _mm_shuffle_ps(vb_sc, vb_sc, _MM_SHUFFLE(1, 0, 3, 2));
-  mask  = _mm_cmpgt_ps(tmpv, vb_sc);
-  vb_sc = _mm_max_ps(tmpv, vb_sc);
-  tmpv  = _mm_shuffle_ps(vb_v, vb_v, _MM_SHUFFLE(1, 0, 3, 2));
+  tmpv  = vec_shufflepermute4sp(vb_sc, vb_sc, _MM_SHUFFLE(1, 0, 3, 2));
+  mask  = vec_comparegt4sp(tmpv, vb_sc);
+  vb_sc = vec_max4sp(tmpv, vb_sc);
+  tmpv  = vec_shufflepermute4sp(vb_v, vb_v, _MM_SHUFFLE(1, 0, 3, 2));
   vb_v  = esl_sse_select_ps(vb_v, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_j, vb_j, _MM_SHUFFLE(1, 0, 3, 2));
+  tmpv  = vec_shufflepermute4sp(vb_j, vb_j, _MM_SHUFFLE(1, 0, 3, 2));
   vb_j  = esl_sse_select_ps(vb_j, tmpv, mask);
-  tmpv  = _mm_shuffle_ps(vb_i, vb_i, _MM_SHUFFLE(1, 0, 3, 2));
+  tmpv  = vec_shufflepermute4sp(vb_i, vb_i, _MM_SHUFFLE(1, 0, 3, 2));
   vb_i  = esl_sse_select_ps(vb_i, tmpv, mask);
 
 union {
@@ -1560,10 +1560,10 @@
 
   /* Allocations and initializations
    */
-  zerov = _mm_setzero_ps();
-  neginfv = _mm_set1_ps(-eslINFINITY);
-  el_self_v = _mm_set1_ps(cm->el_selfsc);
-  doffset = _mm_setr_ps(0.0, 1.0, 2.0, 3.0);
+  zerov = vec_zero4sp();
+  neginfv = vec_splat4sp(-eslINFINITY);
+  el_self_v = vec_splat4sp(cm->el_selfsc);
+  doffset = vec_setreverse4sp(0.0, 1.0, 2.0, 3.0);
   b   = -1;
   bsc = IMPOSSIBLE;
   W   = j0-i0+1;		/* the length of the subsequence -- used in many loops  */
@@ -1661,17 +1661,17 @@
 	      {
 		y = cm->cfirst[v];
 		// alpha[v][j][d] = cm->endsc[v] + (cm->el_selfsc * (d-StateDelta(cm->sttype[v])));
-                tmpv = _mm_mul_ps(el_self_v, _mm_add_ps(_mm_set1_ps((float) dp*vecwidth), doffset));
-		alpha[v]->vec[j][dp] = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), tmpv);
+                tmpv = vec_multiply4sp(el_self_v, vec_add4sp(vec_splat4sp((float) dp*vecwidth), doffset));
+		alpha[v]->vec[j][dp] = vec_add4sp(vec_splat4sp(cm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
-		if (ret_shadow != NULL) shadow[v]->vec[j][dp]  = (__m128) _mm_set1_epi32(USED_EL); 
+		if (ret_shadow != NULL) shadow[v]->vec[j][dp]  = (__m128) vec_splat4sw(USED_EL); 
 		for (yoffset = 0; yoffset < cm->cnum[v]; yoffset++) {
-                  tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
-                  tmpv = _mm_add_ps(alpha[y+yoffset]->vec[j][dp], tscv);
-                  mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                  alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                  tscv = vec_splat4sp(cm->tsc[v][yoffset]);
+                  tmpv = vec_add4sp(alpha[y+yoffset]->vec[j][dp], tscv);
+                  mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                  alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
                   if (ret_shadow != NULL) {
-                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) _mm_set1_epi32(yoffset), mask);
+                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) vec_splat4sw(yoffset), mask);
                   }
                 }
                 //FIXME: SSE conversion is kind of ignoring the possibilty of underflow... this is bad.
@@ -1694,25 +1694,25 @@
              * of how d-k hits the vector boundaries
              */
             vec_access = (float *) (&alpha[z]->vec[j][0]);
-            begr_v = _mm_set1_ps(*vec_access);
-            tmpshad = (__m128) _mm_set1_epi32(0);
+            begr_v = vec_splat4sp(*vec_access);
+            tmpshad = (__m128) vec_splat4sw(0);
 	    for (dp = 0; dp <= sW; dp++)
 	      {
-		alpha[v]->vec[j][dp] = _mm_add_ps(alpha[y]->vec[j][dp], begr_v);
+		alpha[v]->vec[j][dp] = vec_add4sp(alpha[y]->vec[j][dp], begr_v);
 		if (ret_shadow != NULL) shadow[v]->vec[j][dp] = tmpshad;
               }
             for (k = 4; k <=jp; k+=4)
               { 
-                tmpshad = (__m128) _mm_set1_epi32(k);
+                tmpshad = (__m128) vec_splat4sw(k);
                 kp = k/vecwidth;
                 vec_access = (float *) (&alpha[z]->vec[j][kp]);
-                begr_v = _mm_set1_ps(*vec_access);
+                begr_v = vec_splat4sp(*vec_access);
 
                 for (dp = kp; dp <= sW; dp++)
                   {
-                    tmpv = _mm_add_ps(alpha[y]->vec[j-k][dp-kp], begr_v);
-                    mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                    alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                    tmpv = vec_add4sp(alpha[y]->vec[j-k][dp-kp], begr_v);
+                    mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                    alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
                     if (ret_shadow != NULL) {
                       shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], tmpshad, mask);
                     }
@@ -1720,24 +1720,24 @@
               }
             for (k = 1; k <= jp; k+= 4)
               {
-                tmpshad = (__m128) _mm_set1_epi32(k);
+                tmpshad = (__m128) vec_splat4sw(k);
                 kp = k/vecwidth;
                 vec_access = (float *) (&alpha[z]->vec[j][kp]) + 1;
-                begr_v = _mm_set1_ps(*vec_access);
+                begr_v = vec_splat4sp(*vec_access);
 
                 tmpv = esl_sse_rightshift_ps(alpha[y]->vec[j-k][0], neginfv);
-                tmpv = _mm_add_ps(tmpv, begr_v);
-                mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][kp]);
-                alpha[v]->vec[j][kp] = _mm_max_ps(alpha[v]->vec[j][kp], tmpv);
+                tmpv = vec_add4sp(tmpv, begr_v);
+                mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][kp]);
+                alpha[v]->vec[j][kp] = vec_max4sp(alpha[v]->vec[j][kp], tmpv);
                 if (ret_shadow != NULL) {
                   shadow[v]->vec[j][kp] = esl_sse_select_ps(shadow[v]->vec[j][kp], tmpshad, mask);
                 }
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = alt_rightshift_ps(alpha[y]->vec[j-k][dp-kp], alpha[y]->vec[j-k][dp-kp-1]);
-                    tmpv = _mm_add_ps(tmpv, begr_v);
-                    mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                    alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                    tmpv = vec_add4sp(tmpv, begr_v);
+                    mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                    alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
                     if (ret_shadow != NULL) {
                       shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], tmpshad, mask);
                     }
@@ -1745,25 +1745,25 @@
               }
             for (k = 2; k <= jp; k+= 4)
               {
-                tmpshad = (__m128) _mm_set1_epi32(k);
+                tmpshad = (__m128) vec_splat4sw(k);
                 kp = k/vecwidth;
                 vec_access = (float *) (&alpha[z]->vec[j][kp]) + 2;
-                begr_v = _mm_set1_ps(*vec_access);
+                begr_v = vec_splat4sp(*vec_access);
 
-                tmpv = _mm_movelh_ps(neginfv, alpha[y]->vec[j-k][0]);
-                tmpv = _mm_add_ps(tmpv, begr_v);
-                mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][kp]);
-                alpha[v]->vec[j][kp] = _mm_max_ps(alpha[v]->vec[j][kp], tmpv);
+                tmpv = vec_extractlower2spinsertupper2spof4sp(neginfv, alpha[y]->vec[j-k][0]);
+                tmpv = vec_add4sp(tmpv, begr_v);
+                mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][kp]);
+                alpha[v]->vec[j][kp] = vec_max4sp(alpha[v]->vec[j][kp], tmpv);
                 if (ret_shadow != NULL) {
                   shadow[v]->vec[j][kp] = esl_sse_select_ps(shadow[v]->vec[j][kp], tmpshad, mask);
                 }
                 for (dp = kp+1; dp <= sW; dp++)
                   {
-                    tmpv = _mm_movelh_ps(neginfv, alpha[y]->vec[j-k][dp-kp]);
-                    tmpv = _mm_movehl_ps(tmpv, alpha[y]->vec[j-k][dp-kp-1]);
-                    tmpv = _mm_add_ps(tmpv, begr_v);
-                    mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                    alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                    tmpv = vec_extractlower2spinsertupper2spof4sp(neginfv, alpha[y]->vec[j-k][dp-kp]);
+                    tmpv = vec_extractupper2spinsertlower2spof4sp(tmpv, alpha[y]->vec[j-k][dp-kp-1]);
+                    tmpv = vec_add4sp(tmpv, begr_v);
+                    mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                    alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
                     if (ret_shadow != NULL) {
                       shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], tmpshad, mask);
                     }
@@ -1771,24 +1771,24 @@
               }
             for (k = 3; k <= jp; k+= 4)
               {
-                tmpshad = (__m128) _mm_set1_epi32(k);
+                tmpshad = (__m128) vec_splat4sw(k);
                 kp = k/vecwidth;
                 vec_access = (float *) (&alpha[z]->vec[j][kp]) + 3;
-                begr_v = _mm_set1_ps(*vec_access);
+                begr_v = vec_splat4sp(*vec_access);
 
                 tmpv = esl_sse_leftshift_ps(neginfv, alpha[y]->vec[j-k][0]);
-                tmpv = _mm_add_ps(tmpv, begr_v);
-                mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][kp]);
-                alpha[v]->vec[j][kp] = _mm_max_ps(alpha[v]->vec[j][kp], tmpv);
+                tmpv = vec_add4sp(tmpv, begr_v);
+                mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][kp]);
+                alpha[v]->vec[j][kp] = vec_max4sp(alpha[v]->vec[j][kp], tmpv);
                 if (ret_shadow != NULL) {
                   shadow[v]->vec[j][kp] = esl_sse_select_ps(shadow[v]->vec[j][kp], tmpshad, mask);
                 }
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = esl_sse_leftshift_ps(alpha[y]->vec[j-k][dp-kp-1], alpha[y]->vec[j-k][dp-kp]);
-                    tmpv = _mm_add_ps(tmpv, begr_v);
-                    mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                    alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                    tmpv = vec_add4sp(tmpv, begr_v);
+                    mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                    alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
                     if (ret_shadow != NULL) {
                       shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], tmpshad, mask);
                     }
@@ -1813,7 +1813,7 @@
             {
               esc_stale[x] = i0-1;
               for (dp = 0; dp <= W/vecwidth; dp ++) { vec_Pesc[x][dp] = neginfv; }
-              vec_Pesc[x][0] = esl_sse_rightshift_ps(vec_Pesc[x][0], _mm_set1_ps(cm->oesc[v][dsq[i0]*cm->abc->Kp+x]));
+              vec_Pesc[x][0] = esl_sse_rightshift_ps(vec_Pesc[x][0], vec_splat4sp(cm->oesc[v][dsq[i0]*cm->abc->Kp+x]));
             } 
           alpha[v]->vec[i0-1][0] = neginfv; /* jp = 0 */
 	  for (jp = 1; jp <= W; jp++) {
@@ -1823,80 +1823,80 @@
             delta = j>0 ? j - esc_stale[dsq[j]] : 0;
             if (delta == 1) {
               for (dp = sW; dp > 0; dp--) { vec_Pesc[dsq[j]][dp] = alt_rightshift_ps(vec_Pesc[dsq[j]][dp], vec_Pesc[dsq[j]][dp-1]); }
-              vec_Pesc[dsq[j]][0] = alt_rightshift_ps(vec_Pesc[dsq[j]][0], (jp<W) ? _mm_set1_ps(cm->oesc[v][dsq[j+1]*cm->abc->Kp+dsq[j]]) : neginfv);
+              vec_Pesc[dsq[j]][0] = alt_rightshift_ps(vec_Pesc[dsq[j]][0], (jp<W) ? vec_splat4sp(cm->oesc[v][dsq[j+1]*cm->abc->Kp+dsq[j]]) : neginfv);
             }
             else if (delta == 2) {
               for (dp = sW; dp > 0; dp--) {
-                tmpv = _mm_movelh_ps(neginfv, vec_Pesc[dsq[j]][dp]);
-                vec_Pesc[dsq[j]][dp] = _mm_movehl_ps(tmpv, vec_Pesc[dsq[j]][dp-1]);
+                tmpv = vec_extractlower2spinsertupper2spof4sp(neginfv, vec_Pesc[dsq[j]][dp]);
+                vec_Pesc[dsq[j]][dp] = vec_extractupper2spinsertlower2spof4sp(tmpv, vec_Pesc[dsq[j]][dp-1]);
               }
-              tmpv = _mm_setr_ps(jp<W ? cm->oesc[v][dsq[j+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY, cm->oesc[v][dsq[j]*cm->abc->Kp+dsq[j]], -eslINFINITY, -eslINFINITY);
-              vec_Pesc[dsq[j]][0] = _mm_movelh_ps(tmpv, vec_Pesc[dsq[j]][0]);
+              tmpv = vec_setreverse4sp(jp<W ? cm->oesc[v][dsq[j+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY, cm->oesc[v][dsq[j]*cm->abc->Kp+dsq[j]], -eslINFINITY, -eslINFINITY);
+              vec_Pesc[dsq[j]][0] = vec_extractlower2spinsertupper2spof4sp(tmpv, vec_Pesc[dsq[j]][0]);
             } 
             else if (delta == 3) {
               for (dp = sW; dp > 0; dp--) { vec_Pesc[dsq[j]][dp] = esl_sse_leftshift_ps(vec_Pesc[dsq[j]][dp-1], vec_Pesc[dsq[j]][dp]); }
-              tmpv = _mm_setr_ps(-eslINFINITY, jp<W ? cm->oesc[v][dsq[j+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY,
+              tmpv = vec_setreverse4sp(-eslINFINITY, jp<W ? cm->oesc[v][dsq[j+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY,
                                                       cm->oesc[v][dsq[j  ]*cm->abc->Kp+dsq[j]],
                                                       cm->oesc[v][dsq[j-1]*cm->abc->Kp+dsq[j]]);
               vec_Pesc[dsq[j]][0] = esl_sse_leftshift_ps(tmpv, vec_Pesc[dsq[j]][0]);
             }
             else if (delta == 4) {
               for (dp = sW; dp > 0; dp--) { vec_Pesc[dsq[j]][dp] = vec_Pesc[dsq[j]][dp-1]; }
-              vec_Pesc[dsq[j]][0] = _mm_setr_ps(jp<W ? cm->oesc[v][dsq[j+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY,
+              vec_Pesc[dsq[j]][0] = vec_setreverse4sp(jp<W ? cm->oesc[v][dsq[j+1]*cm->abc->Kp+dsq[j]] : -eslINFINITY,
                                                        cm->oesc[v][dsq[j  ]*cm->abc->Kp+dsq[j]],
                                                        cm->oesc[v][dsq[j-1]*cm->abc->Kp+dsq[j]],
                                                        cm->oesc[v][dsq[j-2]*cm->abc->Kp+dsq[j]]);
             }
             if (j>0) esc_stale[dsq[j]] = j; 
 
-            tmpv = _mm_movelh_ps(neginfv, _mm_mul_ps(el_self_v, doffset));
-            alpha[v]->vec[j][0] = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), tmpv);
+            tmpv = vec_extractlower2spinsertupper2spof4sp(neginfv, vec_multiply4sp(el_self_v, doffset));
+            alpha[v]->vec[j][0] = vec_add4sp(vec_splat4sp(cm->endsc[v]), tmpv);
             /* treat EL as emitting only on self transition */
-            if (ret_shadow != NULL) shadow[v]->vec[j][0]  = (__m128) _mm_set1_epi32(USED_EL); 
+            if (ret_shadow != NULL) shadow[v]->vec[j][0]  = (__m128) vec_splat4sw(USED_EL); 
             y = cm->cfirst[v];
             for (yoffset = 0; yoffset < cm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
+              tscv = vec_splat4sp(cm->tsc[v][yoffset]);
               if (j==0) tmpv = neginfv;
               else
-                tmpv = _mm_movelh_ps(neginfv, alpha[y+yoffset]->vec[j-1][0]);
-              tmpv = _mm_add_ps(tmpv, tscv);
-              mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][0]);
-              alpha[v]->vec[j][0] = _mm_max_ps(alpha[v]->vec[j][0], tmpv);
+                tmpv = vec_extractlower2spinsertupper2spof4sp(neginfv, alpha[y+yoffset]->vec[j-1][0]);
+              tmpv = vec_add4sp(tmpv, tscv);
+              mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][0]);
+              alpha[v]->vec[j][0] = vec_max4sp(alpha[v]->vec[j][0], tmpv);
               if (ret_shadow != NULL) {
-                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) _mm_set1_epi32(yoffset), mask);
+                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) vec_splat4sw(yoffset), mask);
               }
             }
-            /*escv = _mm_setr_ps(-eslINFINITY, -eslINFINITY,
+            /*escv = vec_setreverse4sp(-eslINFINITY, -eslINFINITY,
                                j>1?cm->oesc[v][dsq[j-1]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                j>2?cm->oesc[v][dsq[j-2]*cm->abc->Kp+dsq[j]]:-eslINFINITY); */
-            escv = j>0 ? _mm_movehl_ps(vec_Pesc[dsq[j]][0], neginfv) : neginfv;
-            alpha[v]->vec[j][0] = _mm_add_ps(alpha[v]->vec[j][0], escv);
+            escv = j>0 ? vec_extractupper2spinsertlower2spof4sp(vec_Pesc[dsq[j]][0], neginfv) : neginfv;
+            alpha[v]->vec[j][0] = vec_add4sp(alpha[v]->vec[j][0], escv);
 
 	    for (dp = 1; dp <= sW; dp++) 
 	      {
-                tmpv = _mm_mul_ps(el_self_v, _mm_add_ps(_mm_set1_ps((float) dp*vecwidth-2), doffset));
-		alpha[v]->vec[j][dp] = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), tmpv);
+                tmpv = vec_multiply4sp(el_self_v, vec_add4sp(vec_splat4sp((float) dp*vecwidth-2), doffset));
+		alpha[v]->vec[j][dp] = vec_add4sp(vec_splat4sp(cm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
-		if (ret_shadow != NULL) shadow[v]->vec[j][dp] = (__m128) _mm_set1_epi32(USED_EL);
+		if (ret_shadow != NULL) shadow[v]->vec[j][dp] = (__m128) vec_splat4sw(USED_EL);
 		for (yoffset = 0; yoffset < cm->cnum[v]; yoffset++) {
-                  tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
-                  tmpv = _mm_movelh_ps(neginfv, alpha[y+yoffset]->vec[j-1][dp]);
-                  tmpv = _mm_movehl_ps(tmpv, alpha[y+yoffset]->vec[j-1][dp-1]);
-                  tmpv = _mm_add_ps(tmpv, tscv);
-                  mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                  alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                  tscv = vec_splat4sp(cm->tsc[v][yoffset]);
+                  tmpv = vec_extractlower2spinsertupper2spof4sp(neginfv, alpha[y+yoffset]->vec[j-1][dp]);
+                  tmpv = vec_extractupper2spinsertlower2spof4sp(tmpv, alpha[y+yoffset]->vec[j-1][dp-1]);
+                  tmpv = vec_add4sp(tmpv, tscv);
+                  mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                  alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
 		  if (ret_shadow != NULL) {
-                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) _mm_set1_epi32(yoffset), mask);
+                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) vec_splat4sw(yoffset), mask);
                   }
                 }
 		
 		i = j-dp*vecwidth+1;
-                /*escv = _mm_setr_ps(i>0?cm->oesc[v][dsq[i  ]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
+                /*escv = vec_setreverse4sp(i>0?cm->oesc[v][dsq[i  ]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                    i>1?cm->oesc[v][dsq[i-1]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                    i>2?cm->oesc[v][dsq[i-2]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                    i>3?cm->oesc[v][dsq[i-3]*cm->abc->Kp+dsq[j]]:-eslINFINITY);  */
                 escv = vec_Pesc[dsq[j]][dp];
-                alpha[v]->vec[j][dp] = _mm_add_ps(alpha[v]->vec[j][dp], escv);
+                alpha[v]->vec[j][dp] = vec_add4sp(alpha[v]->vec[j][dp], escv);
 	      }
 
             for (x = 0; x < cm->abc->Kp; x++)
@@ -1904,7 +1904,7 @@
                 delta = j - esc_stale[x];
                 if (delta == vecwidth) {
                   for (dp = sW; dp > 0; dp--) { vec_Pesc[x][dp] = vec_Pesc[x][dp-1]; }
-                  vec_Pesc[x][0] = _mm_setr_ps(jp<W ? cm->oesc[v][dsq[j+1]*cm->abc->Kp+x] : -eslINFINITY,
+                  vec_Pesc[x][0] = vec_setreverse4sp(jp<W ? cm->oesc[v][dsq[j+1]*cm->abc->Kp+x] : -eslINFINITY,
                                                       cm->oesc[v][dsq[j  ]*cm->abc->Kp+x],
                                                       cm->oesc[v][dsq[j-1]*cm->abc->Kp+x],
                                                       cm->oesc[v][dsq[j-2]*cm->abc->Kp+x]);
@@ -1917,56 +1917,56 @@
       else if (cm->sttype[v] == ML_st)
 	{
           /* initialize esc vec array */
-          vec_Lesc[0] = _mm_move_ss(neginfv, _mm_set1_ps(cm->oesc[v][dsq[i0]]));
+          vec_Lesc[0] = vec_insert1spintolower4sp(neginfv, vec_splat4sp(cm->oesc[v][dsq[i0]]));
           for (dp = 1; dp <= W/vecwidth; dp++) { vec_Lesc[dp] = neginfv; }
 
 	  for (jp = 0; jp <= W; jp++) {
 	    j = i0-1+jp;
-            tmpv = esl_sse_rightshift_ps(_mm_mul_ps(el_self_v, doffset), neginfv);
-	    alpha[v]->vec[j][0] = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), tmpv);
+            tmpv = esl_sse_rightshift_ps(vec_multiply4sp(el_self_v, doffset), neginfv);
+	    alpha[v]->vec[j][0] = vec_add4sp(vec_splat4sp(cm->endsc[v]), tmpv);
             /* treat EL as emitting only on self transition */
-            if (ret_shadow != NULL) shadow[v]->vec[j][0] = (__m128) _mm_set1_epi32(USED_EL);
+            if (ret_shadow != NULL) shadow[v]->vec[j][0] = (__m128) vec_splat4sw(USED_EL);
             y = cm->cfirst[v];
             for (yoffset = 0; yoffset < cm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
+              tscv = vec_splat4sp(cm->tsc[v][yoffset]);
               tmpv = esl_sse_rightshift_ps(alpha[y+yoffset]->vec[j][0], neginfv);
-              tmpv = _mm_add_ps(tmpv, tscv);
-              mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][0]);
-              alpha[v]->vec[j][0] = _mm_max_ps(alpha[v]->vec[j][0], tmpv);
+              tmpv = vec_add4sp(tmpv, tscv);
+              mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][0]);
+              alpha[v]->vec[j][0] = vec_max4sp(alpha[v]->vec[j][0], tmpv);
               if (ret_shadow != NULL) {
-                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) _mm_set1_epi32(yoffset), mask);
+                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) vec_splat4sw(yoffset), mask);
               }
             }
-            escv = _mm_move_ss(vec_Lesc[0], neginfv);
-            alpha[v]->vec[j][0] = _mm_add_ps(alpha[v]->vec[j][0], escv);
+            escv = vec_insert1spintolower4sp(vec_Lesc[0], neginfv);
+            alpha[v]->vec[j][0] = vec_add4sp(alpha[v]->vec[j][0], escv);
 
             sW = jp/vecwidth;
 	    for (dp = 1; dp <= sW; dp++)
 	      {
-                tmpv = _mm_mul_ps(el_self_v, _mm_add_ps(_mm_set1_ps((float) dp*vecwidth - 1), doffset));
-		alpha[v]->vec[j][dp] = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), tmpv);
+                tmpv = vec_multiply4sp(el_self_v, vec_add4sp(vec_splat4sp((float) dp*vecwidth - 1), doffset));
+		alpha[v]->vec[j][dp] = vec_add4sp(vec_splat4sp(cm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
-		if (ret_shadow != NULL) shadow[v]->vec[j][dp] = (__m128) _mm_set1_epi32(USED_EL);
+		if (ret_shadow != NULL) shadow[v]->vec[j][dp] = (__m128) vec_splat4sw(USED_EL);
 		for (yoffset = 0; yoffset < cm->cnum[v]; yoffset++) {
-                  tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
+                  tscv = vec_splat4sp(cm->tsc[v][yoffset]);
                   tmpv = alt_rightshift_ps(alpha[y+yoffset]->vec[j][dp], alpha[y+yoffset]->vec[j][dp-1]);
-                  tmpv = _mm_add_ps(tmpv, tscv);
-                  mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                  alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                  tmpv = vec_add4sp(tmpv, tscv);
+                  mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                  alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
 		  if (ret_shadow != NULL) {
-                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) _mm_set1_epi32(yoffset), mask);
+                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) vec_splat4sw(yoffset), mask);
                   }
                 }
 		
 		i = j-dp*vecwidth+1;
                 escv = vec_Lesc[dp];
-                alpha[v]->vec[j][dp] = _mm_add_ps(alpha[v]->vec[j][dp], escv);
+                alpha[v]->vec[j][dp] = vec_add4sp(alpha[v]->vec[j][dp], escv);
 	      }
 
             /* slide esc vec array over by one */
             if (sW < W/vecwidth) sW++;
             for (dp = sW; dp > 0; dp--) { vec_Lesc[dp] = alt_rightshift_ps(vec_Lesc[dp], vec_Lesc[dp-1]); }
-            vec_Lesc[0] = alt_rightshift_ps(vec_Lesc[0], (jp<W-1) ? _mm_set1_ps(cm->oesc[v][dsq[j+2]]) : neginfv);
+            vec_Lesc[0] = alt_rightshift_ps(vec_Lesc[0], (jp<W-1) ? vec_splat4sp(cm->oesc[v][dsq[j+2]]) : neginfv);
 	  }
 	}
       /* The self-transition loop on IL_st will need to be completely serialized, since
@@ -1975,38 +1975,38 @@
       else if (cm->sttype[v] == IL_st)
 	{
           /* initialize esc vec array */
-          vec_Lesc[0] = _mm_move_ss(neginfv, _mm_set1_ps(cm->oesc[v][dsq[i0]]));
+          vec_Lesc[0] = vec_insert1spintolower4sp(neginfv, vec_splat4sp(cm->oesc[v][dsq[i0]]));
           for (dp = 1; dp <= W/vecwidth; dp++) { vec_Lesc[dp] = neginfv; }
 
 	  for (jp = 0; jp <= W; jp++) {
 	    j = i0-1+jp;
-            tmpv = esl_sse_rightshift_ps(_mm_mul_ps(el_self_v, doffset), neginfv);
-            //tmpv = _mm_mul_ps(el_self_v, esl_sse_rightshift_ps(doffset, neginfv));
-	    alpha[v]->vec[j][0] = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), tmpv);
+            tmpv = esl_sse_rightshift_ps(vec_multiply4sp(el_self_v, doffset), neginfv);
+            //tmpv = vec_multiply4sp(el_self_v, esl_sse_rightshift_ps(doffset, neginfv));
+	    alpha[v]->vec[j][0] = vec_add4sp(vec_splat4sp(cm->endsc[v]), tmpv);
             /* treat EL as emitting only on self transition */
-            if (ret_shadow != NULL) shadow[v]->vec[j][0] = (__m128) _mm_set1_epi32(USED_EL);
+            if (ret_shadow != NULL) shadow[v]->vec[j][0] = (__m128) vec_splat4sw(USED_EL);
             y = cm->cfirst[v];
             for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
+              tscv = vec_splat4sp(cm->tsc[v][yoffset]);
               tmpv = esl_sse_rightshift_ps(alpha[y+yoffset]->vec[j][0], neginfv);
-              tmpv = _mm_add_ps(tmpv, tscv);
-              mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][0]);
-              alpha[v]->vec[j][0] = _mm_max_ps(alpha[v]->vec[j][0], tmpv);
+              tmpv = vec_add4sp(tmpv, tscv);
+              mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][0]);
+              alpha[v]->vec[j][0] = vec_max4sp(alpha[v]->vec[j][0], tmpv);
               if (ret_shadow != NULL) {
-                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) _mm_set1_epi32(yoffset), mask);
+                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) vec_splat4sw(yoffset), mask);
               }
             }
-            escv = _mm_move_ss(vec_Lesc[0], neginfv);
-            alpha[v]->vec[j][0] = _mm_add_ps(alpha[v]->vec[j][0], escv);
+            escv = vec_insert1spintolower4sp(vec_Lesc[0], neginfv);
+            alpha[v]->vec[j][0] = vec_add4sp(alpha[v]->vec[j][0], escv);
             /* handle yoffset = 0, the self-transition case, seaparately */
-            tscv = _mm_set1_ps(cm->tsc[v][0]);
+            tscv = vec_splat4sp(cm->tsc[v][0]);
             for (k = 2; k < vecwidth; k++) {
               tmpv = esl_sse_rightshift_ps(alpha[y]->vec[j][0], neginfv);
-              tmpv = _mm_add_ps(escv, _mm_add_ps(tscv, tmpv));
-              mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][0]);
-              alpha[v]->vec[j][0] = _mm_max_ps(alpha[v]->vec[j][0], tmpv);
+              tmpv = vec_add4sp(escv, vec_add4sp(tscv, tmpv));
+              mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][0]);
+              alpha[v]->vec[j][0] = vec_max4sp(alpha[v]->vec[j][0], tmpv);
               if (ret_shadow != NULL) {
-                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) _mm_set1_epi32(0), mask);
+                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) vec_splat4sw(0), mask);
               }
               /* could make this a do-while on whether any values in mask are set */
             }
@@ -2014,34 +2014,34 @@
             sW = jp/vecwidth;
 	    for (dp = 1; dp <= sW; dp++)
 	      {
-                tmpv = _mm_mul_ps(el_self_v, _mm_add_ps(_mm_set1_ps((float) dp*vecwidth - 1), doffset));
-		alpha[v]->vec[j][dp] = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), tmpv);
+                tmpv = vec_multiply4sp(el_self_v, vec_add4sp(vec_splat4sp((float) dp*vecwidth - 1), doffset));
+		alpha[v]->vec[j][dp] = vec_add4sp(vec_splat4sp(cm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
-		if (ret_shadow != NULL) shadow[v]->vec[j][dp] = (__m128) _mm_set1_epi32(USED_EL);
+		if (ret_shadow != NULL) shadow[v]->vec[j][dp] = (__m128) vec_splat4sw(USED_EL);
 		for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
-                  tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
+                  tscv = vec_splat4sp(cm->tsc[v][yoffset]);
                   tmpv = alt_rightshift_ps(alpha[y+yoffset]->vec[j][dp], alpha[y+yoffset]->vec[j][dp-1]);
-                  tmpv = _mm_add_ps(tmpv, tscv);
-                  mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                  alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                  tmpv = vec_add4sp(tmpv, tscv);
+                  mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                  alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
 		  if (ret_shadow != NULL) {
-                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) _mm_set1_epi32(yoffset), mask);
+                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) vec_splat4sw(yoffset), mask);
                   }
                 }
 		
 		i = j-dp*vecwidth+1;
                 escv = vec_Lesc[dp];
-                alpha[v]->vec[j][dp] = _mm_add_ps(alpha[v]->vec[j][dp], escv);
+                alpha[v]->vec[j][dp] = vec_add4sp(alpha[v]->vec[j][dp], escv);
 
                 /* handle yoffset = 0, the self-transition case, seaparately */
-                tscv = _mm_set1_ps(cm->tsc[v][0]);
+                tscv = vec_splat4sp(cm->tsc[v][0]);
                 for (k = 0; k < vecwidth; k++) {
                   tmpv = alt_rightshift_ps(alpha[y]->vec[j][dp], alpha[y]->vec[j][dp-1]);
-                  tmpv = _mm_add_ps(escv, _mm_add_ps(tscv, tmpv));
-                  mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                  alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                  tmpv = vec_add4sp(escv, vec_add4sp(tscv, tmpv));
+                  mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                  alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
                   if (ret_shadow != NULL) {
-                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) _mm_set1_epi32(0), mask);
+                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) vec_splat4sw(0), mask);
                   }
                   /* could make this a do-while on whether any values in mask are set */
                 }
@@ -2050,7 +2050,7 @@
             /* slide esc vec array over by one */
             if (sW < W/vecwidth) sW++;
             for (dp = sW; dp > 0; dp--) { vec_Lesc[dp] = alt_rightshift_ps(vec_Lesc[dp], vec_Lesc[dp-1]); }
-            vec_Lesc[0] = alt_rightshift_ps(vec_Lesc[0], (jp<W-1) ? _mm_set1_ps(cm->oesc[v][dsq[j+2]]) : neginfv);
+            vec_Lesc[0] = alt_rightshift_ps(vec_Lesc[0], (jp<W-1) ? vec_splat4sp(cm->oesc[v][dsq[j+2]]) : neginfv);
 	  }
 	}
       else if (cm->sttype[v] == IR_st || cm->sttype[v] == MR_st)
@@ -2058,50 +2058,50 @@
           alpha[v]->vec[i0-1][0] = neginfv; /* jp = 0 */
 	  for (jp = 1; jp <= W; jp++) {
 	    j = i0-1+jp;
-            tmpv = esl_sse_rightshift_ps(_mm_mul_ps(el_self_v, doffset), neginfv);
-	    alpha[v]->vec[j][0] = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), tmpv);
+            tmpv = esl_sse_rightshift_ps(vec_multiply4sp(el_self_v, doffset), neginfv);
+	    alpha[v]->vec[j][0] = vec_add4sp(vec_splat4sp(cm->endsc[v]), tmpv);
             /* treat EL as emitting only on self transition */
-            if (ret_shadow != NULL) shadow[v]->vec[j][0] = (__m128) _mm_set1_epi32(USED_EL);
+            if (ret_shadow != NULL) shadow[v]->vec[j][0] = (__m128) vec_splat4sw(USED_EL);
             y = cm->cfirst[v];
             for (yoffset = 0; yoffset < cm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
+              tscv = vec_splat4sp(cm->tsc[v][yoffset]);
               if (j==0) tmpv = neginfv;
               else
                 tmpv = esl_sse_rightshift_ps(alpha[y+yoffset]->vec[j-1][0], neginfv);
-              tmpv = _mm_add_ps(tmpv, tscv);
-              mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][0]);
-              alpha[v]->vec[j][0] = _mm_max_ps(alpha[v]->vec[j][0], tmpv);
+              tmpv = vec_add4sp(tmpv, tscv);
+              mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][0]);
+              alpha[v]->vec[j][0] = vec_max4sp(alpha[v]->vec[j][0], tmpv);
               if (ret_shadow != NULL) {
-                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) _mm_set1_epi32(yoffset), mask);
+                shadow[v]->vec[j][0] = esl_sse_select_ps(shadow[v]->vec[j][0], (__m128) vec_splat4sw(yoffset), mask);
               }
             }
             if (j==0) escv = neginfv;
             else
-              escv = _mm_setr_ps(-eslINFINITY, cm->oesc[v][dsq[j]], cm->oesc[v][dsq[j]], cm->oesc[v][dsq[j]]);
-            alpha[v]->vec[j][0] = _mm_add_ps(alpha[v]->vec[j][0], escv);
+              escv = vec_setreverse4sp(-eslINFINITY, cm->oesc[v][dsq[j]], cm->oesc[v][dsq[j]], cm->oesc[v][dsq[j]]);
+            alpha[v]->vec[j][0] = vec_add4sp(alpha[v]->vec[j][0], escv);
 
             sW = jp/vecwidth;
             if (j==0) escv = neginfv;
             else
-              escv = _mm_set1_ps(cm->oesc[v][dsq[j]]);
+              escv = vec_splat4sp(cm->oesc[v][dsq[j]]);
 	    for (dp = 1; dp <= sW; dp++)
 	      {
-                tmpv = _mm_mul_ps(el_self_v, _mm_add_ps(_mm_set1_ps((float) dp*vecwidth - 1), doffset));
-		alpha[v]->vec[j][dp] = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), tmpv);
+                tmpv = vec_multiply4sp(el_self_v, vec_add4sp(vec_splat4sp((float) dp*vecwidth - 1), doffset));
+		alpha[v]->vec[j][dp] = vec_add4sp(vec_splat4sp(cm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
-		if (ret_shadow != NULL) shadow[v]->vec[j][dp] = (__m128) _mm_set1_epi32(USED_EL);
+		if (ret_shadow != NULL) shadow[v]->vec[j][dp] = (__m128) vec_splat4sw(USED_EL);
 		for (yoffset = 0; yoffset < cm->cnum[v]; yoffset++) {
-                  tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
+                  tscv = vec_splat4sp(cm->tsc[v][yoffset]);
                   tmpv = alt_rightshift_ps(alpha[y+yoffset]->vec[j-1][dp], alpha[y+yoffset]->vec[j-1][dp-1]);
-                  tmpv = _mm_add_ps(tmpv, tscv);
-                  mask = _mm_cmpgt_ps(tmpv, alpha[v]->vec[j][dp]);
-                  alpha[v]->vec[j][dp] = _mm_max_ps(alpha[v]->vec[j][dp], tmpv);
+                  tmpv = vec_add4sp(tmpv, tscv);
+                  mask = vec_comparegt4sp(tmpv, alpha[v]->vec[j][dp]);
+                  alpha[v]->vec[j][dp] = vec_max4sp(alpha[v]->vec[j][dp], tmpv);
 		  if (ret_shadow != NULL) {
-                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) _mm_set1_epi32(yoffset), mask);
+                    shadow[v]->vec[j][dp] = esl_sse_select_ps(shadow[v]->vec[j][dp], (__m128) vec_splat4sw(yoffset), mask);
                   }
                 }
 		
-                alpha[v]->vec[j][dp] = _mm_add_ps(alpha[v]->vec[j][dp], escv);
+                alpha[v]->vec[j][dp] = vec_add4sp(alpha[v]->vec[j][dp], escv);
 	      }
 	  }
 	}				/* finished calculating deck v. */
@@ -2289,12 +2289,12 @@
   float   *vec_access;
   const int vecwidth = 4;
 
-  doffset = _mm_setr_ps(0.0, 1.0, 2.0, 3.0);
-  el_self_v = _mm_set1_ps(cm->el_selfsc);
+  doffset = vec_setreverse4sp(0.0, 1.0, 2.0, 3.0);
+  el_self_v = vec_splat4sp(cm->el_selfsc);
 
   /* Allocations and initializations
    */
-  neginfv = _mm_set1_ps(-eslINFINITY);
+  neginfv = vec_splat4sp(-eslINFINITY);
   W = j0-i0+1;		/* the length of the subsequence: used in many loops */
 
   			/* if caller didn't give us a deck pool, make one */
@@ -2453,37 +2453,37 @@
 	    for (y = cm->plast[v]-cm->pnum[v]+1; y <= cm->plast[v]; y++) {
 	      if (y < vroot) continue; /* deal with split sets */
 	      voffset = v - cm->cfirst[y]; /* gotta calculate the transition score index for t_y(v) */
-              tscv = _mm_set1_ps(cm->tsc[y][voffset]);
+              tscv = vec_splat4sp(cm->tsc[y][voffset]);
 
 	      switch(cm->sttype[y]) {
 	      case MP_st: 
 		//if (j == j0 || d == jp) continue; /* boundary condition */
 		if (j == j0) continue; /* boundary condition */
                 if (dp == (jp+1)/vecwidth) {
-                  tmpv = _mm_movehl_ps(neginfv, beta[y]->vec[j+1][dp]);
+                  tmpv = vec_extractupper2spinsertlower2spof4sp(neginfv, beta[y]->vec[j+1][dp]);
                 }
                 else {
-                  tmpv = _mm_movelh_ps(neginfv, beta[y]->vec[j+1][dp+1]);
-                  tmpv = _mm_movehl_ps(tmpv, beta[y]->vec[j+1][dp]);
+                  tmpv = vec_extractlower2spinsertupper2spof4sp(neginfv, beta[y]->vec[j+1][dp+1]);
+                  tmpv = vec_extractupper2spinsertlower2spof4sp(tmpv, beta[y]->vec[j+1][dp]);
                 }
 
                 //escore = cm->oesc[y][dsq[i-1]*cm->abc->Kp+dsq[j+1]];
                 escv = j==j0 ? neginfv : 
-                       _mm_setr_ps(i>1?cm->oesc[y][dsq[i-1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
+                       vec_setreverse4sp(i>1?cm->oesc[y][dsq[i-1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i>2?cm->oesc[y][dsq[i-2]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i>3?cm->oesc[y][dsq[i-3]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i>4?cm->oesc[y][dsq[i-4]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY);
 		
-                tmpv = _mm_add_ps(tmpv, tscv);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[j][dp] = _mm_max_ps(beta[v]->vec[j][dp], tmpv);
+                tmpv = vec_add4sp(tmpv, tscv);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[j][dp] = vec_max4sp(beta[v]->vec[j][dp], tmpv);
 		/*if ((sc = beta[y][j+1][d+2] + cm->tsc[y][voffset] + escore) > beta[v][j][d])
 		  beta[v][j][d] = sc; */
 		break;
 
 	      case ML_st:
 		//escore = cm->oesc[y][dsq[i-1]];
-                escv = _mm_setr_ps(i>1?cm->oesc[y][dsq[i-1]]:-eslINFINITY,
+                escv = vec_setreverse4sp(i>1?cm->oesc[y][dsq[i-1]]:-eslINFINITY,
                                    i>2?cm->oesc[y][dsq[i-2]]:-eslINFINITY,
                                    i>3?cm->oesc[y][dsq[i-3]]:-eslINFINITY,
                                    i>4?cm->oesc[y][dsq[i-4]]:-eslINFINITY);
@@ -2494,16 +2494,16 @@
                 else
                   tmpv = esl_sse_leftshift_ps(beta[y]->vec[j][dp], beta[y]->vec[j][dp+1]);
 
-                tmpv = _mm_add_ps(tmpv, tscv);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[j][dp] = _mm_max_ps(beta[v]->vec[j][dp], tmpv);
+                tmpv = vec_add4sp(tmpv, tscv);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[j][dp] = vec_max4sp(beta[v]->vec[j][dp], tmpv);
 		/*if ((sc = beta[y][j][d+1] + cm->tsc[y][voffset] + escore) > beta[v][j][d])
 		  beta[v][j][d] = sc; */
 		break;
 		  
 	      case IL_st: 
 		//escore = cm->oesc[y][dsq[i-1]];
-                escv = _mm_setr_ps(i>1?cm->oesc[y][dsq[i-1]]:-eslINFINITY,
+                escv = vec_setreverse4sp(i>1?cm->oesc[y][dsq[i-1]]:-eslINFINITY,
                                    i>2?cm->oesc[y][dsq[i-2]]:-eslINFINITY,
                                    i>3?cm->oesc[y][dsq[i-3]]:-eslINFINITY,
                                    i>4?cm->oesc[y][dsq[i-4]]:-eslINFINITY);
@@ -2516,9 +2516,9 @@
                     else
                       tmpv = esl_sse_leftshift_ps(beta[y]->vec[j][dp], beta[y]->vec[j][dp+1]);
 
-                    tmpv = _mm_add_ps(tmpv, tscv);
-                    tmpv = _mm_add_ps(tmpv, escv);
-                    beta[v]->vec[j][dp] = _mm_max_ps(beta[v]->vec[j][dp], tmpv);
+                    tmpv = vec_add4sp(tmpv, tscv);
+                    tmpv = vec_add4sp(tmpv, escv);
+                    beta[v]->vec[j][dp] = vec_max4sp(beta[v]->vec[j][dp], tmpv);
                   }
                 }
                 else {
@@ -2527,9 +2527,9 @@
                   else
                     tmpv = esl_sse_leftshift_ps(beta[y]->vec[j][dp], beta[y]->vec[j][dp+1]);
 
-                  tmpv = _mm_add_ps(tmpv, tscv);
-                  tmpv = _mm_add_ps(tmpv, escv);
-                  beta[v]->vec[j][dp] = _mm_max_ps(beta[v]->vec[j][dp], tmpv);
+                  tmpv = vec_add4sp(tmpv, tscv);
+                  tmpv = vec_add4sp(tmpv, escv);
+                  beta[v]->vec[j][dp] = vec_max4sp(beta[v]->vec[j][dp], tmpv);
                 }
 		/*if ((sc = beta[y][j][d+1] + cm->tsc[y][voffset] + escore) > beta[v][j][d])
 		  beta[v][j][d] = sc; */
@@ -2544,11 +2544,11 @@
                   tmpv = esl_sse_leftshift_ps(beta[y]->vec[j+1][dp], beta[y]->vec[j+1][dp+1]);
 		  
 		//escore = cm->oesc[y][dsq[j+1]];
-		escv = j==j0 ? neginfv : _mm_set1_ps(cm->oesc[y][dsq[j+1]]);
+		escv = j==j0 ? neginfv : vec_splat4sp(cm->oesc[y][dsq[j+1]]);
 
-                tmpv = _mm_add_ps(tmpv, tscv);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[j][dp] = _mm_max_ps(beta[v]->vec[j][dp], tmpv);
+                tmpv = vec_add4sp(tmpv, tscv);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[j][dp] = vec_max4sp(beta[v]->vec[j][dp], tmpv);
 		/* if ((sc = beta[y][j+1][d+1] + cm->tsc[y][voffset] + escore) > beta[v][j][d])
 		  beta[v][j][d] = sc; */
 		break;
@@ -2556,8 +2556,8 @@
 	      case S_st:
 	      case E_st:
 	      case D_st:
-                tmpv = _mm_add_ps(beta[y]->vec[j][dp], tscv);
-                beta[v]->vec[j][dp] = _mm_max_ps(beta[v]->vec[j][dp], tmpv);
+                tmpv = vec_add4sp(beta[y]->vec[j][dp], tscv);
+                beta[v]->vec[j][dp] = vec_max4sp(beta[v]->vec[j][dp], tmpv);
 		/* if ((sc = beta[y][j][d] + cm->tsc[y][voffset]) > beta[v][j][d])
 		  beta[v][j][d] = sc; */
 		break;
@@ -2581,35 +2581,35 @@
 	  for (dp = 0; dp <= sW; dp++) 
 	    {
 	      i = j-dp*vecwidth+1;
-              loop_v = _mm_mul_ps(el_self_v, _mm_add_ps(_mm_set1_ps((float) dp*vecwidth), doffset));
+              loop_v = vec_multiply4sp(el_self_v, vec_add4sp(vec_splat4sp((float) dp*vecwidth), doffset));
 	      switch (cm->sttype[v]) {
 	      case MP_st: 
 		//if (j == j0 || d == jp) continue; /* boundary condition */
                 if (dp == (jp+1)/vecwidth)
-                  tmpv = _mm_movehl_ps(neginfv, beta[v]->vec[j+1][dp]);
+                  tmpv = vec_extractupper2spinsertlower2spof4sp(neginfv, beta[v]->vec[j+1][dp]);
                 else {
-                  tmpv = _mm_movelh_ps(neginfv, beta[v]->vec[j+1][dp+1]);
-                  tmpv = _mm_movehl_ps(tmpv, beta[v]->vec[j+1][dp]);
+                  tmpv = vec_extractlower2spinsertupper2spof4sp(neginfv, beta[v]->vec[j+1][dp+1]);
+                  tmpv = vec_extractupper2spinsertlower2spof4sp(tmpv, beta[v]->vec[j+1][dp]);
                 }
 
 		//escore = cm->oesc[v][dsq[i-1]*cm->abc->K+dsq[j+1]];
                 escv = j==j0 ? neginfv:
-                       _mm_setr_ps(i>1?cm->oesc[v][dsq[i-1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
+                       vec_setreverse4sp(i>1?cm->oesc[v][dsq[i-1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i>2?cm->oesc[v][dsq[i-2]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i>3?cm->oesc[v][dsq[i-3]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i>4?cm->oesc[v][dsq[i-4]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY);
 
-                tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->endsc[v]));
-                tmpv = _mm_add_ps(tmpv, loop_v);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[cm->M]->vec[j][dp] = _mm_max_ps(beta[cm->M]->vec[j][dp], tmpv);
+                tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->endsc[v]));
+                tmpv = vec_add4sp(tmpv, loop_v);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[cm->M]->vec[j][dp] = vec_max4sp(beta[cm->M]->vec[j][dp], tmpv);
 		/*if ((sc = beta[v][j+1][d+2] + cm->endsc[v] + 
 		     (cm->el_selfsc * d) + escore) > beta[cm->M][j][d])
 		  beta[cm->M][j][d] = sc; */
 		break;
 	      case ML_st:
 		//escore = cm->oesc[v][dsq[i-1]];
-                escv = _mm_setr_ps(i>1?cm->oesc[v][dsq[i-1]]:-eslINFINITY,
+                escv = vec_setreverse4sp(i>1?cm->oesc[v][dsq[i-1]]:-eslINFINITY,
                                    i>2?cm->oesc[v][dsq[i-2]]:-eslINFINITY,
                                    i>3?cm->oesc[v][dsq[i-3]]:-eslINFINITY,
                                    i>4?cm->oesc[v][dsq[i-4]]:-eslINFINITY);
@@ -2620,17 +2620,17 @@
                 else
                   tmpv = esl_sse_leftshift_ps(beta[v]->vec[j][dp], beta[v]->vec[j][dp+1]);
 
-                tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->endsc[v]));
-                tmpv = _mm_add_ps(tmpv, loop_v);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[cm->M]->vec[j][dp] = _mm_max_ps(beta[cm->M]->vec[j][dp], tmpv);
+                tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->endsc[v]));
+                tmpv = vec_add4sp(tmpv, loop_v);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[cm->M]->vec[j][dp] = vec_max4sp(beta[cm->M]->vec[j][dp], tmpv);
 		/*if ((sc = beta[v][j][d+1] + cm->endsc[v] + 
 		     (cm->el_selfsc * d) + escore) > beta[cm->M][j][d])
 		  beta[cm->M][j][d] = sc; */
 		break;
 	      case IL_st:
 		//escore = cm->oesc[v][dsq[i-1]];
-                escv = _mm_setr_ps(i>1?cm->oesc[v][dsq[i-1]]:-eslINFINITY,
+                escv = vec_setreverse4sp(i>1?cm->oesc[v][dsq[i-1]]:-eslINFINITY,
                                    i>2?cm->oesc[v][dsq[i-2]]:-eslINFINITY,
                                    i>3?cm->oesc[v][dsq[i-3]]:-eslINFINITY,
                                    i>4?cm->oesc[v][dsq[i-4]]:-eslINFINITY);
@@ -2648,10 +2648,10 @@
                     else
                       tmpv = esl_sse_leftshift_ps(beta[v]->vec[j][dp], beta[v]->vec[j][dp+1]);
 
-                    tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->endsc[v]));
-                    tmpv = _mm_add_ps(tmpv, loop_v);
-                    tmpv = _mm_add_ps(tmpv, escv);
-                    beta[cm->M]->vec[j][dp] = _mm_max_ps(beta[cm->M]->vec[j][dp], tmpv);
+                    tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->endsc[v]));
+                    tmpv = vec_add4sp(tmpv, loop_v);
+                    tmpv = vec_add4sp(tmpv, escv);
+                    beta[cm->M]->vec[j][dp] = vec_max4sp(beta[cm->M]->vec[j][dp], tmpv);
                   }
                 }
                 else {
@@ -2661,10 +2661,10 @@
                   else
                     tmpv = esl_sse_leftshift_ps(beta[v]->vec[j][dp], beta[v]->vec[j][dp+1]);
 
-                  tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->endsc[v]));
-                  tmpv = _mm_add_ps(tmpv, loop_v);
-                  tmpv = _mm_add_ps(tmpv, escv);
-                  beta[cm->M]->vec[j][dp] = _mm_max_ps(beta[cm->M]->vec[j][dp], tmpv);
+                  tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->endsc[v]));
+                  tmpv = vec_add4sp(tmpv, loop_v);
+                  tmpv = vec_add4sp(tmpv, escv);
+                  beta[cm->M]->vec[j][dp] = vec_max4sp(beta[cm->M]->vec[j][dp], tmpv);
                 }
 		/*if ((sc = beta[v][j][d+1] + cm->endsc[v] + 
 		     (cm->el_selfsc * d) + escore) > beta[cm->M][j][d])
@@ -2679,12 +2679,12 @@
                   tmpv = esl_sse_leftshift_ps(beta[v]->vec[j+1][dp], beta[v]->vec[j+1][dp+1]);
 
 		//escore = cm->oesc[v][dsq[j+1]];
-                escv = j==j0 ? neginfv : _mm_set1_ps(cm->oesc[v][dsq[j+1]]);
+                escv = j==j0 ? neginfv : vec_splat4sp(cm->oesc[v][dsq[j+1]]);
 
-                tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->endsc[v]));
-                tmpv = _mm_add_ps(tmpv, loop_v);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[cm->M]->vec[j][dp] = _mm_max_ps(beta[cm->M]->vec[j][dp], tmpv);
+                tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->endsc[v]));
+                tmpv = vec_add4sp(tmpv, loop_v);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[cm->M]->vec[j][dp] = vec_max4sp(beta[cm->M]->vec[j][dp], tmpv);
 		/* if ((sc = beta[v][j+1][d+1] + cm->endsc[v] + 
 		     (cm->el_selfsc * d) + escore) > beta[cm->M][j][d])
 		  beta[cm->M][j][d] = sc; */
@@ -2692,9 +2692,9 @@
 	      case S_st:
 	      case D_st:
 	      case E_st:
-                tmpv = _mm_add_ps(beta[v]->vec[j][dp], _mm_set1_ps(cm->endsc[v]));
-                tmpv = _mm_add_ps(tmpv, loop_v);
-                beta[cm->M]->vec[j][dp] = _mm_max_ps(beta[cm->M]->vec[j][dp], tmpv);
+                tmpv = vec_add4sp(beta[v]->vec[j][dp], vec_splat4sp(cm->endsc[v]));
+                tmpv = vec_add4sp(tmpv, loop_v);
+                beta[cm->M]->vec[j][dp] = vec_max4sp(beta[cm->M]->vec[j][dp], tmpv);
 		/*if ((sc = beta[v][j][d] + cm->endsc[v] +
 		     (cm->el_selfsc * d)) > beta[cm->M][j][d])
 		  beta[cm->M][j][d] = sc; */
@@ -2852,9 +2852,9 @@
   float   *vec_access;
   const int vecwidth = 4;
 
-  neginfv = _mm_set1_ps(-eslINFINITY);
-  el_self_v = _mm_set1_ps(cm->el_selfsc);
-  ioffset = _mm_setr_ps(0.0, -1.0, -2.0, -3.0);
+  neginfv = vec_splat4sp(-eslINFINITY);
+  el_self_v = vec_splat4sp(cm->el_selfsc);
+  ioffset = vec_setreverse4sp(0.0, -1.0, -2.0, -3.0);
   sW = (i1-i0)/vecwidth;
 
   /*printf("***in vinside()****\n");
@@ -2926,7 +2926,7 @@
         vec_access = (float *) &(a[z]->vec[jp][ip/vecwidth]) + ip%vecwidth;
 	*vec_access = cm->endsc[z] + (cm->el_selfsc * ((jp+j1)-(ip+i0)+1));
         // FIXME: here and below, this is sloppy - should only set the one shadow cell
-	if (ret_shadow != NULL) shadow[z]->vec[jp][ip/vecwidth] = (__m128) _mm_set1_epi32(USED_EL);
+	if (ret_shadow != NULL) shadow[z]->vec[jp][ip/vecwidth] = (__m128) vec_splat4sw(USED_EL);
 	break;
       case MP_st:
 	if (i0 == i1 || j1 == j0) break;
@@ -2934,7 +2934,7 @@
         vec_access = (float *) &(a[z]->vec[jp+1][(ip-1)/vecwidth]) + (ip-1)%vecwidth;
 	*vec_access = cm->endsc[z] + (cm->el_selfsc * ((jp+j1)-(ip+i0)+1));
         *vec_access += cm->oesc[z][dsq[i1-1]*cm->abc->Kp + dsq[j1+1]];
-	if (ret_shadow != NULL) shadow[z]->vec[jp+1][(ip-1)/vecwidth] = (__m128) _mm_set1_epi32(USED_EL);
+	if (ret_shadow != NULL) shadow[z]->vec[jp+1][(ip-1)/vecwidth] = (__m128) vec_splat4sw(USED_EL);
 	//if (a[z][jp+1][ip-1] < IMPOSSIBLE) a[z][jp+1][ip-1] = IMPOSSIBLE;
 	break;
       case ML_st:
@@ -2944,7 +2944,7 @@
         vec_access = (float *) &(a[z]->vec[jp][(ip-1)/vecwidth]) + (ip-1)%vecwidth;
 	*vec_access = cm->endsc[z] + (cm->el_selfsc * ((jp+j1)-(ip+i0)+1));
         *vec_access += cm->oesc[z][dsq[i1-1]];
-	if (ret_shadow != NULL) shadow[z]->vec[jp][(ip-1)/vecwidth] = (__m128) _mm_set1_epi32(USED_EL);
+	if (ret_shadow != NULL) shadow[z]->vec[jp][(ip-1)/vecwidth] = (__m128) vec_splat4sw(USED_EL);
 	//if (a[z][jp][ip-1] < IMPOSSIBLE) a[z][jp][ip-1] = IMPOSSIBLE;
 	break;
       case MR_st:
@@ -2954,7 +2954,7 @@
         vec_access = (float *) &(a[z]->vec[jp+1][ip/vecwidth]) + ip%vecwidth;
 	*vec_access = cm->endsc[z] + (cm->el_selfsc * ((jp+j1)-(ip+i0)+1));
         *vec_access += cm->oesc[z][dsq[j1+1]];
-	if (ret_shadow != NULL) shadow[z]->vec[jp+1][ip/vecwidth] = (__m128) _mm_set1_epi32(USED_EL);
+	if (ret_shadow != NULL) shadow[z]->vec[jp+1][ip/vecwidth] = (__m128) vec_splat4sw(USED_EL);
 	//if (a[z][jp+1][ip] < IMPOSSIBLE) a[z][jp+1][ip] = IMPOSSIBLE;
 	break;
       }
@@ -2977,7 +2977,7 @@
       if (z == 0) { 
 	*vec_access = bsc;
 	if (ret_shadow != NULL)
-          shadow[0]->vec[0][0] = _mm_move_ss(shadow[0]->vec[0][0], (__m128) _mm_set1_epi32(USED_LOCAL_BEGIN));
+          shadow[0]->vec[0][0] = vec_insert1spintolower4sp(shadow[0]->vec[0][0], (__m128) vec_splat4sw(USED_LOCAL_BEGIN));
       }
     }
 
@@ -3002,9 +3002,9 @@
 	    for (ip = sW; ip >= 0; ip--) {
 	      /*printf("D S jp : %d | ip : %d\n", jp, ip);*/
 	      y = cm->cfirst[v];
-	      a[v]->vec[jp][ip]      = _mm_add_ps(a[y]->vec[jp][ip], _mm_set1_ps(cm->tsc[v][0]));
+	      a[v]->vec[jp][ip]      = vec_add4sp(a[y]->vec[jp][ip], vec_splat4sp(cm->tsc[v][0]));
 	      /*printf("set a[%d][%d][%d] to %f\n", v, jp, ip, sc);*/
-	      if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) _mm_set1_epi32(0);
+	      if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) vec_splat4sw(0);
 
 	      /* if (useEL && NOT_IMPOSSIBLE(cm->endsc[v]) && 
 		  ((cm->endsc[v] + (cm->el_selfsc * (((jp+j1)-(ip+i0)+1) - StateDelta(cm->sttype[v]))))
@@ -3014,14 +3014,14 @@
 		if (ret_shadow != NULL) shadow[v][jp][ip] = USED_EL;
 	      } */
 	      if (useEL && NOT_IMPOSSIBLE(cm->endsc[v])) {
-                loop_v = _mm_set1_ps((float) (jp+j1) - (ip*vecwidth+i0) + 1);
-                loop_v = _mm_add_ps(loop_v, ioffset);
-                loop_v = _mm_mul_ps(el_self_v, loop_v);
-                tmpv   = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), loop_v);
-                mask   = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                loop_v = vec_splat4sp((float) (jp+j1) - (ip*vecwidth+i0) + 1);
+                loop_v = vec_add4sp(loop_v, ioffset);
+                loop_v = vec_multiply4sp(el_self_v, loop_v);
+                tmpv   = vec_add4sp(vec_splat4sp(cm->endsc[v]), loop_v);
+                mask   = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                 if (ret_shadow != NULL) {
-                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(USED_EL), mask);
+                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(USED_EL), mask);
                 }
               }
 	      for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
@@ -3030,12 +3030,12 @@
 		    a[v][jp][ip] = sc;
 		    if (ret_shadow != NULL) shadow[v][jp][ip] = (char) yoffset; 
 		  } */
-                  tscv = _mm_set1_ps(cm->tsc[v][yoffset]);
-                  tmpv = _mm_add_ps(a[y+yoffset]->vec[jp][ip], tscv);
-                  mask = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                  a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                  tscv = vec_splat4sp(cm->tsc[v][yoffset]);
+                  tmpv = vec_add4sp(a[y+yoffset]->vec[jp][ip], tscv);
+                  mask = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                  a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                   if (ret_shadow != NULL) {
-                    shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(yoffset), mask);
+                    shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(yoffset), mask);
                   }
               }
               //FIXME: there's that underflow again...
@@ -3051,34 +3051,34 @@
             ip = sW;
             y = cm->cfirst[v];
 	    tmpv = esl_sse_leftshift_ps(a[y]->vec[jp-1][ip],neginfv); 
-            a[v]->vec[jp][ip] = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][0]));
-            if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) _mm_set1_epi32(0);
+            a[v]->vec[jp][ip] = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][0]));
+            if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) vec_splat4sw(0);
             if (useEL && NOT_IMPOSSIBLE(cm->endsc[v])) {
-              loop_v = _mm_set1_ps((float) (jp+j1) - (ip*vecwidth+i0) - 1); /* j-i+1, -2 for StateDelta */
-              loop_v = _mm_add_ps(loop_v, ioffset);
-              loop_v = _mm_mul_ps(el_self_v, loop_v);
-              tmpv   = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), loop_v);
-              mask   = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-              a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+              loop_v = vec_splat4sp((float) (jp+j1) - (ip*vecwidth+i0) - 1); /* j-i+1, -2 for StateDelta */
+              loop_v = vec_add4sp(loop_v, ioffset);
+              loop_v = vec_multiply4sp(el_self_v, loop_v);
+              tmpv   = vec_add4sp(vec_splat4sp(cm->endsc[v]), loop_v);
+              mask   = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+              a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
               if (ret_shadow != NULL) {
-                shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(USED_EL), mask);
+                shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(USED_EL), mask);
               }
             }
 	    for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
               tmpv = esl_sse_leftshift_ps(a[y+yoffset]->vec[jp-1][ip],neginfv);
-              tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][yoffset]));
-              mask = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-              a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+              tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][yoffset]));
+              mask = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+              a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
               if (ret_shadow != NULL) {
-                shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(yoffset), mask);
+                shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(yoffset), mask);
               }
             }
             i = ip*vecwidth + i0;
-            escv = _mm_setr_ps(i  <i1?cm->oesc[v][dsq[i  ]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
+            escv = vec_setreverse4sp(i  <i1?cm->oesc[v][dsq[i  ]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                i+1<i1?cm->oesc[v][dsq[i+1]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                i+2<i1?cm->oesc[v][dsq[i+2]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                i+3<i1?cm->oesc[v][dsq[i+3]*cm->abc->Kp+dsq[j]]:-eslINFINITY);
-            a[v]->vec[jp][ip] = _mm_add_ps(a[v]->vec[jp][ip], escv);
+            a[v]->vec[jp][ip] = vec_add4sp(a[v]->vec[jp][ip], escv);
 
             /* all other values for ip */
 	    for (ip = sW-1; ip >= 0; ip--) {
@@ -3086,37 +3086,37 @@
 	      i = ip*vecwidth + i0;
 	      y = cm->cfirst[v];
               tmpv = esl_sse_leftshift_ps(a[y]->vec[jp-1][ip], a[y]->vec[jp-1][ip+1]);
-              a[v]->vec[jp][ip] = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][0]));
+              a[v]->vec[jp][ip] = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][0]));
 	      /*printf("set a[%d][%d][%d] to %f\n", v, jp, ip, sc);*/
-	      if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) _mm_set1_epi32(0);
+	      if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) vec_splat4sw(0);
 	      if (useEL && NOT_IMPOSSIBLE(cm->endsc[v])) {
-                loop_v = _mm_set1_ps((float) (jp+j1) - (ip*vecwidth+i0) - 1); /* j-i+1, -2 for StateDelta */
-                loop_v = _mm_add_ps(loop_v, ioffset);
-                loop_v = _mm_mul_ps(el_self_v, loop_v);
-                tmpv   = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), loop_v);
-                mask   = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                loop_v = vec_splat4sp((float) (jp+j1) - (ip*vecwidth+i0) - 1); /* j-i+1, -2 for StateDelta */
+                loop_v = vec_add4sp(loop_v, ioffset);
+                loop_v = vec_multiply4sp(el_self_v, loop_v);
+                tmpv   = vec_add4sp(vec_splat4sp(cm->endsc[v]), loop_v);
+                mask   = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                 if (ret_shadow != NULL) {
-                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(USED_EL), mask);
+                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(USED_EL), mask);
                 }
 	      }
 	      for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
                 tmpv = esl_sse_leftshift_ps(a[y+yoffset]->vec[jp-1][ip],a[y+yoffset]->vec[jp-1][ip+1]);
-                tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][yoffset]));
-                mask = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][yoffset]));
+                mask = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                 if (ret_shadow != NULL) {
-                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(yoffset), mask);
+                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(yoffset), mask);
                 }
               }
               i = ip*vecwidth + i0;
               /* Could drop the <i1 checks here, since they should never be true, but we
                  should be switching to precalculated matrices anyway */
-              escv = _mm_setr_ps(i  <i1?cm->oesc[v][dsq[i  ]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
+              escv = vec_setreverse4sp(i  <i1?cm->oesc[v][dsq[i  ]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                  i+1<i1?cm->oesc[v][dsq[i+1]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                  i+2<i1?cm->oesc[v][dsq[i+2]*cm->abc->Kp+dsq[j]]:-eslINFINITY,
                                  i+3<i1?cm->oesc[v][dsq[i+3]*cm->abc->Kp+dsq[j]]:-eslINFINITY);
-              a[v]->vec[jp][ip] = _mm_add_ps(a[v]->vec[jp][ip], escv);
+              a[v]->vec[jp][ip] = vec_add4sp(a[v]->vec[jp][ip], escv);
 	      //if (a[v][jp][ip] < IMPOSSIBLE) a[v][jp][ip] = IMPOSSIBLE;  
 	    }
 	  }
@@ -3129,44 +3129,44 @@
             y = cm->cfirst[v];
             if (v == y) tmpv = neginfv;
 	    else        tmpv = esl_sse_leftshift_ps(a[y]->vec[jp][ip],neginfv); 
-            a[v]->vec[jp][ip] = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][0]));
-            if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) _mm_set1_epi32(0);
+            a[v]->vec[jp][ip] = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][0]));
+            if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) vec_splat4sw(0);
             if (useEL && NOT_IMPOSSIBLE(cm->endsc[v])) {
-              loop_v = _mm_set1_ps((float) (jp+j1) - (ip*vecwidth+i0)); /* j-i+1, -1 for StateDelta */
-              loop_v = _mm_add_ps(loop_v, ioffset);
-              loop_v = _mm_mul_ps(el_self_v, loop_v);
-              tmpv   = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), loop_v);
-              mask   = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-              a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+              loop_v = vec_splat4sp((float) (jp+j1) - (ip*vecwidth+i0)); /* j-i+1, -1 for StateDelta */
+              loop_v = vec_add4sp(loop_v, ioffset);
+              loop_v = vec_multiply4sp(el_self_v, loop_v);
+              tmpv   = vec_add4sp(vec_splat4sp(cm->endsc[v]), loop_v);
+              mask   = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+              a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
               if (ret_shadow != NULL) {
-                shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(USED_EL), mask);
+                shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(USED_EL), mask);
               }
             }
 	    for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
               tmpv = esl_sse_leftshift_ps(a[y+yoffset]->vec[jp][ip],neginfv);
-              tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][yoffset]));
-              mask = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-              a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+              tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][yoffset]));
+              mask = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+              a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
               if (ret_shadow != NULL) {
-                shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(yoffset), mask);
+                shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(yoffset), mask);
               }
             }
             i = ip*vecwidth + i0;
-            escv = _mm_setr_ps(i  <i1?cm->oesc[v][dsq[i  ]]:-eslINFINITY,
+            escv = vec_setreverse4sp(i  <i1?cm->oesc[v][dsq[i  ]]:-eslINFINITY,
                                i+1<i1?cm->oesc[v][dsq[i+1]]:-eslINFINITY,
                                i+2<i1?cm->oesc[v][dsq[i+2]]:-eslINFINITY,
                                i+3<i1?cm->oesc[v][dsq[i+3]]:-eslINFINITY);
-            a[v]->vec[jp][ip] = _mm_add_ps(a[v]->vec[jp][ip], escv);
+            a[v]->vec[jp][ip] = vec_add4sp(a[v]->vec[jp][ip], escv);
             /* finish the serial IL->IL path */
             if (v == y) {
               for (k = 1; k < vecwidth; k++) {
                 tmpv = esl_sse_leftshift_ps(a[y]->vec[jp][ip],neginfv);
-                tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][0]));
-                tmpv = _mm_add_ps(tmpv, escv);
-                mask = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][0]));
+                tmpv = vec_add4sp(tmpv, escv);
+                mask = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                 if (ret_shadow != NULL) {
-                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(0), mask);
+                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(0), mask);
                 }
               }
             }
@@ -3178,47 +3178,47 @@
 	      y = cm->cfirst[v];
               if (v == y) tmpv = esl_sse_leftshift_ps(neginfv,           a[y]->vec[jp][ip+1]);
               else        tmpv = esl_sse_leftshift_ps(a[y]->vec[jp][ip], a[y]->vec[jp][ip+1]);
-              a[v]->vec[jp][ip] = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][0]));
+              a[v]->vec[jp][ip] = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][0]));
 	      /*printf("set a[%d][%d][%d] to %f\n", v, jp, ip, sc);*/
-	      if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) _mm_set1_epi32(0);
+	      if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = (__m128) vec_splat4sw(0);
 	      if (useEL && NOT_IMPOSSIBLE(cm->endsc[v])) {
-                loop_v = _mm_set1_ps((float) (jp+j1) - (ip*vecwidth+i0)); /* j-i+1, -1 for StateDelta */
-                loop_v = _mm_add_ps(loop_v, ioffset);
-                loop_v = _mm_mul_ps(el_self_v, loop_v);
-                tmpv   = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), loop_v);
-                mask   = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                loop_v = vec_splat4sp((float) (jp+j1) - (ip*vecwidth+i0)); /* j-i+1, -1 for StateDelta */
+                loop_v = vec_add4sp(loop_v, ioffset);
+                loop_v = vec_multiply4sp(el_self_v, loop_v);
+                tmpv   = vec_add4sp(vec_splat4sp(cm->endsc[v]), loop_v);
+                mask   = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                 if (ret_shadow != NULL) {
-                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(USED_EL), mask);
+                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(USED_EL), mask);
                 }
 	      }
 	      for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
                 tmpv = esl_sse_leftshift_ps(a[y+yoffset]->vec[jp][ip],a[y+yoffset]->vec[jp][ip+1]);
-                tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][yoffset]));
-                mask = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][yoffset]));
+                mask = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                 if (ret_shadow != NULL) {
-                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(yoffset), mask);
+                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(yoffset), mask);
                 }
               }
               i = ip*vecwidth + i0;
               /* Could drop the <i1 checks here, since they should never be true, but we
                  should be switching to precalculated matrices anyway */
-              escv = _mm_setr_ps(i  <i1?cm->oesc[v][dsq[i  ]]:-eslINFINITY,
+              escv = vec_setreverse4sp(i  <i1?cm->oesc[v][dsq[i  ]]:-eslINFINITY,
                                  i+1<i1?cm->oesc[v][dsq[i+1]]:-eslINFINITY,
                                  i+2<i1?cm->oesc[v][dsq[i+2]]:-eslINFINITY,
                                  i+3<i1?cm->oesc[v][dsq[i+3]]:-eslINFINITY);
-              a[v]->vec[jp][ip] = _mm_add_ps(a[v]->vec[jp][ip], escv);
+              a[v]->vec[jp][ip] = vec_add4sp(a[v]->vec[jp][ip], escv);
               /* finish the serial IL->IL path */
               if (v == y) {
                 for (k = 1; k < vecwidth; k++) {
                   tmpv = esl_sse_leftshift_ps(a[y]->vec[jp][ip], a[y]->vec[jp][ip+1]);
-                  tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][0]));
-                  tmpv = _mm_add_ps(tmpv, escv);
-                  mask = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                  a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                  tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][0]));
+                  tmpv = vec_add4sp(tmpv, escv);
+                  mask = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                  a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                   if (ret_shadow != NULL) {
-                    shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(0), mask);
+                    shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(0), mask);
                   }
                 }
               }
@@ -3233,32 +3233,32 @@
 	    for (ip = sW; ip >= 0; ip--) {
 	      /*printf("MR IR jp : %d | ip : %d\n", jp, ip);*/
 	      y = cm->cfirst[v];
-	      a[v]->vec[jp][ip] = _mm_add_ps(a[y]->vec[jp-1][ip], _mm_set1_ps(cm->tsc[v][0]));
+	      a[v]->vec[jp][ip] = vec_add4sp(a[y]->vec[jp-1][ip], vec_splat4sp(cm->tsc[v][0]));
 	      /*printf("set a[%d][%d][%d] to %f\n", v, jp, ip, sc);*/
-	      if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = _mm_setzero_ps();
+	      if (ret_shadow != NULL) shadow[v]->vec[jp][ip] = vec_zero4sp();
 	      if (useEL && NOT_IMPOSSIBLE(cm->endsc[v])) { 
-                loop_v = _mm_set1_ps((float) (jp+j1) - (ip*vecwidth+i0)); /* j-i+1, -1 for StateDelta */
-                loop_v = _mm_add_ps(loop_v, ioffset);
-                loop_v = _mm_mul_ps(el_self_v, loop_v);
-                tmpv   = _mm_add_ps(_mm_set1_ps(cm->endsc[v]), loop_v);
-                mask   = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                loop_v = vec_splat4sp((float) (jp+j1) - (ip*vecwidth+i0)); /* j-i+1, -1 for StateDelta */
+                loop_v = vec_add4sp(loop_v, ioffset);
+                loop_v = vec_multiply4sp(el_self_v, loop_v);
+                tmpv   = vec_add4sp(vec_splat4sp(cm->endsc[v]), loop_v);
+                mask   = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                 if (ret_shadow != NULL) {
-                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(USED_EL), mask);
+                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(USED_EL), mask);
                 }
 	      }
 	      for (yoffset = 1; yoffset < cm->cnum[v]; yoffset++) {
                 tmpv = a[y+yoffset]->vec[jp-1][ip];
-                tmpv = _mm_add_ps(tmpv, _mm_set1_ps(cm->tsc[v][yoffset]));
-                mask = _mm_cmpgt_ps(tmpv, a[v]->vec[jp][ip]);
-                a[v]->vec[jp][ip] = _mm_max_ps(tmpv, a[v]->vec[jp][ip]);
+                tmpv = vec_add4sp(tmpv, vec_splat4sp(cm->tsc[v][yoffset]));
+                mask = vec_comparegt4sp(tmpv, a[v]->vec[jp][ip]);
+                a[v]->vec[jp][ip] = vec_max4sp(tmpv, a[v]->vec[jp][ip]);
                 if (ret_shadow != NULL) {
-                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) _mm_set1_epi32(yoffset), mask);
+                  shadow[v]->vec[jp][ip] = esl_sse_select_ps(shadow[v]->vec[jp][ip], (__m128) vec_splat4sw(yoffset), mask);
                 }
               }
 	      
-              escv = _mm_set1_ps(cm->oesc[v][dsq[j]]);
-              a[v]->vec[jp][ip] = _mm_add_ps(a[v]->vec[jp][ip], escv);
+              escv = vec_splat4sp(cm->oesc[v][dsq[j]]);
+              a[v]->vec[jp][ip] = vec_add4sp(a[v]->vec[jp][ip], escv);
 	      //if (a[v][jp][ip] < IMPOSSIBLE) a[v][jp][ip] = IMPOSSIBLE;  
 	    }
 	  }
@@ -3280,7 +3280,7 @@
 	{
 	  *vec_access = bsc;
 	  if (ret_shadow != NULL)
-            shadow[v]->vec[j0-j1][0] = _mm_move_ss(shadow[v]->vec[j0-j1][0], (__m128) _mm_set1_epi32(USED_LOCAL_BEGIN));
+            shadow[v]->vec[j0-j1][0] = vec_insert1spintolower4sp(shadow[v]->vec[j0-j1][0], (__m128) vec_splat4sw(USED_LOCAL_BEGIN));
 	}
 
 
@@ -3402,9 +3402,9 @@
   float   *vec_access;
   const int vecwidth = 4;
 
-  neginfv = _mm_set1_ps(-eslINFINITY);
-  el_self_v = _mm_set1_ps(cm->el_selfsc);
-  ioffset = _mm_setr_ps(0.0, -1.0, -2.0, -3.0);
+  neginfv = vec_splat4sp(-eslINFINITY);
+  el_self_v = vec_splat4sp(cm->el_selfsc);
+  ioffset = vec_setreverse4sp(0.0, -1.0, -2.0, -3.0);
 
   /* Allocations and initializations
    */
@@ -3427,7 +3427,7 @@
     for (ip = 0; ip <= sW; ip++)
       beta[r]->vec[jp][ip] = neginfv;
   }
-  beta[r]->vec[j0-j1][0] = _mm_move_ss(beta[r]->vec[j0-j1][0], _mm_set1_ps(0.0));		
+  beta[r]->vec[j0-j1][0] = vec_insert1spintolower4sp(beta[r]->vec[j0-j1][0], vec_splat4sp(0.0));		
 
   /* Initialize the EL deck, if we're in local mode w.r.t. ends.
    * Deal with the special initialization case of the root state r
@@ -3529,35 +3529,35 @@
 	for (y = cm->plast[v]-cm->pnum[v]+1; y <= cm->plast[v]; y++) {
 	  if (y < r) continue; /* deal with split sets */
 	  voffset = v - cm->cfirst[y]; /* gotta calculate the transition score index for t_y(v) */
-          tscv = _mm_set1_ps(cm->tsc[y][voffset]);
+          tscv = vec_splat4sp(cm->tsc[y][voffset]);
 
 	  switch(cm->sttype[y]) {
 	  case MP_st:  /* i == i0 boundary condition true */
 	    if (j == j0) continue; /* boundary condition */
-            escv = _mm_setr_ps(                                                  -eslINFINITY,
+            escv = vec_setreverse4sp(                                                  -eslINFINITY,
                                i  <i1?cm->oesc[y][dsq[i  ]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                i+1<i1?cm->oesc[y][dsq[i+1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                i+2<i1?cm->oesc[y][dsq[i+2]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY);
 	
             tmpv = alt_rightshift_ps(beta[y]->vec[jp+1][ip], neginfv);
-            tmpv = _mm_add_ps(tmpv, tscv);
-            tmpv = _mm_add_ps(tmpv, escv);
-            beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+            tmpv = vec_add4sp(tmpv, tscv);
+            tmpv = vec_add4sp(tmpv, escv);
+            beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 	    break;
 
 	  case ML_st:
-            escv = _mm_setr_ps(                             -eslINFINITY,
+            escv = vec_setreverse4sp(                             -eslINFINITY,
                                i  <i1?cm->oesc[y][dsq[i  ]]:-eslINFINITY,
                                i+1<i1?cm->oesc[y][dsq[i+1]]:-eslINFINITY,
                                i+2<i1?cm->oesc[y][dsq[i+2]]:-eslINFINITY);
 		  
             tmpv = alt_rightshift_ps(beta[y]->vec[jp][ip], neginfv);
-            tmpv = _mm_add_ps(tmpv, tscv);
-            tmpv = _mm_add_ps(tmpv, escv);
-            beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+            tmpv = vec_add4sp(tmpv, tscv);
+            tmpv = vec_add4sp(tmpv, escv);
+            beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 	    break;
 	  case IL_st: 
-            escv = _mm_setr_ps(                             -eslINFINITY,
+            escv = vec_setreverse4sp(                             -eslINFINITY,
                                i  <i1?cm->oesc[y][dsq[i  ]]:-eslINFINITY,
                                i+1<i1?cm->oesc[y][dsq[i+1]]:-eslINFINITY,
                                i+2<i1?cm->oesc[y][dsq[i+2]]:-eslINFINITY);
@@ -3565,33 +3565,33 @@
             if (y == v) {
               for (k = 1; k < vecwidth; k++) {
                 tmpv = alt_rightshift_ps(beta[y]->vec[jp][ip], neginfv);
-                tmpv = _mm_add_ps(tmpv, tscv);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                tmpv = vec_add4sp(tmpv, tscv);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
               }
             }
             else {
               tmpv = alt_rightshift_ps(beta[y]->vec[jp][ip], neginfv);
-              tmpv = _mm_add_ps(tmpv, tscv);
-              tmpv = _mm_add_ps(tmpv, escv);
-              beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+              tmpv = vec_add4sp(tmpv, tscv);
+              tmpv = vec_add4sp(tmpv, escv);
+              beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
             }
 	    break;
 		  
 	  case MR_st:
 	  case IR_st:
 	    if (j == j0) continue;
-	    escv = _mm_set1_ps(cm->oesc[y][dsq[j+1]]);
-            tmpv = _mm_add_ps(beta[y]->vec[jp+1][ip], tscv);
-            tmpv = _mm_add_ps(tmpv, escv);
-            beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+	    escv = vec_splat4sp(cm->oesc[y][dsq[j+1]]);
+            tmpv = vec_add4sp(beta[y]->vec[jp+1][ip], tscv);
+            tmpv = vec_add4sp(tmpv, escv);
+            beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 	    break;
 		  
 	  case S_st:
 	  case E_st:
 	  case D_st:
-            tmpv = _mm_add_ps(beta[y]->vec[jp][ip], tscv);
-            beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+            tmpv = vec_add4sp(beta[y]->vec[jp][ip], tscv);
+            beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 	    break;
 
 	  default: cm_Fail("bogus parent state %d\n", cm->sttype[y]);
@@ -3608,39 +3608,39 @@
 	    for (y = cm->plast[v]-cm->pnum[v]+1; y <= cm->plast[v]; y++) {
 	      if (y < r) continue; /* deal with split sets */
 	      voffset = v - cm->cfirst[y]; /* gotta calculate the transition score index for t_y(v) */
-              tscv = _mm_set1_ps(cm->tsc[y][voffset]);
+              tscv = vec_splat4sp(cm->tsc[y][voffset]);
 
 	      switch(cm->sttype[y]) {
 	      case MP_st: 
 		if (j == j0) continue; /* boundary condition */
 	        //escore = cm->oesc[y][dsq[i-1]*cm->abc->Kp+dsq[j+1]];
-                escv = _mm_setr_ps(i-1<i1?cm->oesc[y][dsq[i-1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
+                escv = vec_setreverse4sp(i-1<i1?cm->oesc[y][dsq[i-1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i  <i1?cm->oesc[y][dsq[i  ]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i+1<i1?cm->oesc[y][dsq[i+1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i+2<i1?cm->oesc[y][dsq[i+2]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY);
 		
                 tmpv = alt_rightshift_ps(beta[y]->vec[jp+1][ip], beta[y]->vec[jp+1][ip-1]);
-                tmpv = _mm_add_ps(tmpv, tscv);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                tmpv = vec_add4sp(tmpv, tscv);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 		break;
 
 	      case ML_st:
 		//escore = cm->oesc[y][dsq[i-1]];
-                escv = _mm_setr_ps(i-1<i1?cm->oesc[y][dsq[i-1]]:-eslINFINITY,
+                escv = vec_setreverse4sp(i-1<i1?cm->oesc[y][dsq[i-1]]:-eslINFINITY,
                                    i  <i1?cm->oesc[y][dsq[i  ]]:-eslINFINITY,
                                    i+1<i1?cm->oesc[y][dsq[i+1]]:-eslINFINITY,
                                    i+2<i1?cm->oesc[y][dsq[i+2]]:-eslINFINITY);
 		  
                 tmpv = alt_rightshift_ps(beta[y]->vec[jp][ip], beta[y]->vec[jp][ip-1]);
-                tmpv = _mm_add_ps(tmpv, tscv);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                tmpv = vec_add4sp(tmpv, tscv);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 		break;
 		  
 	      case IL_st: 
 		//escore = cm->oesc[y][dsq[i-1]];
-                escv = _mm_setr_ps(i-1<i1?cm->oesc[y][dsq[i-1]]:-eslINFINITY,
+                escv = vec_setreverse4sp(i-1<i1?cm->oesc[y][dsq[i-1]]:-eslINFINITY,
                                    i  <i1?cm->oesc[y][dsq[i  ]]:-eslINFINITY,
                                    i+1<i1?cm->oesc[y][dsq[i+1]]:-eslINFINITY,
                                    i+2<i1?cm->oesc[y][dsq[i+2]]:-eslINFINITY);
@@ -3648,16 +3648,16 @@
                 if (y == v) {
                   for (k = 0; k < vecwidth; k++) {
                     tmpv = alt_rightshift_ps(beta[y]->vec[jp][ip], beta[y]->vec[jp][ip-1]);
-                    tmpv = _mm_add_ps(tmpv, tscv);
-                    tmpv = _mm_add_ps(tmpv, escv);
-                    beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                    tmpv = vec_add4sp(tmpv, tscv);
+                    tmpv = vec_add4sp(tmpv, escv);
+                    beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
                   }
                 }
                 else {
                   tmpv = alt_rightshift_ps(beta[y]->vec[jp][ip], beta[y]->vec[jp][ip-1]);
-                  tmpv = _mm_add_ps(tmpv, tscv);
-                  tmpv = _mm_add_ps(tmpv, escv);
-                  beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                  tmpv = vec_add4sp(tmpv, tscv);
+                  tmpv = vec_add4sp(tmpv, escv);
+                  beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
                 }
 		break;
 		  
@@ -3665,18 +3665,18 @@
 	      case IR_st:
 		if (j == j0) continue;
 		//escore = cm->oesc[y][dsq[j+1]];
-                escv = _mm_set1_ps(cm->oesc[y][dsq[j+1]]);
+                escv = vec_splat4sp(cm->oesc[y][dsq[j+1]]);
 
-                tmpv = _mm_add_ps(beta[y]->vec[jp+1][ip], tscv);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                tmpv = vec_add4sp(beta[y]->vec[jp+1][ip], tscv);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 		break;
 		  
 	      case S_st:
 	      case E_st:
 	      case D_st:
-                tmpv = _mm_add_ps(beta[y]->vec[jp][ip], tscv);
-                beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                tmpv = vec_add4sp(beta[y]->vec[jp][ip], tscv);
+                beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 		break;
 
 	      default: cm_Fail("bogus parent state %d\n", cm->sttype[y]);
@@ -3699,52 +3699,52 @@
           /* handle ip = 0 separately */
           ip = 0;
           i = i0;
-          loop_v = _mm_set1_ps((float) (j - i + 1));
-          loop_v = _mm_add_ps(loop_v, ioffset);
-          loop_v = _mm_mul_ps(el_self_v, loop_v);
-          loop_v = _mm_add_ps(loop_v, _mm_set1_ps(cm->endsc[v]));
+          loop_v = vec_splat4sp((float) (j - i + 1));
+          loop_v = vec_add4sp(loop_v, ioffset);
+          loop_v = vec_multiply4sp(el_self_v, loop_v);
+          loop_v = vec_add4sp(loop_v, vec_splat4sp(cm->endsc[v]));
 
 	  switch(cm->sttype[v]) {
 	  case MP_st:  /* i == i0 boundary condition true */
 	    if (j == j0) continue; 
-            escv = _mm_setr_ps(                                                  -eslINFINITY,
+            escv = vec_setreverse4sp(                                                  -eslINFINITY,
                                i  <i1?cm->oesc[v][dsq[i  ]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                i+1<i1?cm->oesc[v][dsq[i+1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                i+2<i1?cm->oesc[v][dsq[i+2]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY);
 	
             tmpv = alt_rightshift_ps(beta[v]->vec[jp+1][ip], neginfv);
-            tmpv = _mm_add_ps(tmpv, loop_v);
-            tmpv = _mm_add_ps(tmpv, escv);
-            beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+            tmpv = vec_add4sp(tmpv, loop_v);
+            tmpv = vec_add4sp(tmpv, escv);
+            beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 	    break;
 
 	  case ML_st:
 	  case IL_st: 
-            escv = _mm_setr_ps(                             -eslINFINITY,
+            escv = vec_setreverse4sp(                             -eslINFINITY,
                                i  <i1?cm->oesc[v][dsq[i  ]]:-eslINFINITY,
                                i+1<i1?cm->oesc[v][dsq[i+1]]:-eslINFINITY,
                                i+2<i1?cm->oesc[v][dsq[i+2]]:-eslINFINITY);
 		
             tmpv = alt_rightshift_ps(beta[v]->vec[jp][ip], neginfv);
-            tmpv = _mm_add_ps(tmpv, loop_v);
-            tmpv = _mm_add_ps(tmpv, escv);
-            beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+            tmpv = vec_add4sp(tmpv, loop_v);
+            tmpv = vec_add4sp(tmpv, escv);
+            beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 	    break;
 		  
 	  case MR_st:
 	  case IR_st:
 	    if (j == j0) continue;
-	    escv = _mm_set1_ps(cm->oesc[v][dsq[j+1]]);
-            tmpv = _mm_add_ps(beta[v]->vec[jp+1][ip], loop_v);
-            tmpv = _mm_add_ps(tmpv, escv);
-            beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+	    escv = vec_splat4sp(cm->oesc[v][dsq[j+1]]);
+            tmpv = vec_add4sp(beta[v]->vec[jp+1][ip], loop_v);
+            tmpv = vec_add4sp(tmpv, escv);
+            beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 	    break;
 		  
 	  case S_st:
 	  case E_st:
 	  case D_st:
-            tmpv = _mm_add_ps(beta[v]->vec[jp][ip], loop_v);
-            beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+            tmpv = vec_add4sp(beta[v]->vec[jp][ip], loop_v);
+            beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 	    break;
 
 	  default: cm_Fail("bogus parent state %d\n", cm->sttype[v]);
@@ -3753,22 +3753,22 @@
 	  for (ip = 1; ip <= sW; ip++) 
 	    {
 	      i = ip*vecwidth+i0;
-              loop_v = _mm_set1_ps((float) (j - i + 1));
-              loop_v = _mm_add_ps(loop_v, ioffset);
-              loop_v = _mm_mul_ps(el_self_v, loop_v);
+              loop_v = vec_splat4sp((float) (j - i + 1));
+              loop_v = vec_add4sp(loop_v, ioffset);
+              loop_v = vec_multiply4sp(el_self_v, loop_v);
 	      switch (cm->sttype[v]) {
 	      case MP_st:
 		if (j == j0) continue; 
 		//escore = cm->oesc[v][dsq[i-1]*cm->abc->Kp+dsq[j+1]];
-                escv = _mm_setr_ps(i-1<i1?cm->oesc[v][dsq[i-1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
+                escv = vec_setreverse4sp(i-1<i1?cm->oesc[v][dsq[i-1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i  <i1?cm->oesc[v][dsq[i  ]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i+1<i1?cm->oesc[v][dsq[i+1]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY,
                                    i+2<i1?cm->oesc[v][dsq[i+2]*cm->abc->Kp+dsq[j+1]]:-eslINFINITY);
 		
                 tmpv = alt_rightshift_ps(beta[v]->vec[jp+1][ip], beta[v]->vec[jp+1][ip-1]);
-                tmpv = _mm_add_ps(tmpv, loop_v);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                tmpv = vec_add4sp(tmpv, loop_v);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 		/* if ((sc = beta[v][jp+1][ip-1] + cm->endsc[v] + 
 		     (cm->el_selfsc * (j-i+1)) + escore) > beta[cm->M][jp][ip])
 		  beta[cm->M][jp][ip] = sc; */
@@ -3776,15 +3776,15 @@
 	      case ML_st:
 	      case IL_st:
 		//escore = cm->oesc[v][dsq[i-1]];
-                escv = _mm_setr_ps(i-1<i1?cm->oesc[v][dsq[i-1]]:-eslINFINITY,
+                escv = vec_setreverse4sp(i-1<i1?cm->oesc[v][dsq[i-1]]:-eslINFINITY,
                                    i  <i1?cm->oesc[v][dsq[i  ]]:-eslINFINITY,
                                    i+1<i1?cm->oesc[v][dsq[i+1]]:-eslINFINITY,
                                    i+2<i1?cm->oesc[v][dsq[i+2]]:-eslINFINITY);
 		
                 tmpv = alt_rightshift_ps(beta[v]->vec[jp][ip], beta[v]->vec[jp][ip-1]);
-                tmpv = _mm_add_ps(tmpv, loop_v);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                tmpv = vec_add4sp(tmpv, loop_v);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 		/* if ((sc = beta[v][jp][ip-1] + cm->endsc[v] + 
 		     (cm->el_selfsc * (j-i+1)) + escore) > beta[cm->M][jp][ip])
 		  beta[cm->M][jp][ip] = sc; */
@@ -3793,12 +3793,12 @@
 	      case IR_st:
 		if (j == j0) continue;
 		escore = cm->oesc[v][dsq[j+1]];
-                escv = _mm_set1_ps(cm->oesc[v][dsq[j+1]]);
+                escv = vec_splat4sp(cm->oesc[v][dsq[j+1]]);
 
                 tmpv = beta[v]->vec[jp+1][ip];
-                tmpv = _mm_add_ps(tmpv, loop_v);
-                tmpv = _mm_add_ps(tmpv, escv);
-                beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                tmpv = vec_add4sp(tmpv, loop_v);
+                tmpv = vec_add4sp(tmpv, escv);
+                beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 		/*if ((sc = beta[v][jp+1][ip] + cm->endsc[v] + 
 		     (cm->el_selfsc * (j-i+1)) + escore) > beta[cm->M][jp][ip])
 		  beta[cm->M][jp][ip] = sc; */
@@ -3807,8 +3807,8 @@
 	      case D_st:
 	      case E_st:
                 tmpv = beta[v]->vec[jp][ip];
-                tmpv = _mm_add_ps(tmpv, loop_v);
-                beta[v]->vec[jp][ip] = _mm_max_ps(beta[v]->vec[jp][ip], tmpv);
+                tmpv = vec_add4sp(tmpv, loop_v);
+                beta[v]->vec[jp][ip] = vec_max4sp(beta[v]->vec[jp][ip], tmpv);
 		/* if ((sc = beta[v][jp][ip] + cm->endsc[v] + 
 		     (cm->el_selfsc * (j-i+1))) > beta[cm->M][jp][ip])
 		    beta[cm->M][jp][ip] = sc; */
@@ -3968,10 +3968,10 @@
 
   /* Allocations and initializations
    */
-  zerov = _mm_set1_epi16(0);
-  neginfv = _mm_set1_epi16(-32768);
-  el_self_v = _mm_set1_epi16(ocm->el_selfsc);
-  doffset = _mm_setr_epi16(0, 1, 2, 3, 4, 5, 6, 7);
+  zerov = vec_splat8sh(0);
+  neginfv = vec_splat8sh(-32768);
+  el_self_v = vec_splat8sh(ocm->el_selfsc);
+  doffset = vec_setreverse8sh(0, 1, 2, 3, 4, 5, 6, 7);
   b   = -1;
   bsc = -32768;
   W   = j0-i0+1;		/* the length of the subsequence -- used in many loops  */
@@ -3992,7 +3992,7 @@
 
 #ifndef LOBAL_MODE
   vb_sc = neginfv;
-  vb_v = vb_j  = vb_d  = _mm_set1_epi16(-1);
+  vb_v = vb_j  = vb_d  = vec_splat8sh(-1);
 
   ESL_ALLOC(mem_unaligned_sc, sizeof(__m128i) * (sW+1) + 15);
   vec_unaligned_sc = (__m128i *) (((unsigned long int) mem_unaligned_sc + 15) & (~0xf));
@@ -4007,11 +4007,11 @@
   tmp = wordify(ocm->scale_w, constpart);
   logp = wordify(ocm->scale_w, sreLOG2(p));
   for (dp = 0; dp <= sW; dp++) {
-    tmpv = _mm_mullo_epi16(_mm_set1_epi16(logp), _mm_adds_epi16(doffset, _mm_set1_epi16(dp*vecwidth)));
-    mask = _mm_cmpgt_epi16(zerov,tmpv);
-    tmpv = _mm_and_si128(tmpv,mask);
-    tmpv = _mm_or_si128(tmpv,_mm_andnot_si128(mask,_mm_set1_epi16(0x8000)));
-    vec_unaligned_sc[dp] = _mm_subs_epi16(_mm_set1_epi16(tmp), tmpv);
+    tmpv = vec_multiply8sh(vec_splat8sh(logp), vec_addsaturating8sh(doffset, vec_splat8sh(dp*vecwidth)));
+    mask = vec_comparegt8sh(zerov,tmpv);
+    tmpv = vec_bitand1q(tmpv,mask);
+    tmpv = vec_bitor1q(tmpv,vec_bitandnotleft1q(mask,vec_splat8sh(0x8000)));
+    vec_unaligned_sc[dp] = vec_subtractsaturating8sh(vec_splat8sh(tmp), tmpv);
   }
 #endif
 
@@ -4066,20 +4066,20 @@
 	    for (dp = 0; dp <= sW; dp++)
 	      {
 		// alpha[v][j][d] = cm->endsc[v] + (cm->el_selfsc * (d-StateDelta(cm->sttype[v])));
-                tmpv = _mm_mullo_epi16(el_self_v, _mm_adds_epi16(_mm_set1_epi16(dp*vecwidth), doffset));
-                mask = _mm_cmpgt_epi16(zerov,tmpv);
-                tmpv = _mm_and_si128(tmpv,mask);
-                tmpv = _mm_or_si128(tmpv,_mm_andnot_si128(mask,_mm_set1_epi16(0x8000)));
-		alpha[v]->ivec[j][dp] = _mm_adds_epi16(_mm_set1_epi16(ocm->endsc[v]), tmpv);
+                tmpv = vec_multiply8sh(el_self_v, vec_addsaturating8sh(vec_splat8sh(dp*vecwidth), doffset));
+                mask = vec_comparegt8sh(zerov,tmpv);
+                tmpv = vec_bitand1q(tmpv,mask);
+                tmpv = vec_bitor1q(tmpv,vec_bitandnotleft1q(mask,vec_splat8sh(0x8000)));
+		alpha[v]->ivec[j][dp] = vec_addsaturating8sh(vec_splat8sh(ocm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
 	      }
 
             for (yoffset = 0; yoffset < ocm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_epi16(ocm->tsc[v][yoffset]);
+              tscv = vec_splat8sh(ocm->tsc[v][yoffset]);
 	      for (dp = 0; dp <= sW; dp++)
 	        {
-                  tmpv = _mm_adds_epi16(alpha[y+yoffset]->ivec[j][dp], tscv);
-                  alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                  tmpv = vec_addsaturating8sh(alpha[y+yoffset]->ivec[j][dp], tscv);
+                  alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                 }  
             }
             //FIXME: SSE conversion is kind of ignoring the possibilty of underflow... this is bad.
@@ -4100,130 +4100,130 @@
              * the section below is distinctly not, as I have enumerated the cases
              * of how d-k hits the vector boundaries
              */
-            begr_v = _mm_set1_epi16(_mm_extract_epi16(alpha[z]->ivec[j][0],0));
+            begr_v = vec_splat8sh(vec_extract8sh(alpha[z]->ivec[j][0],0));
 	    for (dp = 0; dp <= sW; dp++)
 	      {
-		alpha[v]->ivec[j][dp] = _mm_adds_epi16(alpha[y]->ivec[j][dp], begr_v);
+		alpha[v]->ivec[j][dp] = vec_addsaturating8sh(alpha[y]->ivec[j][dp], begr_v);
               }
             for (k = 8; k <= jp; k += vecwidth)
               {
                 kp = k/vecwidth;
-                begr_v = _mm_set1_epi16(_mm_extract_epi16(alpha[z]->ivec[j][kp],0));
+                begr_v = vec_splat8sh(vec_extract8sh(alpha[z]->ivec[j][kp],0));
 
                 tmpv = alpha[y]->ivec[j-k][0];
-                tmpv = _mm_adds_epi16(tmpv, begr_v);
-                alpha[v]->ivec[j][kp] = _mm_max_epi16(alpha[v]->ivec[j][kp], tmpv);
+                tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                alpha[v]->ivec[j][kp] = vec_max8sh(alpha[v]->ivec[j][kp], tmpv);
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = alpha[y]->ivec[j-k][dp-kp];
-                    tmpv = _mm_adds_epi16(tmpv, begr_v);
-                    alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                    tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                    alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                   }
               }
             /* Expanding all the cases just because the shift argument needs to be an immediate */
             for (k = 1; k <= jp; k += vecwidth)
               {
                 kp = k/vecwidth;
-                begr_v = _mm_set1_epi16(_mm_extract_epi16(alpha[z]->ivec[j][kp],1));
+                begr_v = vec_splat8sh(vec_extract8sh(alpha[z]->ivec[j][kp],1));
 
                 tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][0],neginfv,1);
-                tmpv = _mm_adds_epi16(tmpv, begr_v);
-                alpha[v]->ivec[j][kp] = _mm_max_epi16(alpha[v]->ivec[j][kp], tmpv);
+                tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                alpha[v]->ivec[j][kp] = vec_max8sh(alpha[v]->ivec[j][kp], tmpv);
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][dp-kp], alpha[y]->ivec[j-k][dp-kp-1],1);
-                    tmpv = _mm_adds_epi16(tmpv, begr_v);
-                    alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                    tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                    alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                   }
               }
             for (k = 2; k <= jp; k += vecwidth)
               {
                 kp = k/vecwidth;
-                begr_v = _mm_set1_epi16(_mm_extract_epi16(alpha[z]->ivec[j][kp],2));
+                begr_v = vec_splat8sh(vec_extract8sh(alpha[z]->ivec[j][kp],2));
 
                 tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][0],neginfv,2);
-                tmpv = _mm_adds_epi16(tmpv, begr_v);
-                alpha[v]->ivec[j][kp] = _mm_max_epi16(alpha[v]->ivec[j][kp], tmpv);
+                tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                alpha[v]->ivec[j][kp] = vec_max8sh(alpha[v]->ivec[j][kp], tmpv);
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][dp-kp], alpha[y]->ivec[j-k][dp-kp-1],2);
-                    tmpv = _mm_adds_epi16(tmpv, begr_v);
-                    alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                    tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                    alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                   }
               }
             for (k = 3; k <= jp; k += vecwidth)
               {
                 kp = k/vecwidth;
-                begr_v = _mm_set1_epi16(_mm_extract_epi16(alpha[z]->ivec[j][kp],3));
+                begr_v = vec_splat8sh(vec_extract8sh(alpha[z]->ivec[j][kp],3));
 
                 tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][0],neginfv,3);
-                tmpv = _mm_adds_epi16(tmpv, begr_v);
-                alpha[v]->ivec[j][kp] = _mm_max_epi16(alpha[v]->ivec[j][kp], tmpv);
+                tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                alpha[v]->ivec[j][kp] = vec_max8sh(alpha[v]->ivec[j][kp], tmpv);
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][dp-kp], alpha[y]->ivec[j-k][dp-kp-1],3);
-                    tmpv = _mm_adds_epi16(tmpv, begr_v);
-                    alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                    tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                    alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                   }
               }
             for (k = 4; k <= jp; k += vecwidth)
               {
                 kp = k/vecwidth;
-                begr_v = _mm_set1_epi16(_mm_extract_epi16(alpha[z]->ivec[j][kp],4));
+                begr_v = vec_splat8sh(vec_extract8sh(alpha[z]->ivec[j][kp],4));
 
                 tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][0],neginfv,4);
-                tmpv = _mm_adds_epi16(tmpv, begr_v);
-                alpha[v]->ivec[j][kp] = _mm_max_epi16(alpha[v]->ivec[j][kp], tmpv);
+                tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                alpha[v]->ivec[j][kp] = vec_max8sh(alpha[v]->ivec[j][kp], tmpv);
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][dp-kp], alpha[y]->ivec[j-k][dp-kp-1],4);
-                    tmpv = _mm_adds_epi16(tmpv, begr_v);
-                    alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                    tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                    alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                   }
               }
             for (k = 5; k <= jp; k += vecwidth)
               {
                 kp = k/vecwidth;
-                begr_v = _mm_set1_epi16(_mm_extract_epi16(alpha[z]->ivec[j][kp],5));
+                begr_v = vec_splat8sh(vec_extract8sh(alpha[z]->ivec[j][kp],5));
 
                 tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][0],neginfv,5);
-                tmpv = _mm_adds_epi16(tmpv, begr_v);
-                alpha[v]->ivec[j][kp] = _mm_max_epi16(alpha[v]->ivec[j][kp], tmpv);
+                tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                alpha[v]->ivec[j][kp] = vec_max8sh(alpha[v]->ivec[j][kp], tmpv);
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][dp-kp], alpha[y]->ivec[j-k][dp-kp-1],5);
-                    tmpv = _mm_adds_epi16(tmpv, begr_v);
-                    alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                    tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                    alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                   }
               }
             for (k = 6; k <= jp; k += vecwidth)
               {
                 kp = k/vecwidth;
-                begr_v = _mm_set1_epi16(_mm_extract_epi16(alpha[z]->ivec[j][kp],6));
+                begr_v = vec_splat8sh(vec_extract8sh(alpha[z]->ivec[j][kp],6));
 
                 tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][0],neginfv,6);
-                tmpv = _mm_adds_epi16(tmpv, begr_v);
-                alpha[v]->ivec[j][kp] = _mm_max_epi16(alpha[v]->ivec[j][kp], tmpv);
+                tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                alpha[v]->ivec[j][kp] = vec_max8sh(alpha[v]->ivec[j][kp], tmpv);
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][dp-kp], alpha[y]->ivec[j-k][dp-kp-1],6);
-                    tmpv = _mm_adds_epi16(tmpv, begr_v);
-                    alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                    tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                    alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                   }
               }
             for (k = 7; k <= jp; k += vecwidth)
               {
                 kp = k/vecwidth;
-                begr_v = _mm_set1_epi16(_mm_extract_epi16(alpha[z]->ivec[j][kp],7));
+                begr_v = vec_splat8sh(vec_extract8sh(alpha[z]->ivec[j][kp],7));
 
                 tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][0],neginfv,7);
-                tmpv = _mm_adds_epi16(tmpv, begr_v);
-                alpha[v]->ivec[j][kp] = _mm_max_epi16(alpha[v]->ivec[j][kp], tmpv);
+                tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                alpha[v]->ivec[j][kp] = vec_max8sh(alpha[v]->ivec[j][kp], tmpv);
                 for (dp = kp+1; dp <= sW; dp++)
                   {
                     tmpv = WORDRSHIFTX(alpha[y]->ivec[j-k][dp-kp], alpha[y]->ivec[j-k][dp-kp-1],7);
-                    tmpv = _mm_adds_epi16(tmpv, begr_v);
-                    alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                    tmpv = vec_addsaturating8sh(tmpv, begr_v);
+                    alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                   }
               }
 /*
@@ -4244,7 +4244,7 @@
             {
               esc_stale[x] = i0-1;
               for (dp = 0; dp <= W/vecwidth; dp ++) { vec_Pesc[x][dp] = neginfv; }
-              vec_Pesc[x][0] = WORDRSHIFTX(vec_Pesc[x][0], _mm_set1_epi16(ocm->oesc[v][dsq[i0]*ocm->abc->Kp+x]), 1);
+              vec_Pesc[x][0] = WORDRSHIFTX(vec_Pesc[x][0], vec_splat8sh(ocm->oesc[v][dsq[i0]*ocm->abc->Kp+x]), 1);
             } 
           alpha[v]->ivec[i0-1][0] = neginfv; /* jp = 0 */
 	  for (jp = 1; jp <= W; jp++) {
@@ -4252,7 +4252,7 @@
             /* slide esc vec array over */
             sW = jp/vecwidth;
             delta = j>0 ? j - esc_stale[dsq[j]] : 0;
-            tmpv = _mm_setr_epi16(jp<W ? ocm->oesc[v][dsq[j+1]*ocm->abc->Kp+dsq[j]] : -32768,
+            tmpv = vec_setreverse8sh(jp<W ? ocm->oesc[v][dsq[j+1]*ocm->abc->Kp+dsq[j]] : -32768,
                                   jp>0 ? ocm->oesc[v][dsq[j  ]*ocm->abc->Kp+dsq[j]] : -32768,
                                   jp>1 ? ocm->oesc[v][dsq[j-1]*ocm->abc->Kp+dsq[j]] : -32768,
                                   jp>2 ? ocm->oesc[v][dsq[j-2]*ocm->abc->Kp+dsq[j]] : -32768,
@@ -4295,40 +4295,40 @@
             vec_Pesc[dsq[j]][0] = tmpv;
             if (j>0) esc_stale[dsq[j]] = j; 
 
-            tmpv = (__m128i) esl_sse_rightshift_ps((__m128) _mm_mullo_epi16(el_self_v, doffset), (__m128) neginfv);
-            alpha[v]->ivec[j][0] = _mm_adds_epi16(_mm_set1_epi16(ocm->endsc[v]), tmpv);
+            tmpv = (__m128i) esl_sse_rightshift_ps((__m128) vec_multiply8sh(el_self_v, doffset), (__m128) neginfv);
+            alpha[v]->ivec[j][0] = vec_addsaturating8sh(vec_splat8sh(ocm->endsc[v]), tmpv);
             /* treat EL as emitting only on self transition */
             y = ocm->cfirst[v];
             for (yoffset = 0; yoffset < ocm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_epi16(ocm->tsc[v][yoffset]);
+              tscv = vec_splat8sh(ocm->tsc[v][yoffset]);
               if (j==0) tmpv = neginfv;
               else
                 tmpv = (__m128i) esl_sse_rightshift_ps((__m128) alpha[y+yoffset]->ivec[j-1][0], (__m128) neginfv);
-              tmpv = _mm_adds_epi16(tmpv, tscv);
-              alpha[v]->ivec[j][0] = _mm_max_epi16(alpha[v]->ivec[j][0], tmpv);
+              tmpv = vec_addsaturating8sh(tmpv, tscv);
+              alpha[v]->ivec[j][0] = vec_max8sh(alpha[v]->ivec[j][0], tmpv);
             }
             //not sure this logic is correct... and might not be necessary anyway
             //escv = j>0 ? (__m128i) esl_sse_rightshift_ps((__m128) vec_Pesc[dsq[j]][0], (__m128) neginfv) : neginfv;
             escv = j>0 ? vec_Pesc[dsq[j]][0] : neginfv;
-            alpha[v]->ivec[j][0] = _mm_adds_epi16(alpha[v]->ivec[j][0], escv);
+            alpha[v]->ivec[j][0] = vec_addsaturating8sh(alpha[v]->ivec[j][0], escv);
 
 	    for (dp = 1; dp <= sW; dp++) 
 	      {
-                tmpv = _mm_mullo_epi16(el_self_v, _mm_adds_epi16(_mm_set1_epi16(dp*vecwidth-2), doffset));
-                mask = _mm_cmpgt_epi16(zerov,tmpv);
-                tmpv = _mm_and_si128(tmpv,mask);
-                tmpv = _mm_or_si128(tmpv,_mm_andnot_si128(mask,_mm_set1_epi16(0x8000)));
-		alpha[v]->ivec[j][dp] = _mm_adds_epi16(_mm_set1_epi16(ocm->endsc[v]), tmpv);
+                tmpv = vec_multiply8sh(el_self_v, vec_addsaturating8sh(vec_splat8sh(dp*vecwidth-2), doffset));
+                mask = vec_comparegt8sh(zerov,tmpv);
+                tmpv = vec_bitand1q(tmpv,mask);
+                tmpv = vec_bitor1q(tmpv,vec_bitandnotleft1q(mask,vec_splat8sh(0x8000)));
+		alpha[v]->ivec[j][dp] = vec_addsaturating8sh(vec_splat8sh(ocm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
               }
 
 	    for (yoffset = 0; yoffset < ocm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_epi16(ocm->tsc[v][yoffset]);
+              tscv = vec_splat8sh(ocm->tsc[v][yoffset]);
 	      for (dp = 1; dp <= sW; dp++) 
 	        {
                   tmpv = WORDRSHIFTX(alpha[y+yoffset]->ivec[j-1][dp],alpha[y+yoffset]->ivec[j-1][dp-1],2);
-                  tmpv = _mm_adds_epi16(tmpv, tscv);
-                  alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                  tmpv = vec_addsaturating8sh(tmpv, tscv);
+                  alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                 }
               }
 		
@@ -4336,7 +4336,7 @@
               {
 		i = j-dp*vecwidth+1;
                 escv = vec_Pesc[dsq[j]][dp];
-                alpha[v]->ivec[j][dp] = _mm_adds_epi16(alpha[v]->ivec[j][dp], escv);
+                alpha[v]->ivec[j][dp] = vec_addsaturating8sh(alpha[v]->ivec[j][dp], escv);
 	      }
 
             for (x = 0; x < ocm->abc->Kp; x++)
@@ -4344,7 +4344,7 @@
                 delta = j - esc_stale[x];
                 if (delta == vecwidth) {
                   for (dp = sW; dp > 0; dp--) { vec_Pesc[x][dp] = vec_Pesc[x][dp-1]; }
-                  vec_Pesc[x][0] = _mm_setr_epi16(jp<W ? ocm->oesc[v][dsq[j+1]*ocm->abc->Kp+x] : -32768,
+                  vec_Pesc[x][0] = vec_setreverse8sh(jp<W ? ocm->oesc[v][dsq[j+1]*ocm->abc->Kp+x] : -32768,
                                                          ocm->oesc[v][dsq[j  ]*ocm->abc->Kp+x],
                                                          ocm->oesc[v][dsq[j-1]*ocm->abc->Kp+x],
                                                          ocm->oesc[v][dsq[j-2]*ocm->abc->Kp+x],
@@ -4361,42 +4361,42 @@
       else if (ocm->sttype[v] == ML_st)
 	{
           /* initialize esc vec array */
-          vec_Lesc[0] = WORDRSHIFTX(neginfv,_mm_set1_epi16(ocm->oesc[v][dsq[i0]]),1);
+          vec_Lesc[0] = WORDRSHIFTX(neginfv,vec_splat8sh(ocm->oesc[v][dsq[i0]]),1);
           for (dp = 1; dp <= W/vecwidth; dp++) { vec_Lesc[dp] = neginfv; }
 
 	  for (jp = 0; jp <= W; jp++) {
 	    j = i0-1+jp;
-            tmpv = WORDRSHIFTX(_mm_mullo_epi16(el_self_v, doffset), neginfv, 1);
-	    alpha[v]->ivec[j][0] = _mm_adds_epi16(_mm_set1_epi16(ocm->endsc[v]), tmpv);
+            tmpv = WORDRSHIFTX(vec_multiply8sh(el_self_v, doffset), neginfv, 1);
+	    alpha[v]->ivec[j][0] = vec_addsaturating8sh(vec_splat8sh(ocm->endsc[v]), tmpv);
             /* treat EL as emitting only on self transition */
             y = ocm->cfirst[v];
             for (yoffset = 0; yoffset < ocm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_epi16(ocm->tsc[v][yoffset]);
+              tscv = vec_splat8sh(ocm->tsc[v][yoffset]);
               tmpv = WORDRSHIFTX(alpha[y+yoffset]->ivec[j][0], neginfv, 1);
-              tmpv = _mm_adds_epi16(tmpv, tscv);
-              alpha[v]->ivec[j][0] = _mm_max_epi16(alpha[v]->ivec[j][0], tmpv);
+              tmpv = vec_addsaturating8sh(tmpv, tscv);
+              alpha[v]->ivec[j][0] = vec_max8sh(alpha[v]->ivec[j][0], tmpv);
             }
             escv = sse_setlw_neginfv(vec_Lesc[0]);
-            alpha[v]->ivec[j][0] = _mm_adds_epi16(alpha[v]->ivec[j][0], escv);
+            alpha[v]->ivec[j][0] = vec_addsaturating8sh(alpha[v]->ivec[j][0], escv);
 
             sW = jp/vecwidth;
 	    for (dp = 1; dp <= sW; dp++)
 	      {
-                tmpv = _mm_mullo_epi16(el_self_v, _mm_adds_epi16(_mm_set1_epi16(dp*vecwidth - 1), doffset));
-                mask = _mm_cmpgt_epi16(zerov,tmpv);
-                tmpv = _mm_and_si128(tmpv,mask);
-                tmpv = _mm_or_si128(tmpv,_mm_andnot_si128(mask,_mm_set1_epi16(0x8000)));
-		alpha[v]->ivec[j][dp] = _mm_adds_epi16(_mm_set1_epi16(ocm->endsc[v]), tmpv);
+                tmpv = vec_multiply8sh(el_self_v, vec_addsaturating8sh(vec_splat8sh(dp*vecwidth - 1), doffset));
+                mask = vec_comparegt8sh(zerov,tmpv);
+                tmpv = vec_bitand1q(tmpv,mask);
+                tmpv = vec_bitor1q(tmpv,vec_bitandnotleft1q(mask,vec_splat8sh(0x8000)));
+		alpha[v]->ivec[j][dp] = vec_addsaturating8sh(vec_splat8sh(ocm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
               }
 
 	    for (yoffset = 0; yoffset < ocm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_epi16(ocm->tsc[v][yoffset]);
+              tscv = vec_splat8sh(ocm->tsc[v][yoffset]);
 	      for (dp = 1; dp <= sW; dp++)
 	        {
                   tmpv = WORDRSHIFTX(alpha[y+yoffset]->ivec[j][dp], alpha[y+yoffset]->ivec[j][dp-1], 1);
-                  tmpv = _mm_adds_epi16(tmpv, tscv);
-                  alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                  tmpv = vec_addsaturating8sh(tmpv, tscv);
+                  alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                 }
               }
 
@@ -4404,13 +4404,13 @@
 	      {
 		i = j-dp*vecwidth+1;
                 escv = vec_Lesc[dp];
-                alpha[v]->ivec[j][dp] = _mm_adds_epi16(alpha[v]->ivec[j][dp], escv);
+                alpha[v]->ivec[j][dp] = vec_addsaturating8sh(alpha[v]->ivec[j][dp], escv);
 	      }
 
             /* slide esc vec array over by one */
             if (sW < W/vecwidth) sW++;
             for (dp = sW; dp > 0; dp--) { vec_Lesc[dp] = WORDRSHIFTX(vec_Lesc[dp], vec_Lesc[dp-1], 1); }
-            vec_Lesc[0] = WORDRSHIFTX(vec_Lesc[0], (jp<W-1) ? _mm_set1_epi16(ocm->oesc[v][dsq[j+2]]) : neginfv, 1);
+            vec_Lesc[0] = WORDRSHIFTX(vec_Lesc[0], (jp<W-1) ? vec_splat8sh(ocm->oesc[v][dsq[j+2]]) : neginfv, 1);
 	  }
 	}
       /* The self-transition loop on IL_st will need to be completely serialized, since
@@ -4419,64 +4419,64 @@
       else if (ocm->sttype[v] == IL_st)
 	{
           /* initialize esc vec array */
-          vec_Lesc[0] = WORDRSHIFTX(neginfv,_mm_set1_epi16(ocm->oesc[v][dsq[i0]]),1);
+          vec_Lesc[0] = WORDRSHIFTX(neginfv,vec_splat8sh(ocm->oesc[v][dsq[i0]]),1);
           for (dp = 1; dp <= W/vecwidth; dp++) { vec_Lesc[dp] = neginfv; }
 
 	  for (jp = 0; jp <= W; jp++) {
 	    j = i0-1+jp;
-            tmpv = WORDRSHIFTX(_mm_mullo_epi16(el_self_v, doffset), neginfv, 1);
-	    alpha[v]->ivec[j][0] = _mm_adds_epi16(_mm_set1_epi16(ocm->endsc[v]), tmpv);
+            tmpv = WORDRSHIFTX(vec_multiply8sh(el_self_v, doffset), neginfv, 1);
+	    alpha[v]->ivec[j][0] = vec_addsaturating8sh(vec_splat8sh(ocm->endsc[v]), tmpv);
             /* treat EL as emitting only on self transition */
             y = ocm->cfirst[v];
             for (yoffset = 1; yoffset < ocm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_epi16(ocm->tsc[v][yoffset]);
+              tscv = vec_splat8sh(ocm->tsc[v][yoffset]);
               tmpv = WORDRSHIFTX(alpha[y+yoffset]->ivec[j][0], neginfv, 1);
-              tmpv = _mm_adds_epi16(tmpv, tscv);
-              alpha[v]->ivec[j][0] = _mm_max_epi16(alpha[v]->ivec[j][0], tmpv);
+              tmpv = vec_addsaturating8sh(tmpv, tscv);
+              alpha[v]->ivec[j][0] = vec_max8sh(alpha[v]->ivec[j][0], tmpv);
             }
             escv = sse_setlw_neginfv(vec_Lesc[0]);
-            alpha[v]->ivec[j][0] = _mm_adds_epi16(alpha[v]->ivec[j][0], escv);
+            alpha[v]->ivec[j][0] = vec_addsaturating8sh(alpha[v]->ivec[j][0], escv);
             /* handle yoffset = 0, the self-transition case, seaparately */
-            tscv = _mm_set1_epi16(ocm->tsc[v][0]);
+            tscv = vec_splat8sh(ocm->tsc[v][0]);
             for (k = 2; k < vecwidth; k++) {
               tmpv = WORDRSHIFTX(alpha[y]->ivec[j][0], neginfv, 1);
-              tmpv = _mm_adds_epi16(escv, _mm_adds_epi16(tscv, tmpv));
-              alpha[v]->ivec[j][0] = _mm_max_epi16(alpha[v]->ivec[j][0], tmpv);
+              tmpv = vec_addsaturating8sh(escv, vec_addsaturating8sh(tscv, tmpv));
+              alpha[v]->ivec[j][0] = vec_max8sh(alpha[v]->ivec[j][0], tmpv);
               /* could make this a do-while on whether any values changed */
             }
 
             sW = jp/vecwidth;
 	    for (dp = 1; dp <= sW; dp++)
 	      {
-                tmpv = _mm_mullo_epi16(el_self_v, _mm_adds_epi16(_mm_set1_epi16(dp*vecwidth - 1), doffset));
-                mask = _mm_cmpgt_epi16(zerov,tmpv);
-                tmpv = _mm_and_si128(tmpv,mask);
-                tmpv = _mm_or_si128(tmpv,_mm_andnot_si128(mask,_mm_set1_epi16(0x8000)));
-		alpha[v]->ivec[j][dp] = _mm_adds_epi16(_mm_set1_epi16(ocm->endsc[v]), tmpv);
+                tmpv = vec_multiply8sh(el_self_v, vec_addsaturating8sh(vec_splat8sh(dp*vecwidth - 1), doffset));
+                mask = vec_comparegt8sh(zerov,tmpv);
+                tmpv = vec_bitand1q(tmpv,mask);
+                tmpv = vec_bitor1q(tmpv,vec_bitandnotleft1q(mask,vec_splat8sh(0x8000)));
+		alpha[v]->ivec[j][dp] = vec_addsaturating8sh(vec_splat8sh(ocm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
               }
 
 	    for (yoffset = 1; yoffset < ocm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_epi16(ocm->tsc[v][yoffset]);
+              tscv = vec_splat8sh(ocm->tsc[v][yoffset]);
 	      for (dp = 1; dp <= sW; dp++)
 	        {
                   tmpv = WORDRSHIFTX(alpha[y+yoffset]->ivec[j][dp], alpha[y+yoffset]->ivec[j][dp-1], 1);
-                  tmpv = _mm_adds_epi16(tmpv, tscv);
-                  alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                  tmpv = vec_addsaturating8sh(tmpv, tscv);
+                  alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                 }
               }
 		
-            tscv = _mm_set1_epi16(ocm->tsc[v][0]);
+            tscv = vec_splat8sh(ocm->tsc[v][0]);
 	    for (dp = 1; dp <= sW; dp++)
 	      {
                 escv = vec_Lesc[dp];
-                alpha[v]->ivec[j][dp] = _mm_adds_epi16(alpha[v]->ivec[j][dp], escv);
+                alpha[v]->ivec[j][dp] = vec_addsaturating8sh(alpha[v]->ivec[j][dp], escv);
 
                 /* handle yoffset = 0, the self-transition case, seaparately */
                 for (k = 0; k < vecwidth; k++) {
                   tmpv = WORDRSHIFTX(alpha[y]->ivec[j][dp], alpha[y]->ivec[j][dp-1], 1);
-                  tmpv = _mm_adds_epi16(escv, _mm_adds_epi16(tscv, tmpv));
-                  alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                  tmpv = vec_addsaturating8sh(escv, vec_addsaturating8sh(tscv, tmpv));
+                  alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                   /* could make this a do-while on whether any values changed */
                 }
 	      }
@@ -4484,7 +4484,7 @@
             /* slide esc vec array over by one */
             if (sW < W/vecwidth) sW++;
             for (dp = sW; dp > 0; dp--) { vec_Lesc[dp] = WORDRSHIFTX(vec_Lesc[dp], vec_Lesc[dp-1], 1); }
-            vec_Lesc[0] = WORDRSHIFTX(vec_Lesc[0], (jp<W-1) ? _mm_set1_epi16(ocm->oesc[v][dsq[j+2]]) : neginfv, 1);
+            vec_Lesc[0] = WORDRSHIFTX(vec_Lesc[0], (jp<W-1) ? vec_splat8sh(ocm->oesc[v][dsq[j+2]]) : neginfv, 1);
 	  }
 	}
       else if (ocm->sttype[v] == IR_st || ocm->sttype[v] == MR_st)
@@ -4492,48 +4492,48 @@
           alpha[v]->ivec[i0-1][0] = neginfv; /* jp = 0 */
 	  for (jp = 1; jp <= W; jp++) {
 	    j = i0-1+jp;
-            tmpv = WORDRSHIFTX(_mm_mullo_epi16(el_self_v, doffset), neginfv, 1);
-	    alpha[v]->ivec[j][0] = _mm_adds_epi16(_mm_set1_epi16(ocm->endsc[v]), tmpv);
+            tmpv = WORDRSHIFTX(vec_multiply8sh(el_self_v, doffset), neginfv, 1);
+	    alpha[v]->ivec[j][0] = vec_addsaturating8sh(vec_splat8sh(ocm->endsc[v]), tmpv);
             /* treat EL as emitting only on self transition */
             y = ocm->cfirst[v];
             for (yoffset = 0; yoffset < ocm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_epi16(ocm->tsc[v][yoffset]);
+              tscv = vec_splat8sh(ocm->tsc[v][yoffset]);
               if (j==0) tmpv = neginfv;
               else
                 tmpv = WORDRSHIFTX(alpha[y+yoffset]->ivec[j-1][0], neginfv, 1);
-              tmpv = _mm_adds_epi16(tmpv, tscv);
-              alpha[v]->ivec[j][0] = _mm_max_epi16(alpha[v]->ivec[j][0], tmpv);
+              tmpv = vec_addsaturating8sh(tmpv, tscv);
+              alpha[v]->ivec[j][0] = vec_max8sh(alpha[v]->ivec[j][0], tmpv);
             }
             if (j==0) escv = neginfv;
             else
-              escv = sse_setlw_neginfv(_mm_set1_epi16(ocm->oesc[v][dsq[j]]));
-            alpha[v]->ivec[j][0] = _mm_adds_epi16(alpha[v]->ivec[j][0], escv);
+              escv = sse_setlw_neginfv(vec_splat8sh(ocm->oesc[v][dsq[j]]));
+            alpha[v]->ivec[j][0] = vec_addsaturating8sh(alpha[v]->ivec[j][0], escv);
 
             sW = jp/vecwidth;
             if (j==0) escv = neginfv;
             else
-              escv = _mm_set1_epi16(ocm->oesc[v][dsq[j]]);
+              escv = vec_splat8sh(ocm->oesc[v][dsq[j]]);
 
 	    for (dp = 1; dp <= sW; dp++)
 	      {
-                tmpv = _mm_mullo_epi16(el_self_v, _mm_adds_epi16(_mm_set1_epi16(dp*vecwidth - 1), doffset));
-                mask = _mm_cmpgt_epi16(zerov,tmpv);
-                tmpv = _mm_and_si128(tmpv,mask);
-                tmpv = _mm_or_si128(tmpv,_mm_andnot_si128(mask,_mm_set1_epi16(0x8000)));
-		alpha[v]->ivec[j][dp] = _mm_adds_epi16(_mm_set1_epi16(ocm->endsc[v]), tmpv);
+                tmpv = vec_multiply8sh(el_self_v, vec_addsaturating8sh(vec_splat8sh(dp*vecwidth - 1), doffset));
+                mask = vec_comparegt8sh(zerov,tmpv);
+                tmpv = vec_bitand1q(tmpv,mask);
+                tmpv = vec_bitor1q(tmpv,vec_bitandnotleft1q(mask,vec_splat8sh(0x8000)));
+		alpha[v]->ivec[j][dp] = vec_addsaturating8sh(vec_splat8sh(ocm->endsc[v]), tmpv);
 		/* treat EL as emitting only on self transition */
               }
 
 	    for (yoffset = 0; yoffset < ocm->cnum[v]; yoffset++) {
-              tscv = _mm_set1_epi16(ocm->tsc[v][yoffset]);
+              tscv = vec_splat8sh(ocm->tsc[v][yoffset]);
 	      for (dp = 1; dp <= sW; dp++)
 	        {
                   tmpv = WORDRSHIFTX(alpha[y+yoffset]->ivec[j-1][dp], alpha[y+yoffset]->ivec[j-1][dp-1], 1);
-                  tmpv = _mm_adds_epi16(tmpv, tscv);
-                  alpha[v]->ivec[j][dp] = _mm_max_epi16(alpha[v]->ivec[j][dp], tmpv);
+                  tmpv = vec_addsaturating8sh(tmpv, tscv);
+                  alpha[v]->ivec[j][dp] = vec_max8sh(alpha[v]->ivec[j][dp], tmpv);
                 }
 		
-                alpha[v]->ivec[j][dp] = _mm_adds_epi16(alpha[v]->ivec[j][dp], escv);
+                alpha[v]->ivec[j][dp] = vec_addsaturating8sh(alpha[v]->ivec[j][dp], escv);
 	      }
 	  }
 	}				/* finished calculating deck v. */
@@ -4557,15 +4557,15 @@
 //FIXME: On the other hand, saturation may counter-indicate this, as later alignments
 //FIXME: comprising larger parts of the model and larger sequence windows will not
 //FIXME: be able to exceed previously seen scores
-      tmp_v = _mm_set1_epi16((int16_t)v);
+      tmp_v = vec_splat8sh((int16_t)v);
       for (jp = 0; jp <= W; jp++) {
 	j = i0-1+jp;
 //if(v<8 && j==26) fprintf(stderr,"v = %d  j = %d  beginsc %e %f\n",v,j,cm->beginsc[v],(float)ocm->beginsc[v]/ocm->scale_w); 
-        tmp_j = _mm_set1_epi16((int16_t)jp);
+        tmp_j = vec_splat8sh((int16_t)jp);
         sW = jp/vecwidth;
 	for (dp = 0; dp <= sW; dp++) {
-          tmp_d = _mm_adds_epi16(_mm_set1_epi16((int16_t)dp*vecwidth), doffset);
-          //tmpv = _mm_adds_epi16(alpha[v]->ivec[j][dp], vec_unaligned_sc[dp]);
+          tmp_d = vec_addsaturating8sh(vec_splat8sh((int16_t)dp*vecwidth), doffset);
+          //tmpv = vec_addsaturating8sh(alpha[v]->ivec[j][dp], vec_unaligned_sc[dp]);
 //FIXME: Testing whether skipping unaligned_sc helps with coordinates returned here
 tmpv = alpha[v]->ivec[j][dp];
 /*
@@ -4581,10 +4581,10 @@
 */
           //FIXME: what is ocm->beginsc[0] set to - does this cover our
           //FIXME: root deck, or do we need to handle that separately?
-          tmpv = _mm_adds_epi16(tmpv, _mm_set1_epi16(ocm->beginsc[v]));
-          mask  = _mm_cmpgt_epi16(tmpv, vb_sc);
-//mask = _mm_or_si128(mask,_mm_cmpeq_epi16(tmpv,vb_sc));	//FIXME: Test - break ties in favor of new values
-          vb_sc = _mm_max_epi16(tmpv, vb_sc);
+          tmpv = vec_addsaturating8sh(tmpv, vec_splat8sh(ocm->beginsc[v]));
+          mask  = vec_comparegt8sh(tmpv, vb_sc);
+//mask = vec_bitor1q(mask,vec_compareeq8sh(tmpv,vb_sc));	//FIXME: Test - break ties in favor of new values
+          vb_sc = vec_max8sh(tmpv, vb_sc);
           vb_j  = sse_select_si128(vb_j, tmp_j, mask);
           vb_d  = sse_select_si128(vb_d, tmp_d, mask);
           vb_v  = sse_select_si128(vb_v, tmp_v, mask);
@@ -4644,21 +4644,21 @@
   if (ret_coord != NULL) {
     /* Narrow return coordinates to those that match highest score */
     ret_coord->score = sc;
-    mask = _mm_cmpeq_epi16(_mm_set1_epi16(sc),vb_sc);
-    vb_d = _mm_and_si128(vb_d,mask);
-    vb_j = _mm_and_si128(vb_j,mask);
-    vb_v = _mm_and_si128(vb_v,mask);
+    mask = vec_compareeq8sh(vec_splat8sh(sc),vb_sc);
+    vb_d = vec_bitand1q(vb_d,mask);
+    vb_j = vec_bitand1q(vb_j,mask);
+    vb_v = vec_bitand1q(vb_v,mask);
 
     /* Break ties in favor of longer segments */
     ret_coord->d = esl_sse_hmax_epi16(vb_d);
-    mask = _mm_cmpeq_epi16(_mm_set1_epi16(ret_coord->d),vb_d);
-    vb_j = _mm_and_si128(vb_j,mask);
-    vb_v = _mm_and_si128(vb_v,mask);
+    mask = vec_compareeq8sh(vec_splat8sh(ret_coord->d),vb_d);
+    vb_j = vec_bitand1q(vb_j,mask);
+    vb_v = vec_bitand1q(vb_v,mask);
 
     /* Break ties in favor of larger v (smaller model segments) */
     ret_coord->v = esl_sse_hmax_epi16(vb_v);
-    mask = _mm_cmpeq_epi16(_mm_set1_epi16(ret_coord->v),vb_v);
-    vb_j = _mm_and_si128(vb_j,mask);
+    mask = vec_compareeq8sh(vec_splat8sh(ret_coord->v),vb_v);
+    vb_j = vec_bitand1q(vb_j,mask);
 
     /* Break (remaining?!) ties in favor of larger j */
     ret_coord->j = i0-1+esl_sse_hmax_epi16(vb_j);
--- src/impl_sse/sse_cmcons_hitmx.c
+++ src/impl_sse/sse_cmcons_hitmx.c
@@ -22,8 +22,8 @@
 #include <stdio.h>
 #include <stdlib.h>
 
-#include <xmmintrin.h>
-#include <emmintrin.h>
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_vectorops.h"
--- src/impl_sse/sse_cmcons_mscyk.c
+++ src/impl_sse/sse_cmcons_mscyk.c
@@ -22,8 +22,8 @@
 #include <stdio.h>
 #include <stdlib.h>
 
-#include <xmmintrin.h>		/* SSE  */
-#include <emmintrin.h>		/* SSE2 */
+#include <vec128int.h>
+#include <vec128sp.h>
 
 #include "easel.h"
 #include "esl_sqio.h"
@@ -37,9 +37,9 @@
 
 #include "impl_sse.h"
 
-#define BYTERSHIFT1(a)     (_mm_or_si128(_mm_slli_si128(a,1), LB_NEG_INF))
-#define BYTERSHIFT2(a,b)   (_mm_or_si128(_mm_slli_si128(a,1), _mm_srli_si128(b,15)))
-#define BYTERSHIFT3(a,b,c) (_mm_or_si128(_mm_slli_si128(a,c), _mm_srli_si128(b,16-c)))
+#define BYTERSHIFT1(a)     (vec_bitor1q(vec_shiftleftbytes1q(a,1), LB_NEG_INF))
+#define BYTERSHIFT2(a,b)   (vec_bitor1q(vec_shiftleftbytes1q(a,1), vec_shiftrightbytes1q(b,15)))
+#define BYTERSHIFT3(a,b,c) (vec_bitor1q(vec_shiftleftbytes1q(a,c), vec_shiftrightbytes1q(b,16-c)))
 
 static __m128i MSCYK_add_BIF(__m128i bifl, __m128i bifr, __m128i tsv, __m128i base)
 {
@@ -47,19 +47,19 @@
    __m128i umask;	/* underflow mask */
    __m128i omask;	/* overflow mask */
    __m128i ocx;		/* overflow correction term */
-   __m128i neginfv = _mm_set1_epi8(0x00);
-   __m128i allonev = _mm_set1_epi8(0xff);
+   __m128i neginfv = vec_splat16sb(0x00);
+   __m128i allonev = vec_splat16sb(0xff);
 
-   umask = _mm_xor_si128(_mm_or_si128(_mm_cmpeq_epi8(bifl,neginfv),_mm_cmpeq_epi8(bifr,neginfv)),allonev);
+   umask = vec_bitxor1q(vec_bitor1q(vec_compareeq16sb(bifl,neginfv),vec_compareeq16sb(bifr,neginfv)),allonev);
 						/* Check and mask any element where either vector had a 0 value */
-   ret_v = _mm_adds_epu8(bifl,bifr);		/* Normal, saturated addition */
-   omask = _mm_cmpeq_epi8(ret_v,allonev);	/* Check for overflow */
-   ret_v = _mm_subs_epu8(ret_v, tsv);		/* Subtract transition cost */
-   ret_v = _mm_subs_epu8(ret_v, base);		/* both the L and R bif values already include base offset, so subtract base once */
-   ocx   = _mm_add_epi8(_mm_add_epi8(bifl,bifr),_mm_set1_epi8(1)); /* Unsaturated math, purposefully overflow */
-   ocx   = _mm_and_si128(ocx, omask);		/* zero values that didn't overflow */
-   ret_v = _mm_adds_epu8(ret_v, ocx);		/* saturated add of overflow amount */
-   ret_v = _mm_and_si128(ret_v, umask);		/* underflow mask zeroes sum if an operand was zero (0 = -infty) */
+   ret_v = vec_addsaturating16ub(bifl,bifr);		/* Normal, saturated addition */
+   omask = vec_compareeq16sb(ret_v,allonev);	/* Check for overflow */
+   ret_v = vec_subtractsaturating16ub(ret_v, tsv);		/* Subtract transition cost */
+   ret_v = vec_subtractsaturating16ub(ret_v, base);		/* both the L and R bif values already include base offset, so subtract base once */
+   ocx   = vec_add16sb(vec_add16sb(bifl,bifr),vec_splat16sb(1)); /* Unsaturated math, purposefully overflow */
+   ocx   = vec_bitand1q(ocx, omask);		/* zero values that didn't overflow */
+   ret_v = vec_addsaturating16ub(ret_v, ocx);		/* saturated add of overflow amount */
+   ret_v = vec_bitand1q(ret_v, umask);		/* underflow mask zeroes sum if an operand was zero (0 = -infty) */
 
    return ret_v;
 }
@@ -115,10 +115,10 @@
   __m128i   tsv_S_Sa, tsv_S_SM, tsv_S_e, tsv_M_S;
   __m128i   tmpv, tmpv2;
 
-  tsv_S_Sa = _mm_set1_epi8(ccm->tsb_S_Sa);
-  tsv_S_SM = _mm_set1_epi8(ccm->tsb_S_SM);
-  tsv_S_e  = _mm_set1_epi8(ccm->tsb_S_e );
-  tsv_M_S  = _mm_set1_epi8(ccm->tsb_M_S );
+  tsv_S_Sa = vec_splat16sb(ccm->tsb_S_Sa);
+  tsv_S_SM = vec_splat16sb(ccm->tsb_S_SM);
+  tsv_S_e  = vec_splat16sb(ccm->tsb_S_e );
+  tsv_M_S  = vec_splat16sb(ccm->tsb_M_S );
 /*
 fprintf(stderr,"S -> Sa: %d\t", unbiased_byteify(ccm,tsc_S_Sa));
 fprintf(stderr,"S -> SM: %d\t", unbiased_byteify(ccm,tsc_S_SM));
@@ -174,7 +174,7 @@
 
   char BADVAL = 0x00; /* Our local equivalent of -eslINFINITY */
   char escBADVAL = biased_byteify(ccm, -eslINFINITY); /* -inf as a cost for non-permitted esc */
-  __m128i LB_NEG_INF = _mm_setr_epi8(BADVAL,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
+  __m128i LB_NEG_INF = vec_setreverse16sb(BADVAL,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
 
   L = j0-i0+1;
   if (W > L) W = L; 
@@ -222,10 +222,10 @@
    * by 2 to allow clean access for the range -2..sW.  
    */
   sW = W/16 + 1; 
-//  zerov = _mm_setzero_ps();
-  neginfv = _mm_set1_epi8(BADVAL);
-  biasv   = _mm_set1_epi8(ccm->bias_b);
-  basev   = _mm_set1_epi8(ccm->base_b);
+//  zerov = vec_zero4sp();
+  neginfv = vec_splat16sb(BADVAL);
+  biasv   = vec_splat16sb(ccm->bias_b);
+  basev   = vec_splat16sb(ccm->base_b);
   ESL_ALLOC(vec_ntM_v,        sizeof(__m128i**) * 2);
   ESL_ALLOC(vec_ntM_v[0],     sizeof(__m128i *) * 2 * ccm->M);
   ESL_ALLOC(mem_ntM_v,        sizeof(__m128i  ) * 2 * ccm->M * (sW+2) + 15);
@@ -272,9 +272,9 @@
 
   for (jp_v = 0; jp_v <= W; jp_v++)
     {
-      vec_ntS[jp_v][0] = _mm_setr_epi8(ccm->base_b,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,
+      vec_ntS[jp_v][0] = vec_setreverse16sb(ccm->base_b,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,
                                             BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL);
-      vec_ntS[jp_v][0] = _mm_subs_epu8(vec_ntS[jp_v][0], tsv_S_e);
+      vec_ntS[jp_v][0] = vec_subtractsaturating16ub(vec_ntS[jp_v][0], tsv_S_e);
 
       for (d = 1; d < sW; d++)
         {
@@ -353,12 +353,12 @@
             else if (ccm->sttype[v] == MR_st) {
               /* esc constant across the row */
                 for (d = 0; d < sW; d++) {
-                  vec_esc[dsq[j]][v][d] = _mm_set1_epi8(ccm->oesc[v][dsq[j]]);
+                  vec_esc[dsq[j]][v][d] = vec_splat16sb(ccm->oesc[v][dsq[j]]);
                 }
             }
             else if (ccm->sttype[v] == ML_st) {
               for (d = 0; d < sW; d++) {
-                vec_esc[dsq[j]][v][d] = _mm_setr_epi8((d <= j && ((j^j0)|d) ) ? ccm->oesc[v][dsq[j-(      d)+1]] : escBADVAL,
+                vec_esc[dsq[j]][v][d] = vec_setreverse16sb((d <= j && ((j^j0)|d) ) ? ccm->oesc[v][dsq[j-(      d)+1]] : escBADVAL,
                                                 (   sW+d <= j               ) ? ccm->oesc[v][dsq[j-(   sW+d)+1]] : escBADVAL,
                                                 ( 2*sW+d <= j               ) ? ccm->oesc[v][dsq[j-( 2*sW+d)+1]] : escBADVAL,
                                                 ( 3*sW+d <= j               ) ? ccm->oesc[v][dsq[j-( 3*sW+d)+1]] : escBADVAL,
@@ -378,7 +378,7 @@
             }
             else if (ccm->sttype[v] == MP_st) {
               for (d = 0; d < sW; d++) {
-                vec_esc[dsq[j]][v][d] = _mm_setr_epi8((d <= j && ((j^j0)|d) ) ? ccm->oesc[v][dsq[j-(      d)+1]*ccm->abc->Kp+dsq[j]] : escBADVAL,
+                vec_esc[dsq[j]][v][d] = vec_setreverse16sb((d <= j && ((j^j0)|d) ) ? ccm->oesc[v][dsq[j-(      d)+1]*ccm->abc->Kp+dsq[j]] : escBADVAL,
                                                 (   sW+d <= j               ) ? ccm->oesc[v][dsq[j-(   sW+d)+1]*ccm->abc->Kp+dsq[j]] : escBADVAL,
                                                 ( 2*sW+d <= j               ) ? ccm->oesc[v][dsq[j-( 2*sW+d)+1]*ccm->abc->Kp+dsq[j]] : escBADVAL,
                                                 ( 3*sW+d <= j               ) ? ccm->oesc[v][dsq[j-( 3*sW+d)+1]*ccm->abc->Kp+dsq[j]] : escBADVAL,
@@ -407,7 +407,7 @@
             for (d = 0; d < delta; d++) {
               tmpary[d] = vec_esc[dsq[j]][v][sW-delta+d];
               tmp_esc     = (j^j0)|d ? ccm->oesc[v][dsq[j-d+1]] : escBADVAL;
-              tmpary[d] = BYTERSHIFT2(tmpary[d],_mm_set1_epi8(tmp_esc));
+              tmpary[d] = BYTERSHIFT2(tmpary[d],vec_splat16sb(tmp_esc));
             }
             for (d = sW-1; d >= delta; d--) {
               vec_esc[dsq[j]][v][d] = vec_esc[dsq[j]][v][d-delta];
@@ -420,7 +420,7 @@
             for (d = 0; d < delta; d++) {
               tmpary[d] = vec_esc[dsq[j]][v][sW-delta+d];
               tmp_esc     = (j^j0)|d ? ccm->oesc[v][dsq[j-d+1]*ccm->abc->Kp+dsq[j]] : -eslINFINITY;
-              tmpary[d] = BYTERSHIFT2(tmpary[d],_mm_set1_epi8(tmp_esc));
+              tmpary[d] = BYTERSHIFT2(tmpary[d],vec_splat16sb(tmp_esc));
             }
             for (d = sW-1; d >= delta; d--) {
               vec_esc[dsq[j]][v][d] = vec_esc[dsq[j]][v][d-delta];
@@ -445,9 +445,9 @@
 
       /* Start updating matrix values */
       jp_Sv = j % (W+1);
-      vec_ntS[jp_Sv][0] = _mm_setr_epi8(ccm->base_b,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,
+      vec_ntS[jp_Sv][0] = vec_setreverse16sb(ccm->base_b,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,
                                             BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL);
-      vec_ntS[jp_Sv][0] = _mm_subs_epu8(vec_ntS[jp_Sv][0], tsv_S_e);
+      vec_ntS[jp_Sv][0] = vec_subtractsaturating16ub(vec_ntS[jp_Sv][0], tsv_S_e);
 
       for (d = 1; d < sW; d++)
         {
@@ -471,8 +471,8 @@
       /* Rule: S -> Sa */
       jp_Sy = (jp_Sv == 0) ? W : jp_Sv-1;
       for (d = 0; d < sW; d++) {
-        tmpv = _mm_subs_epu8(vec_ntS[jp_Sy][d-1], tsv_S_Sa);
-        vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+        tmpv = vec_subtractsaturating16ub(vec_ntS[jp_Sy][d-1], tsv_S_Sa);
+        vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
       }
       vec_ntS[jp_Sv][-1] = BYTERSHIFT1(vec_ntS[jp_Sv][sW-1]);
       vec_ntS[jp_Sv][-2] = BYTERSHIFT1(vec_ntS[jp_Sv][sW-2]);
@@ -500,29 +500,29 @@
 	    y = ccm->next[v]; 
 
             for (d = 0; d < sW; d++) {
-              tmpv = _mm_subs_epu8(vec_ntS[jp_Sy][d-sd], tsv_M_S);
+              tmpv = vec_subtractsaturating16ub(vec_ntS[jp_Sy][d-sd], tsv_M_S);
               tmpv2= vec_ntM_v[jp_y][y][d-sd];
-              vec_ntM_v[jp_v][v][d] = _mm_max_epu8(tmpv, tmpv2);
-              vec_ntM_v[jp_v][v][d] = _mm_subs_epu8(vec_ntM_v[jp_v][v][d], vec_esc[dsq[j]][v][d]);
-              umask= _mm_xor_si128(_mm_cmpeq_epi8(vec_ntM_v[jp_v][v][d],neginfv),_mm_set1_epi8(0xff));
-              vec_ntM_v[jp_v][v][d] = _mm_adds_epu8(vec_ntM_v[jp_v][v][d], biasv);
-              vec_ntM_v[jp_v][v][d] = _mm_and_si128(vec_ntM_v[jp_v][v][d], umask);
+              vec_ntM_v[jp_v][v][d] = vec_max16ub(tmpv, tmpv2);
+              vec_ntM_v[jp_v][v][d] = vec_subtractsaturating16ub(vec_ntM_v[jp_v][v][d], vec_esc[dsq[j]][v][d]);
+              umask= vec_bitxor1q(vec_compareeq16sb(vec_ntM_v[jp_v][v][d],neginfv),vec_splat16sb(0xff));
+              vec_ntM_v[jp_v][v][d] = vec_addsaturating16ub(vec_ntM_v[jp_v][v][d], biasv);
+              vec_ntM_v[jp_v][v][d] = vec_bitand1q(vec_ntM_v[jp_v][v][d], umask);
 
-              vec_ntM_all[jp_v][d] = _mm_max_epu8(vec_ntM_all[jp_v][d], vec_ntM_v[jp_v][v][d]);
+              vec_ntM_all[jp_v][d] = vec_max16ub(vec_ntM_all[jp_v][d], vec_ntM_v[jp_v][v][d]);
             }
 
             vec_ntM_v[jp_v][v][-1] = BYTERSHIFT1(vec_ntM_v[jp_v][v][sW-1]);
             vec_ntM_v[jp_v][v][-2] = BYTERSHIFT1(vec_ntM_v[jp_v][v][sW-2]);
 
-            vec_ntM_all[jp_v][-1] = _mm_max_epu8(vec_ntM_all[jp_v][-1], vec_ntM_v[jp_v][v][-1]);
-            vec_ntM_all[jp_v][-2] = _mm_max_epu8(vec_ntM_all[jp_v][-2], vec_ntM_v[jp_v][v][-2]);
+            vec_ntM_all[jp_v][-1] = vec_max16ub(vec_ntM_all[jp_v][-1], vec_ntM_v[jp_v][v][-1]);
+            vec_ntM_all[jp_v][-2] = vec_max16ub(vec_ntM_all[jp_v][-2], vec_ntM_v[jp_v][v][-2]);
           }
           else if (ccm->sttype[v] == E_st) {
             /* Part of modified E-state treatment, set d = 0 value to base - frag penalty,
                all other values to -inf                                                     */
-            tmpv = _mm_setr_epi8(ccm->base_b,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,
+            tmpv = vec_setreverse16sb(ccm->base_b,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,
                                       BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL,BADVAL);
-            vec_ntM_v[jp_v][v][0] = _mm_subs_epu8(tmpv,tsv_M_S);
+            vec_ntM_v[jp_v][v][0] = vec_subtractsaturating16ub(tmpv,tsv_M_S);
             for (d = 1; d < sW; d++) {
               vec_ntM_v[jp_v][v][d] = neginfv;
             }
@@ -536,8 +536,8 @@
           }
 //	  if(vsc != NULL) {
 //            tmpv = neginfv;
-//	    if(cm->stid[v] != BEGL_S) for (d = 0; d <= sW; d++) tmpv = _mm_max_ps(tmpv, vec_alpha[jp_v][v][d]);
-//	    else                      for (d = 0; d <= sW; d++) tmpv = _mm_max_ps(tmpv, vec_alpha_begl[jp_v][v][d]);
+//	    if(cm->stid[v] != BEGL_S) for (d = 0; d <= sW; d++) tmpv = vec_max4sp(tmpv, vec_alpha[jp_v][v][d]);
+//	    else                      for (d = 0; d <= sW; d++) tmpv = vec_max4sp(tmpv, vec_alpha_begl[jp_v][v][d]);
 //            esl_sse_hmax_ps(tmpv, &vsc[v]);
 //	  }
 #ifdef DEBUG_0
@@ -571,7 +571,7 @@
 #ifdef _DONT_UNROLL_BIF
       for (k = 0; k <= W && k <=j; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
 
         for (d = 0; d < sW; d++) {
           if (dkindex >= sW) dkindex -= sW;
@@ -595,9 +595,9 @@
 
           dkindex++;
 
-          umask= _mm_xor_si128(_mm_or_si128(_mm_cmpeq_epi8(vec_tmp_bifl,neginfv),_mm_cmpeq_epi8(vec_tmp_bifr,neginfv)),_mm_set1_epi8(0xff));
+          umask= vec_bitxor1q(vec_bitor1q(vec_compareeq16sb(vec_tmp_bifl,neginfv),vec_compareeq16sb(vec_tmp_bifr,neginfv)),vec_splat16sb(0xff));
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
         }
         dkindex--;
       }
@@ -610,13 +610,13 @@
       kmax = W < kmax ? W : kmax;;
       for (k = 0; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = sW - k;
         for (d = 0; d < k; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 1);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -624,7 +624,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl =             vec_ntS[jp_wA[k]][dkindex];
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -633,13 +633,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 2*sW - k;
         for (d = 0; d < k - sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 2);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -647,7 +647,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 1);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -656,13 +656,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 3*sW - k;
         for (d = 0; d < k - 2*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 3);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -670,7 +670,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 2);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -679,13 +679,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 4*sW - k;
         for (d = 0; d < k - 3*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 4);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -693,7 +693,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 3);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -702,13 +702,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 5*sW - k;
         for (d = 0; d < k - 4*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 5);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -716,7 +716,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 4);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -725,13 +725,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 6*sW - k;
         for (d = 0; d < k - 5*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 6);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -739,7 +739,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 5);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -748,13 +748,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 7*sW - k;
         for (d = 0; d < k - 6*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 7);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -762,7 +762,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 6);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -771,13 +771,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 8*sW - k;
         for (d = 0; d < k - 7*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 8);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -785,7 +785,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 7);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -794,13 +794,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 9*sW - k;
         for (d = 0; d < k - 8*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 9);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -808,7 +808,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 8);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -817,13 +817,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 10*sW - k;
         for (d = 0; d < k - 9*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,10);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -831,7 +831,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv, 9);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -840,13 +840,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 11*sW - k;
         for (d = 0; d < k - 10*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,11);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -854,7 +854,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,10);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -863,13 +863,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 12*sW - k;
         for (d = 0; d < k - 11*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,12);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -877,7 +877,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,11);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -886,13 +886,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 13*sW - k;
         for (d = 0; d < k - 12*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,13);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -900,7 +900,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,12);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -909,13 +909,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 14*sW - k;
         for (d = 0; d < k - 13*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,14);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -923,7 +923,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,13);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -932,13 +932,13 @@
       kmax = W < kmax ? W : kmax;;
       for ( ; k <= kmax; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 15*sW - k;
         for (d = 0; d < k - 14*sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,15);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
 
@@ -946,7 +946,7 @@
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,14);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -954,13 +954,13 @@
       kmax = j < W ? j : W;
       for ( ; k <= kmax && k <= W; k++) {
         vec_access = (uint8_t *) (&vec_ntM_all[cur][k%sW])+k/sW;
-        vec_tmp_bifr = _mm_set1_epi8((char) *vec_access);
+        vec_tmp_bifr = vec_splat16sb((char) *vec_access);
       
         dkindex = 0;
         for (     ; d < sW; d++) {
           vec_tmp_bifl = BYTERSHIFT3(vec_ntS[jp_wA[k]][dkindex],neginfv,15);
           tmpv = MSCYK_add_BIF(vec_tmp_bifl, vec_tmp_bifr, tsv_S_SM, basev);
-          vec_ntS[jp_Sv][d] = _mm_max_epu8(vec_ntS[jp_Sv][d], tmpv);
+          vec_ntS[jp_Sv][d] = vec_max16ub(vec_ntS[jp_Sv][d], tmpv);
           dkindex++;
         }
       }
@@ -981,37 +981,37 @@
       /* determine min/max d we're allowing for the root state and this position j */
 //      jp_v = cur;
 //      for (d = 0; d < sW; d++) {
-//	vec_bestr[d] = _mm_setzero_ps();	/* root of the traceback = root state 0 */
+//	vec_bestr[d] = vec_zero4sp();	/* root of the traceback = root state 0 */
 //	y = cm->cfirst[0];
-//        vec_tsc = _mm_set1_ps(tsc_v[0]);
-//	vec_alpha[jp_v][0][d] = _mm_max_ps(neginfv, _mm_add_ps(vec_alpha[cur][y][d],vec_tsc));
+//        vec_tsc = vec_splat4sp(tsc_v[0]);
+//	vec_alpha[jp_v][0][d] = vec_max4sp(neginfv, vec_add4sp(vec_alpha[cur][y][d],vec_tsc));
 //	for (yoffset = 1; yoffset < cm->cnum[0]; yoffset++) {
-//          vec_tsc = _mm_set1_ps(tsc_v[yoffset]);
-//	  vec_alpha[jp_v][0][d] = _mm_max_ps(vec_alpha[jp_v][0][d], _mm_add_ps(vec_alpha[cur][y+yoffset][d],vec_tsc));
+//          vec_tsc = vec_splat4sp(tsc_v[yoffset]);
+//	  vec_alpha[jp_v][0][d] = vec_max4sp(vec_alpha[jp_v][0][d], vec_add4sp(vec_alpha[cur][y+yoffset][d],vec_tsc));
 //        }
 //      }
 //	
 //      if (cm->flags & CMH_LOCAL_BEGIN) {
 //	for (y = 1; y < cm->M; y++) {
 //	  if(NOT_IMPOSSIBLE(cm->beginsc[y])) {
-//            vec_beginsc = _mm_set1_ps(cm->beginsc[y]);
+//            vec_beginsc = vec_splat4sp(cm->beginsc[y]);
 //	    if(cm->stid[y] == BEGL_S)
 //	      {
 //		jp_y = j % (W+1);
 //		for (d = 0; d < sW; d++) {
-//                  tmpv = _mm_add_ps(vec_alpha_begl[jp_y][y][d], vec_beginsc);
-//                  mask  = _mm_cmpgt_ps(tmpv, vec_alpha[jp_v][0][d]);
-//                  vec_alpha[jp_v][0][d] = _mm_max_ps(tmpv, vec_alpha[jp_v][0][d]);
-//                  vec_bestr[d] = esl_sse_select_ps(_mm_set1_ps((float) y), vec_bestr[d], mask);
+//                  tmpv = vec_add4sp(vec_alpha_begl[jp_y][y][d], vec_beginsc);
+//                  mask  = vec_comparegt4sp(tmpv, vec_alpha[jp_v][0][d]);
+//                  vec_alpha[jp_v][0][d] = vec_max4sp(tmpv, vec_alpha[jp_v][0][d]);
+//                  vec_bestr[d] = esl_sse_select_ps(vec_splat4sp((float) y), vec_bestr[d], mask);
 //		}
 //	      }
 //	    else { /* y != BEGL_S */
 //	      jp_y = cur;
 //	      for (d = 0; d < sW; d++) {
-//                  tmpv = _mm_add_ps(vec_alpha[jp_y][y][d], vec_beginsc);
-//                  mask  = _mm_cmpgt_ps(tmpv, vec_alpha[jp_v][0][d]);
-//                  vec_alpha[jp_v][0][d] = _mm_max_ps(tmpv, vec_alpha[jp_v][0][d]);
-//                  vec_bestr[d] = esl_sse_select_ps(_mm_set1_ps((float) y), vec_bestr[d], mask);
+//                  tmpv = vec_add4sp(vec_alpha[jp_y][y][d], vec_beginsc);
+//                  mask  = vec_comparegt4sp(tmpv, vec_alpha[jp_v][0][d]);
+//                  vec_alpha[jp_v][0][d] = vec_max4sp(tmpv, vec_alpha[jp_v][0][d]);
+//                  vec_bestr[d] = esl_sse_select_ps(vec_splat4sp((float) y), vec_bestr[d], mask);
 //	        }
 //	      }
 //	  }
@@ -1020,7 +1020,7 @@
       /* find the best score */
 //      tmpv = neginfv;
 //      for (d = 0; d < sW; d++) 
-//	tmpv = _mm_max_ps(tmpv, vec_alpha[jp_v][0][d]);
+//	tmpv = vec_max4sp(tmpv, vec_alpha[jp_v][0][d]);
 //      esl_sse_hmax_ps(tmpv, &tmp_esc);		/* overloaded tmp_esc, just need a temporary float */
 //      vsc_root = ESL_MAX(vsc_root, tmp_esc);
       /* for UpdateGammaHitMxCM to work, these data need to be un-vectorized: alpha[jp_v][0], bestr */
@@ -1050,7 +1050,7 @@
 #endif
       //if(hitlist != NULL) if((status = UpdateGammaHitMx_epu8(ccm, errbuf, gamma, j, vec_ntS[jp_Sv], hitlist, W, sW)) != eslOK) return status;
       for (d = 0; d < sW; d++) {
-        tmpary[d] = _mm_adds_epu8(vec_ntS[jp_Sv][d],vec_nmlc[d]);
+        tmpary[d] = vec_addsaturating16ub(vec_ntS[jp_Sv][d],vec_nmlc[d]);
       }
       if(hitlist != NULL) { if((status = UpdateGammaHitMx_epu8(ccm, errbuf, gamma, j, tmpary, hitlist, W, sW)) != eslOK) return status; }
       else { 
--- src/impl_sse/sse_util.c
+++ src/impl_sse/sse_util.c
@@ -20,7 +20,7 @@
 inline __m128
 alt_rightshift_ps(__m128 a, __m128 b)
 {
-  return _mm_move_ss(_mm_shuffle_ps(a, a, _MM_SHUFFLE(2, 1, 0, 0)), _mm_shuffle_ps(b, b, _MM_SHUFFLE(3, 3, 3, 3)));
+  return vec_insert1spintolower4sp(vec_shufflepermute4sp(a, a, _MM_SHUFFLE(2, 1, 0, 0)), vec_shufflepermute4sp(b, b, _MM_SHUFFLE(3, 3, 3, 3)));
 }
 
 void vecprint_ps(__m128 a)
@@ -50,8 +50,8 @@
 inline __m128i
 sse_leftshift_epi16(__m128i a, __m128i b)
 {
-  register __m128i v = _mm_srli_si128(a, 2); /* now a[1] a[2] a[3] a[4] a[5] a[6] a[7] 0 */
-  return _mm_insert_epi16(v,_mm_extract_epi16(b,0),7);
+  register __m128i v = vec_shiftrightbytes1q(a, 2); /* now a[1] a[2] a[3] a[4] a[5] a[6] a[7] 0 */
+  return vec_insert8sh(v,vec_extract8sh(b,0),7);
 }
 
 /* Function:  sse_rightshift_epi16()
@@ -66,7 +66,7 @@
 inline __m128i
 sse_rightshift_epi16(__m128i a, __m128i b)
 {
-  register __m128i v = _mm_slli_si128(a, 2); /* now 0 a[0] a[1] a[2] a[3] a[4] a[5] a[6] */
-  return _mm_insert_epi16(v,_mm_extract_epi16(b,7),0);
+  register __m128i v = vec_shiftleftbytes1q(a, 2); /* now 0 a[0] a[1] a[2] a[3] a[4] a[5] a[6] */
+  return vec_insert8sh(v,vec_extract8sh(b,7),0);
 }
 #endif
